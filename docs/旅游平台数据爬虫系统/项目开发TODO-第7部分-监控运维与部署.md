# æ—…æ¸¸å¹³å°æ•°æ®çˆ¬è™«ç³»ç»Ÿ - é¡¹ç›®å¼€å‘TODOè¯¦ç»†æŒ‡å—
## ç¬¬7éƒ¨åˆ†ï¼šç›‘æ§è¿ç»´ä¸éƒ¨ç½²

> æœ¬éƒ¨åˆ†å®ç°ç³»ç»Ÿç›‘æ§ã€æ—¥å¿—ç®¡ç†ã€æ€§èƒ½ä¼˜åŒ–å’Œç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

---

## Phase 7: ç›‘æ§è¿ç»´ä¸éƒ¨ç½²ï¼ˆDay 21-25ï¼‰

### Task 7.1: å®ç°ç›‘æ§ç³»ç»Ÿ

#### 7.1.1 PrometheusæŒ‡æ ‡æ”¶é›†
```python
# src/monitoring/metrics.py
"""ç³»ç»ŸæŒ‡æ ‡æ”¶é›†"""
from prometheus_client import Counter, Histogram, Gauge, Info
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
from functools import wraps
import time
from typing import Callable, Any
from datetime import datetime

from src.utils.logger.logger import get_logger

logger = get_logger("monitoring.metrics")

# å®šä¹‰æŒ‡æ ‡
# çˆ¬è™«ç›¸å…³æŒ‡æ ‡
crawl_requests_total = Counter(
    'crawler_requests_total',
    'Total number of crawl requests',
    ['platform', 'engine', 'status']
)

crawl_duration_seconds = Histogram(
    'crawler_duration_seconds',
    'Crawl request duration in seconds',
    ['platform', 'engine'],
    buckets=(0.1, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0, 120.0, 300.0)
)

active_crawl_tasks = Gauge(
    'crawler_active_tasks',
    'Number of active crawl tasks',
    ['platform', 'engine']
)

# æ•°æ®å¤„ç†æŒ‡æ ‡
data_processed_total = Counter(
    'data_processed_total',
    'Total number of data items processed',
    ['data_type', 'processor', 'status']
)

data_processing_duration = Histogram(
    'data_processing_duration_seconds',
    'Data processing duration in seconds',
    ['data_type', 'processor']
)

# ä»£ç†æ± æŒ‡æ ‡
proxy_pool_size = Gauge(
    'proxy_pool_size',
    'Number of proxies in pool',
    ['status', 'platform']
)

proxy_success_rate = Gauge(
    'proxy_success_rate',
    'Proxy success rate',
    ['platform']
)

# æŒ‡çº¹åº“æŒ‡æ ‡
fingerprint_pool_size = Gauge(
    'fingerprint_pool_size',
    'Number of fingerprints in pool',
    ['platform']
)

# APIæŒ‡æ ‡
api_requests_total = Counter(
    'api_requests_total',
    'Total number of API requests',
    ['method', 'endpoint', 'status']
)

api_request_duration = Histogram(
    'api_request_duration_seconds',
    'API request duration in seconds',
    ['method', 'endpoint']
)

# æ•°æ®åº“æŒ‡æ ‡
db_connections_active = Gauge(
    'db_connections_active',
    'Number of active database connections'
)

db_query_duration = Histogram(
    'db_query_duration_seconds',
    'Database query duration in seconds',
    ['operation']
)

# RedisæŒ‡æ ‡
redis_connections_active = Gauge(
    'redis_connections_active',
    'Number of active Redis connections'
)

redis_memory_usage_bytes = Gauge(
    'redis_memory_usage_bytes',
    'Redis memory usage in bytes'
)

# ç³»ç»Ÿä¿¡æ¯
system_info = Info(
    'crawler_system',
    'Crawler system information'
)

# åˆå§‹åŒ–ç³»ç»Ÿä¿¡æ¯
system_info.info({
    'version': '1.0.0',
    'start_time': datetime.utcnow().isoformat()
})

# è£…é¥°å™¨ï¼šç›‘æ§å‡½æ•°æ‰§è¡Œ
def monitor_execution(
    metric_name: str = None,
    labels: dict = None
):
    """ç›‘æ§å‡½æ•°æ‰§è¡Œæ—¶é—´å’ŒçŠ¶æ€"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def async_wrapper(*args, **kwargs) -> Any:
            start_time = time.time()
            status = "success"
            
            try:
                result = await func(*args, **kwargs)
                return result
            except Exception as e:
                status = "error"
                raise
            finally:
                duration = time.time() - start_time
                
                # è®°å½•æŒ‡æ ‡
                if metric_name == "crawl":
                    crawl_requests_total.labels(
                        platform=labels.get('platform', 'unknown'),
                        engine=labels.get('engine', 'unknown'),
                        status=status
                    ).inc()
                    
                    crawl_duration_seconds.labels(
                        platform=labels.get('platform', 'unknown'),
                        engine=labels.get('engine', 'unknown')
                    ).observe(duration)
                
                elif metric_name == "api":
                    api_requests_total.labels(
                        method=labels.get('method', 'unknown'),
                        endpoint=labels.get('endpoint', 'unknown'),
                        status=status
                    ).inc()
                    
                    api_request_duration.labels(
                        method=labels.get('method', 'unknown'),
                        endpoint=labels.get('endpoint', 'unknown')
                    ).observe(duration)
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs) -> Any:
            start_time = time.time()
            status = "success"
            
            try:
                result = func(*args, **kwargs)
                return result
            except Exception as e:
                status = "error"
                raise
            finally:
                duration = time.time() - start_time
                # åŒæ ·çš„æŒ‡æ ‡è®°å½•é€»è¾‘
        
        # æ ¹æ®å‡½æ•°ç±»å‹è¿”å›å¯¹åº”çš„è£…é¥°å™¨
        import asyncio
        if asyncio.iscoroutinefunction(func):
            return async_wrapper
        else:
            return sync_wrapper
    
    return decorator

# æŒ‡æ ‡æ”¶é›†å™¨
class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.logger = get_logger("metrics.collector")
    
    async def collect_system_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        import psutil
        
        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        
        # å†…å­˜ä½¿ç”¨
        memory = psutil.virtual_memory()
        
        # ç£ç›˜ä½¿ç”¨
        disk = psutil.disk_usage('/')
        
        # ç½‘ç»œIO
        net_io = psutil.net_io_counters()
        
        # æ›´æ–°GaugeæŒ‡æ ‡
        system_cpu_usage = Gauge('system_cpu_usage_percent', 'System CPU usage')
        system_cpu_usage.set(cpu_percent)
        
        system_memory_usage = Gauge('system_memory_usage_percent', 'System memory usage')
        system_memory_usage.set(memory.percent)
        
        system_disk_usage = Gauge('system_disk_usage_percent', 'System disk usage')
        system_disk_usage.set(disk.percent)
        
        self.logger.debug(f"System metrics collected: CPU={cpu_percent}%, Memory={memory.percent}%")
    
    async def collect_crawler_metrics(self, coordinator):
        """æ”¶é›†çˆ¬è™«æŒ‡æ ‡"""
        # ä»åè°ƒå™¨è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = await coordinator.get_statistics()
        
        # æ›´æ–°æ´»è·ƒä»»åŠ¡æ•°
        for platform, engines in stats.get('active_tasks', {}).items():
            for engine, count in engines.items():
                active_crawl_tasks.labels(
                    platform=platform,
                    engine=engine
                ).set(count)
    
    async def collect_proxy_metrics(self, proxy_manager):
        """æ”¶é›†ä»£ç†æ± æŒ‡æ ‡"""
        # è·å–ä»£ç†æ± ç»Ÿè®¡
        stats = await proxy_manager.get_statistics()
        
        # æ›´æ–°ä»£ç†æ± å¤§å°
        for status, platforms in stats.get('pool_size', {}).items():
            for platform, count in platforms.items():
                proxy_pool_size.labels(
                    status=status,
                    platform=platform
                ).set(count)
        
        # æ›´æ–°æˆåŠŸç‡
        for platform, rate in stats.get('success_rate', {}).items():
            proxy_success_rate.labels(platform=platform).set(rate)
    
    def get_metrics(self) -> bytes:
        """è·å–Prometheusæ ¼å¼çš„æŒ‡æ ‡"""
        return generate_latest()
```

#### 7.1.2 å¥åº·æ£€æŸ¥å®ç°
```python
# src/monitoring/health.py
"""å¥åº·æ£€æŸ¥"""
from typing import Dict, Any, List
from datetime import datetime, timedelta
from enum import Enum

from src.database.connection import get_db_pool, get_redis_client
from src.core.scheduler.coordinator import coordinator
from src.anti_detection.proxy.manager import proxy_manager
from src.utils.logger.logger import get_logger

logger = get_logger("monitoring.health")

class HealthStatus(str, Enum):
    """å¥åº·çŠ¶æ€"""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"

class ComponentHealth:
    """ç»„ä»¶å¥åº·çŠ¶æ€"""
    
    def __init__(self, name: str):
        self.name = name
        self.status = HealthStatus.HEALTHY
        self.message = "OK"
        self.details = {}
        self.last_check = datetime.utcnow()
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "name": self.name,
            "status": self.status,
            "message": self.message,
            "details": self.details,
            "last_check": self.last_check.isoformat()
        }

class HealthChecker:
    """å¥åº·æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.components = {}
        self.logger = get_logger("health.checker")
    
    async def check_database(self) -> ComponentHealth:
        """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶æ€"""
        health = ComponentHealth("database")
        
        try:
            # æµ‹è¯•æ•°æ®åº“è¿æ¥
            pool = await get_db_pool()
            
            async with pool.acquire() as conn:
                # æ‰§è¡Œç®€å•æŸ¥è¯¢
                result = await conn.fetchval("SELECT 1")
                
                # è·å–è¿æ¥æ± çŠ¶æ€
                health.details = {
                    "pool_size": pool.get_size(),
                    "pool_free": pool.get_idle_size(),
                    "pool_used": pool.get_size() - pool.get_idle_size()
                }
                
                if pool.get_idle_size() == 0:
                    health.status = HealthStatus.DEGRADED
                    health.message = "Connection pool exhausted"
                
        except Exception as e:
            health.status = HealthStatus.UNHEALTHY
            health.message = f"Database error: {str(e)}"
            logger.error(f"Database health check failed: {str(e)}")
        
        self.components["database"] = health
        return health
    
    async def check_redis(self) -> ComponentHealth:
        """æ£€æŸ¥Rediså¥åº·çŠ¶æ€"""
        health = ComponentHealth("redis")
        
        try:
            # æµ‹è¯•Redisè¿æ¥
            redis = await get_redis_client()
            
            # PINGæµ‹è¯•
            pong = await redis.ping()
            if not pong:
                raise Exception("Redis PING failed")
            
            # è·å–Redisä¿¡æ¯
            info = await redis.info()
            
            health.details = {
                "version": info.get("redis_version"),
                "connected_clients": info.get("connected_clients", 0),
                "used_memory_human": info.get("used_memory_human"),
                "uptime_days": info.get("uptime_in_days", 0)
            }
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            used_memory = info.get("used_memory", 0)
            max_memory = info.get("maxmemory", 0)
            
            if max_memory > 0 and used_memory / max_memory > 0.9:
                health.status = HealthStatus.DEGRADED
                health.message = "Memory usage above 90%"
                
        except Exception as e:
            health.status = HealthStatus.UNHEALTHY
            health.message = f"Redis error: {str(e)}"
            logger.error(f"Redis health check failed: {str(e)}")
        
        self.components["redis"] = health
        return health
    
    async def check_crawler_engines(self) -> ComponentHealth:
        """æ£€æŸ¥çˆ¬è™«å¼•æ“å¥åº·çŠ¶æ€"""
        health = ComponentHealth("crawler_engines")
        
        try:
            # è·å–å¼•æ“çŠ¶æ€
            engine_status = await coordinator.get_engine_status()
            
            health.details = {
                "crawl4ai": engine_status.get("crawl4ai", {}).get("status"),
                "mediacrawl": engine_status.get("mediacrawl", {}).get("status"),
                "active_tasks": engine_status.get("active_tasks", 0),
                "queued_tasks": engine_status.get("queued_tasks", 0)
            }
            
            # æ£€æŸ¥å¼•æ“çŠ¶æ€
            unhealthy_engines = []
            for engine, status in engine_status.items():
                if isinstance(status, dict) and status.get("status") == "error":
                    unhealthy_engines.append(engine)
            
            if unhealthy_engines:
                health.status = HealthStatus.DEGRADED
                health.message = f"Engines unhealthy: {', '.join(unhealthy_engines)}"
            
            # æ£€æŸ¥ä»»åŠ¡ç§¯å‹
            if health.details["queued_tasks"] > 1000:
                health.status = HealthStatus.DEGRADED
                health.message = "Task queue backlog detected"
                
        except Exception as e:
            health.status = HealthStatus.UNHEALTHY
            health.message = f"Engine check error: {str(e)}"
            logger.error(f"Crawler engine health check failed: {str(e)}")
        
        self.components["crawler_engines"] = health
        return health
    
    async def check_proxy_pool(self) -> ComponentHealth:
        """æ£€æŸ¥ä»£ç†æ± å¥åº·çŠ¶æ€"""
        health = ComponentHealth("proxy_pool")
        
        try:
            # è·å–ä»£ç†æ± ç»Ÿè®¡
            stats = await proxy_manager.get_statistics()
            
            total_proxies = stats.get("total", 0)
            active_proxies = stats.get("active", 0)
            success_rate = stats.get("overall_success_rate", 0)
            
            health.details = {
                "total_proxies": total_proxies,
                "active_proxies": active_proxies,
                "success_rate": f"{success_rate:.2%}",
                "platforms": stats.get("platforms", {})
            }
            
            # æ£€æŸ¥ä»£ç†æ± çŠ¶æ€
            if total_proxies == 0:
                health.status = HealthStatus.UNHEALTHY
                health.message = "No proxies available"
            elif active_proxies < 10:
                health.status = HealthStatus.DEGRADED
                health.message = "Low active proxy count"
            elif success_rate < 0.5:
                health.status = HealthStatus.DEGRADED
                health.message = "Low proxy success rate"
                
        except Exception as e:
            health.status = HealthStatus.UNHEALTHY
            health.message = f"Proxy pool error: {str(e)}"
            logger.error(f"Proxy pool health check failed: {str(e)}")
        
        self.components["proxy_pool"] = health
        return health
    
    async def check_api_service(self) -> ComponentHealth:
        """æ£€æŸ¥APIæœåŠ¡å¥åº·çŠ¶æ€"""
        health = ComponentHealth("api_service")
        
        try:
            # è¿™é‡Œå¯ä»¥æ£€æŸ¥APIçš„å†…éƒ¨çŠ¶æ€
            # ä¾‹å¦‚ï¼šè¯·æ±‚é˜Ÿåˆ—ã€å“åº”æ—¶é—´ç­‰
            
            health.details = {
                "version": "1.0.0",
                "uptime": "24h",  # å®é™…åº”è¯¥è®¡ç®—
                "request_rate": "100/min"  # å®é™…åº”è¯¥ç»Ÿè®¡
            }
            
        except Exception as e:
            health.status = HealthStatus.UNHEALTHY
            health.message = f"API service error: {str(e)}"
            logger.error(f"API service health check failed: {str(e)}")
        
        self.components["api_service"] = health
        return health
    
    async def check_all(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ‰€æœ‰å¥åº·æ£€æŸ¥"""
        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
        import asyncio
        
        checks = [
            self.check_database(),
            self.check_redis(),
            self.check_crawler_engines(),
            self.check_proxy_pool(),
            self.check_api_service()
        ]
        
        await asyncio.gather(*checks, return_exceptions=True)
        
        # è®¡ç®—æ•´ä½“å¥åº·çŠ¶æ€
        overall_status = HealthStatus.HEALTHY
        unhealthy_components = []
        degraded_components = []
        
        for name, component in self.components.items():
            if component.status == HealthStatus.UNHEALTHY:
                unhealthy_components.append(name)
                overall_status = HealthStatus.UNHEALTHY
            elif component.status == HealthStatus.DEGRADED:
                degraded_components.append(name)
                if overall_status != HealthStatus.UNHEALTHY:
                    overall_status = HealthStatus.DEGRADED
        
        # æ„å»ºå“åº”
        response = {
            "status": overall_status,
            "timestamp": datetime.utcnow().isoformat(),
            "components": {
                name: component.to_dict() 
                for name, component in self.components.items()
            }
        }
        
        if unhealthy_components:
            response["message"] = f"Unhealthy components: {', '.join(unhealthy_components)}"
        elif degraded_components:
            response["message"] = f"Degraded components: {', '.join(degraded_components)}"
        else:
            response["message"] = "All systems operational"
        
        return response

# å…¨å±€å¥åº·æ£€æŸ¥å™¨å®ä¾‹
health_checker = HealthChecker()
```

#### 7.1.3 ç›‘æ§APIç«¯ç‚¹
```python
# src/api/v1/endpoints/monitoring.py
"""ç›‘æ§APIç«¯ç‚¹"""
from fastapi import APIRouter, Response
from fastapi.responses import PlainTextResponse, JSONResponse

from src.monitoring.metrics import MetricsCollector
from src.monitoring.health import health_checker
from src.utils.logger.logger import get_logger

logger = get_logger("api.monitoring")
router = APIRouter()

# æŒ‡æ ‡æ”¶é›†å™¨
metrics_collector = MetricsCollector()

@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    try:
        result = await health_checker.check_all()
        
        # æ ¹æ®çŠ¶æ€è¿”å›ä¸åŒçš„HTTPçŠ¶æ€ç 
        status_code = 200
        if result["status"] == "degraded":
            status_code = 200  # ä»ç„¶å¯ç”¨ï¼Œä½†æ€§èƒ½ä¸‹é™
        elif result["status"] == "unhealthy":
            status_code = 503  # æœåŠ¡ä¸å¯ç”¨
        
        return JSONResponse(
            content=result,
            status_code=status_code
        )
        
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return JSONResponse(
            content={
                "status": "unhealthy",
                "message": f"Health check error: {str(e)}"
            },
            status_code=503
        )

@router.get("/health/live")
async def liveness_probe():
    """å­˜æ´»æ¢é’ˆ - K8sä½¿ç”¨"""
    return {"status": "alive"}

@router.get("/health/ready")
async def readiness_probe():
    """å°±ç»ªæ¢é’ˆ - K8sä½¿ç”¨"""
    try:
        # å¿«é€Ÿæ£€æŸ¥å…³é”®ç»„ä»¶
        result = await health_checker.check_database()
        
        if result.status == "unhealthy":
            return JSONResponse(
                content={"status": "not ready", "reason": result.message},
                status_code=503
            )
        
        return {"status": "ready"}
        
    except Exception as e:
        return JSONResponse(
            content={"status": "not ready", "reason": str(e)},
            status_code=503
        )

@router.get("/metrics")
async def prometheus_metrics():
    """PrometheusæŒ‡æ ‡ç«¯ç‚¹"""
    try:
        # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
        await metrics_collector.collect_system_metrics()
        
        # è·å–æ‰€æœ‰æŒ‡æ ‡
        metrics_data = metrics_collector.get_metrics()
        
        return Response(
            content=metrics_data,
            media_type="text/plain; version=0.0.4; charset=utf-8"
        )
        
    except Exception as e:
        logger.error(f"Failed to collect metrics: {str(e)}")
        return PlainTextResponse(
            content=f"# Error collecting metrics: {str(e)}",
            status_code=500
        )

@router.get("/metrics/custom")
async def custom_metrics():
    """è‡ªå®šä¹‰æŒ‡æ ‡æŸ¥è¯¢"""
    try:
        from src.core.scheduler.coordinator import coordinator
        from src.anti_detection.proxy.manager import proxy_manager
        
        # æ”¶é›†å„ç»„ä»¶çš„è¯¦ç»†æŒ‡æ ‡
        crawler_stats = await coordinator.get_statistics()
        proxy_stats = await proxy_manager.get_statistics()
        
        metrics = {
            "crawler": {
                "total_tasks": crawler_stats.get("total_tasks", 0),
                "active_tasks": crawler_stats.get("active_tasks", 0),
                "success_rate": crawler_stats.get("success_rate", 0),
                "platforms": crawler_stats.get("platforms", {})
            },
            "proxy_pool": {
                "total": proxy_stats.get("total", 0),
                "active": proxy_stats.get("active", 0),
                "success_rate": proxy_stats.get("overall_success_rate", 0)
            },
            "timestamp": datetime.utcnow().isoformat()
        }
        
        return JSONResponse(content=metrics)
        
    except Exception as e:
        logger.error(f"Failed to collect custom metrics: {str(e)}")
        return JSONResponse(
            content={"error": str(e)},
            status_code=500
        )
```

### Task 7.2: å®ç°æ—¥å¿—ç®¡ç†

#### 7.2.1 ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿ
```python
# src/utils/logger/structured_logger.py
"""ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿ"""
import json
import sys
from typing import Dict, Any, Optional
from datetime import datetime
from loguru import logger
import traceback

# ç§»é™¤é»˜è®¤å¤„ç†å™¨
logger.remove()

# è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼
def json_formatter(record: dict) -> str:
    """JSONæ ¼å¼åŒ–å™¨"""
    log_entry = {
        "timestamp": record["time"].isoformat(),
        "level": record["level"].name,
        "logger": record["name"],
        "message": record["message"],
        "module": record["module"],
        "function": record["function"],
        "line": record["line"],
        "thread": record["thread"].name,
        "process": record["process"].name
    }
    
    # æ·»åŠ é¢å¤–å­—æ®µ
    if record.get("extra"):
        log_entry["extra"] = record["extra"]
    
    # æ·»åŠ å¼‚å¸¸ä¿¡æ¯
    if record.get("exception"):
        log_entry["exception"] = {
            "type": record["exception"].type.__name__,
            "value": str(record["exception"].value),
            "traceback": "".join(traceback.format_tb(record["exception"].traceback))
        }
    
    return json.dumps(log_entry, ensure_ascii=False)

# æ§åˆ¶å°å¤„ç†å™¨ï¼ˆå¼€å‘ç¯å¢ƒï¼‰
logger.add(
    sys.stdout,
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
    level="DEBUG",
    colorize=True,
    backtrace=True,
    diagnose=True
)

# JSONæ–‡ä»¶å¤„ç†å™¨ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
logger.add(
    "logs/crawler_{time:YYYY-MM-DD}.json",
    format=json_formatter,
    level="INFO",
    rotation="00:00",
    retention="30 days",
    compression="gz",
    serialize=True
)

# é”™è¯¯æ—¥å¿—å•ç‹¬å¤„ç†
logger.add(
    "logs/errors_{time:YYYY-MM-DD}.log",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line}\n{message}\n{exception}\n",
    level="ERROR",
    rotation="1 day",
    retention="7 days",
    backtrace=True,
    diagnose=True
)

class StructuredLogger:
    """ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨"""
    
    def __init__(self, name: str):
        self.logger = logger.bind(name=name)
    
    def debug(self, message: str, **kwargs):
        """è°ƒè¯•æ—¥å¿—"""
        self.logger.debug(message, **kwargs)
    
    def info(self, message: str, **kwargs):
        """ä¿¡æ¯æ—¥å¿—"""
        self.logger.info(message, **kwargs)
    
    def warning(self, message: str, **kwargs):
        """è­¦å‘Šæ—¥å¿—"""
        self.logger.warning(message, **kwargs)
    
    def error(self, message: str, **kwargs):
        """é”™è¯¯æ—¥å¿—"""
        self.logger.error(message, **kwargs)
    
    def critical(self, message: str, **kwargs):
        """ä¸¥é‡é”™è¯¯æ—¥å¿—"""
        self.logger.critical(message, **kwargs)
    
    def with_context(self, **context) -> "StructuredLogger":
        """æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        return StructuredLogger(
            logger.bind(name=self.logger._name, **context)
        )
    
    def log_event(
        self, 
        event_type: str, 
        event_data: Dict[str, Any],
        level: str = "INFO"
    ):
        """è®°å½•ç»“æ„åŒ–äº‹ä»¶"""
        log_method = getattr(self.logger, level.lower())
        log_method(
            f"Event: {event_type}",
            event_type=event_type,
            event_data=event_data
        )
    
    def log_metric(
        self,
        metric_name: str,
        value: float,
        tags: Optional[Dict[str, str]] = None
    ):
        """è®°å½•æŒ‡æ ‡"""
        self.logger.info(
            f"Metric: {metric_name}={value}",
            metric_name=metric_name,
            metric_value=value,
            metric_tags=tags or {}
        )
    
    def log_request(
        self,
        method: str,
        url: str,
        status_code: int,
        duration: float,
        **kwargs
    ):
        """è®°å½•HTTPè¯·æ±‚"""
        self.logger.info(
            f"{method} {url} - {status_code} ({duration:.3f}s)",
            request_method=method,
            request_url=url,
            response_status=status_code,
            response_time=duration,
            **kwargs
        )
    
    def log_task(
        self,
        task_id: str,
        task_type: str,
        status: str,
        duration: Optional[float] = None,
        **kwargs
    ):
        """è®°å½•ä»»åŠ¡æ‰§è¡Œ"""
        message = f"Task {task_id} ({task_type}) - {status}"
        if duration:
            message += f" ({duration:.3f}s)"
        
        self.logger.info(
            message,
            task_id=task_id,
            task_type=task_type,
            task_status=status,
            task_duration=duration,
            **kwargs
        )

# æ—¥å¿—èšåˆå™¨
class LogAggregator:
    """æ—¥å¿—èšåˆå™¨ - ç”¨äºå‘é€åˆ°é›†ä¸­å¼æ—¥å¿—ç³»ç»Ÿ"""
    
    def __init__(self):
        self.buffer = []
        self.max_buffer_size = 100
        self.flush_interval = 10  # ç§’
    
    async def send_to_elasticsearch(self, logs: List[Dict[str, Any]]):
        """å‘é€æ—¥å¿—åˆ°Elasticsearch"""
        # å®ç°å‘é€é€»è¾‘
        pass
    
    async def send_to_loki(self, logs: List[Dict[str, Any]]):
        """å‘é€æ—¥å¿—åˆ°Loki"""
        # å®ç°å‘é€é€»è¾‘
        pass

# åˆ›å»ºæ—¥å¿—è®°å½•å™¨çš„å·¥å‚å‡½æ•°
def get_structured_logger(name: str) -> StructuredLogger:
    """è·å–ç»“æ„åŒ–æ—¥å¿—è®°å½•å™¨"""
    return StructuredLogger(name)
```

#### 7.2.2 æ—¥å¿—ä¸­é—´ä»¶
```python
# src/api/middleware/logging.py
"""æ—¥å¿—ä¸­é—´ä»¶"""
import time
import uuid
from typing import Callable
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.types import ASGIApp

from src.utils.logger.structured_logger import get_structured_logger

logger = get_structured_logger("api.middleware.logging")

class LoggingMiddleware(BaseHTTPMiddleware):
    """è¯·æ±‚æ—¥å¿—ä¸­é—´ä»¶"""
    
    def __init__(self, app: ASGIApp):
        super().__init__(app)
    
    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        # ç”Ÿæˆè¯·æ±‚ID
        request_id = str(uuid.uuid4())
        
        # è®°å½•è¯·æ±‚å¼€å§‹
        start_time = time.time()
        
        # æ·»åŠ è¯·æ±‚IDåˆ°æ—¥å¿—ä¸Šä¸‹æ–‡
        request_logger = logger.with_context(request_id=request_id)
        
        # è®°å½•è¯·æ±‚ä¿¡æ¯
        request_logger.info(
            f"Request started: {request.method} {request.url.path}",
            request_method=request.method,
            request_path=request.url.path,
            request_query=str(request.url.query),
            client_host=request.client.host if request.client else None,
            user_agent=request.headers.get("user-agent")
        )
        
        # å¤„ç†è¯·æ±‚
        try:
            response = await call_next(request)
            
            # è®¡ç®—å“åº”æ—¶é—´
            duration = time.time() - start_time
            
            # è®°å½•å“åº”ä¿¡æ¯
            request_logger.log_request(
                method=request.method,
                url=str(request.url),
                status_code=response.status_code,
                duration=duration,
                request_id=request_id
            )
            
            # æ·»åŠ è¯·æ±‚IDåˆ°å“åº”å¤´
            response.headers["X-Request-ID"] = request_id
            
            return response
            
        except Exception as e:
            # è®°å½•å¼‚å¸¸
            duration = time.time() - start_time
            
            request_logger.error(
                f"Request failed: {request.method} {request.url.path}",
                request_method=request.method,
                request_path=request.url.path,
                error_type=type(e).__name__,
                error_message=str(e),
                duration=duration,
                request_id=request_id
            )
            
            raise

class PerformanceLoggingMiddleware(BaseHTTPMiddleware):
    """æ€§èƒ½æ—¥å¿—ä¸­é—´ä»¶"""
    
    def __init__(self, app: ASGIApp):
        super().__init__(app)
        self.slow_request_threshold = 5.0  # ç§’
    
    async def dispatch(self, request: Request, call_next: Callable) -> Response:
        start_time = time.time()
        
        # å¤„ç†è¯·æ±‚
        response = await call_next(request)
        
        # è®¡ç®—å“åº”æ—¶é—´
        duration = time.time() - start_time
        
        # è®°å½•æ…¢è¯·æ±‚
        if duration > self.slow_request_threshold:
            logger.warning(
                f"Slow request detected: {request.method} {request.url.path}",
                request_method=request.method,
                request_path=request.url.path,
                duration=duration,
                threshold=self.slow_request_threshold
            )
        
        # æ·»åŠ æ€§èƒ½å¤´
        response.headers["X-Response-Time"] = f"{duration:.3f}"
        
        return response
```

### Task 7.3: éƒ¨ç½²é…ç½®

#### 7.3.1 Docker Composeé…ç½®
```yaml
# docker-compose.yml
version: '3.8'

services:
  # PostgreSQLæ•°æ®åº“
  postgres:
    image: postgis/postgis:14-3.2
    container_name: crawler_postgres
    environment:
      POSTGRES_DB: crawler_db
      POSTGRES_USER: crawler_user
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U crawler_user"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - crawler_network

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    container_name: crawler_redis
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - crawler_network

  # çˆ¬è™«ä¸»æœåŠ¡
  crawler_api:
    build:
      context: .
      dockerfile: docker/production/Dockerfile
      target: api
    container_name: crawler_api
    environment:
      - DATABASE_URL=postgresql://crawler_user:${DB_PASSWORD}@postgres:5432/crawler_db
      - REDIS_URL=redis://redis:6379/0
      - ENVIRONMENT=production
      - LOG_LEVEL=INFO
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    networks:
      - crawler_network
    restart: unless-stopped

  # Crawl4AIå¼•æ“
  crawl4ai_engine:
    build:
      context: .
      dockerfile: docker/production/Dockerfile
      target: crawl4ai
    container_name: crawl4ai_engine
    environment:
      - REDIS_URL=redis://redis:6379/0
      - ENGINE_TYPE=crawl4ai
      - MAX_WORKERS=10
    depends_on:
      - redis
    volumes:
      - ./logs:/app/logs
    networks:
      - crawler_network
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '2'
          memory: 2G

  # MediaCrawlå¼•æ“
  mediacrawl_engine:
    build:
      context: .
      dockerfile: docker/production/Dockerfile
      target: mediacrawl
    container_name: mediacrawl_engine
    environment:
      - REDIS_URL=redis://redis:6379/0
      - ENGINE_TYPE=mediacrawl
      - MAX_BROWSERS=5
    depends_on:
      - redis
    volumes:
      - ./logs:/app/logs
      - ./data/screenshots:/app/screenshots
    networks:
      - crawler_network
    restart: unless-stopped
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '4'
          memory: 4G

  # ä»»åŠ¡è°ƒåº¦å™¨
  scheduler:
    build:
      context: .
      dockerfile: docker/production/Dockerfile
      target: scheduler
    container_name: crawler_scheduler
    environment:
      - DATABASE_URL=postgresql://crawler_user:${DB_PASSWORD}@postgres:5432/crawler_db
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - postgres
      - redis
    volumes:
      - ./logs:/app/logs
    networks:
      - crawler_network
    restart: unless-stopped

  # Prometheusç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    container_name: crawler_prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    networks:
      - crawler_network
    restart: unless-stopped

  # Grafanaå¯è§†åŒ–
  grafana:
    image: grafana/grafana:latest
    container_name: crawler_grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    networks:
      - crawler_network
    restart: unless-stopped

  # Nginxåå‘ä»£ç†
  nginx:
    image: nginx:alpine
    container_name: crawler_nginx
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/conf.d:/etc/nginx/conf.d
      - ./ssl:/etc/nginx/ssl
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - crawler_api
    networks:
      - crawler_network
    restart: unless-stopped

  # æ—¥å¿—æ”¶é›†å™¨ - Loki
  loki:
    image: grafana/loki:latest
    container_name: crawler_loki
    ports:
      - "3100:3100"
    volumes:
      - ./monitoring/loki-config.yaml:/etc/loki/local-config.yaml
      - loki_data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    networks:
      - crawler_network
    restart: unless-stopped

  # æ—¥å¿—æ”¶é›†ä»£ç† - Promtail
  promtail:
    image: grafana/promtail:latest
    container_name: crawler_promtail
    volumes:
      - ./monitoring/promtail-config.yaml:/etc/promtail/config.yml
      - ./logs:/var/log/crawler
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki
    networks:
      - crawler_network
    restart: unless-stopped

networks:
  crawler_network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:
  loki_data:
```

#### 7.3.2 Kuberneteséƒ¨ç½²é…ç½®
```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: crawler-system

---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: crawler-config
  namespace: crawler-system
data:
  APP_ENV: "production"
  LOG_LEVEL: "INFO"
  DATABASE_HOST: "postgres-service"
  DATABASE_PORT: "5432"
  DATABASE_NAME: "crawler_db"
  REDIS_HOST: "redis-service"
  REDIS_PORT: "6379"

---
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: crawler-secrets
  namespace: crawler-system
type: Opaque
data:
  database-user: Y3Jhd2xlcl91c2Vy  # base64 encoded
  database-password: <base64-encoded-password>
  redis-password: <base64-encoded-password>

---
# k8s/postgres.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: crawler-system
spec:
  serviceName: postgres-service
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgis/postgis:14-3.2
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_DB
          value: crawler_db
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: crawler-secrets
              key: database-user
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: crawler-secrets
              key: database-password
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        livenessProbe:
          exec:
            command: ["pg_isready", "-U", "crawler_user"]
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command: ["pg_isready", "-U", "crawler_user"]
          initialDelaySeconds: 5
          periodSeconds: 5
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 50Gi

---
# k8s/postgres-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
  namespace: crawler-system
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432

---
# k8s/redis.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: crawler-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        command:
          - redis-server
          - --appendonly yes
          - --maxmemory 2gb
          - --maxmemory-policy allkeys-lru
        volumeMounts:
        - name: redis-storage
          mountPath: /data
        livenessProbe:
          exec:
            command: ["redis-cli", "ping"]
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command: ["redis-cli", "ping"]
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: redis-storage
        persistentVolumeClaim:
          claimName: redis-pvc

---
# k8s/api-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crawler-api
  namespace: crawler-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: crawler-api
  template:
    metadata:
      labels:
        app: crawler-api
    spec:
      containers:
      - name: crawler-api
        image: crawler-system/api:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          value: "postgresql://$(DATABASE_USER):$(DATABASE_PASSWORD)@$(DATABASE_HOST):$(DATABASE_PORT)/$(DATABASE_NAME)"
        - name: REDIS_URL
          value: "redis://$(REDIS_HOST):$(REDIS_PORT)/0"
        envFrom:
        - configMapRef:
            name: crawler-config
        - secretRef:
            name: crawler-secrets
        livenessProbe:
          httpGet:
            path: /api/v1/health/live
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/v1/health/ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"

---
# k8s/api-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: crawler-api-service
  namespace: crawler-system
spec:
  selector:
    app: crawler-api
  ports:
  - port: 80
    targetPort: 8000
  type: ClusterIP

---
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: crawler-ingress
  namespace: crawler-system
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - api.crawler.example.com
    secretName: crawler-tls
  rules:
  - host: api.crawler.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: crawler-api-service
            port:
              number: 80

---
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: crawler-api-hpa
  namespace: crawler-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: crawler-api
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

### Task 7.4: éƒ¨ç½²éªŒè¯è„šæœ¬

#### 7.4.1 éƒ¨ç½²æ£€æŸ¥è„šæœ¬
```bash
#!/bin/bash
# scripts/deploy/check_deployment.sh

echo "ğŸ” Checking Travel Crawler System Deployment..."

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# æ£€æŸ¥å‡½æ•°
check_service() {
    local service_name=$1
    local check_command=$2
    
    echo -n "Checking $service_name... "
    
    if eval $check_command > /dev/null 2>&1; then
        echo -e "${GREEN}âœ“ OK${NC}"
        return 0
    else
        echo -e "${RED}âœ— FAILED${NC}"
        return 1
    fi
}

# æ£€æŸ¥DockeræœåŠ¡
echo "=== Docker Services ==="
check_service "PostgreSQL" "docker exec crawler_postgres pg_isready -U crawler_user"
check_service "Redis" "docker exec crawler_redis redis-cli ping"
check_service "API Service" "curl -s http://localhost:8000/api/v1/health"
check_service "Prometheus" "curl -s http://localhost:9090/-/healthy"
check_service "Grafana" "curl -s http://localhost:3000/api/health"

# æ£€æŸ¥APIç«¯ç‚¹
echo -e "\n=== API Endpoints ==="
check_service "Health Check" "curl -s http://localhost:8000/api/v1/health | grep -q 'healthy'"
check_service "Metrics" "curl -s http://localhost:8000/api/v1/metrics | grep -q 'crawler_requests_total'"
check_service "Platforms" "curl -s http://localhost:8000/api/v1/crawler/platforms | grep -q 'amap'"

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
echo -e "\n=== Database Check ==="
docker exec crawler_postgres psql -U crawler_user -d crawler_db -c "SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public';" | grep -q "[0-9]" && echo -e "${GREEN}âœ“ Database tables created${NC}" || echo -e "${RED}âœ— Database tables missing${NC}"

# æ£€æŸ¥æ—¥å¿—
echo -e "\n=== Log Files ==="
if [ -d "./logs" ] && [ "$(ls -A ./logs)" ]; then
    echo -e "${GREEN}âœ“ Log files present${NC}"
    ls -la ./logs | head -5
else
    echo -e "${YELLOW}âš  No log files found${NC}"
fi

# æ£€æŸ¥æ€§èƒ½
echo -e "\n=== Performance Check ==="
response_time=$(curl -o /dev/null -s -w '%{time_total}' http://localhost:8000/api/v1/health)
echo "API Response Time: ${response_time}s"

if (( $(echo "$response_time < 1" | bc -l) )); then
    echo -e "${GREEN}âœ“ Response time is good${NC}"
else
    echo -e "${YELLOW}âš  Response time is slow${NC}"
fi

# æ€»ç»“
echo -e "\n=== Deployment Summary ==="
echo "Deployment check completed."
echo "Please check any failed services and review the logs for more details."
```

#### 7.4.2 ç›‘æ§ä»ªè¡¨æ¿é…ç½®
```json
// monitoring/grafana/dashboards/crawler-dashboard.json
{
  "dashboard": {
    "id": null,
    "title": "Travel Crawler System Dashboard",
    "tags": ["crawler", "monitoring"],
    "timezone": "browser",
    "panels": [
      {
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "id": 1,
        "title": "Crawl Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(crawler_requests_total[5m])",
            "legendFormat": "{{platform}} - {{status}}"
          }
        ]
      },
      {
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
        "id": 2,
        "title": "Crawl Duration",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, crawler_duration_seconds_bucket)",
            "legendFormat": "95th percentile"
          }
        ]
      },
      {
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
        "id": 3,
        "title": "Active Tasks",
        "type": "graph",
        "targets": [
          {
            "expr": "crawler_active_tasks",
            "legendFormat": "{{platform}} - {{engine}}"
          }
        ]
      },
      {
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8},
        "id": 4,
        "title": "Proxy Pool Status",
        "type": "graph",
        "targets": [
          {
            "expr": "proxy_pool_size",
            "legendFormat": "{{status}} - {{platform}}"
          }
        ]
      },
      {
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 16},
        "id": 5,
        "title": "API Performance",
        "type": "table",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, api_request_duration_seconds_bucket)",
            "format": "table"
          }
        ]
      }
    ],
    "schemaVersion": 16,
    "version": 0
  }
}
```

---

## æ‰§è¡ŒéªŒè¯

### Claude Codeæ‰§è¡Œæ­¥éª¤ï¼š

1. **å¯åŠ¨ç›‘æ§ç³»ç»Ÿ**ï¼š
```bash
cd /Users/xiao/Documents/BaiduNetSyncDownload/CodeDev/CrawlPOIData/travel-crawler-system

# ä½¿ç”¨Docker Composeå¯åŠ¨
docker-compose up -d

# ç­‰å¾…æœåŠ¡å¯åŠ¨
sleep 30
```

2. **è¿è¡Œéƒ¨ç½²æ£€æŸ¥**ï¼š
```bash
chmod +x scripts/deploy/check_deployment.sh
./scripts/deploy/check_deployment.sh
```

é¢„æœŸè¾“å‡ºï¼š
- æ‰€æœ‰æœåŠ¡å¥åº·æ£€æŸ¥é€šè¿‡
- APIå“åº”æ—¶é—´å°äº1ç§’
- æ—¥å¿—æ–‡ä»¶æ­£å¸¸ç”Ÿæˆ

3. **è®¿é—®ç›‘æ§ç•Œé¢**ï¼š
```bash
# Prometheus
open http://localhost:9090

# Grafana (é»˜è®¤ç”¨æˆ·å/å¯†ç : admin/admin)
open http://localhost:3000

# APIå¥åº·æ£€æŸ¥
curl http://localhost:8000/api/v1/health | jq .
```

4. **æ£€æŸ¥æ—¥å¿—**ï¼š
```bash
# æŸ¥çœ‹APIæ—¥å¿—
tail -f logs/crawler_*.json | jq .

# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
tail -f logs/errors_*.log
```

## ç”Ÿäº§ç¯å¢ƒæ³¨æ„äº‹é¡¹

1. **å®‰å…¨é…ç½®**ï¼š
   - ä½¿ç”¨å¼ºå¯†ç å’Œå¯†é’¥
   - å¯ç”¨HTTPS
   - é…ç½®é˜²ç«å¢™è§„åˆ™
   - ä½¿ç”¨Secretsç®¡ç†æ•æ„Ÿä¿¡æ¯

2. **æ€§èƒ½ä¼˜åŒ–**ï¼š
   - é…ç½®é€‚å½“çš„èµ„æºé™åˆ¶
   - å¯ç”¨ç¼“å­˜
   - ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•
   - ä½¿ç”¨CDNåŠ é€Ÿé™æ€èµ„æº

3. **é«˜å¯ç”¨æ€§**ï¼š
   - é…ç½®å¤šå‰¯æœ¬
   - ä½¿ç”¨è´Ÿè½½å‡è¡¡
   - é…ç½®è‡ªåŠ¨æ•…éšœè½¬ç§»
   - å®šæœŸå¤‡ä»½æ•°æ®

4. **ç›‘æ§å‘Šè­¦**ï¼š
   - é…ç½®å…³é”®æŒ‡æ ‡å‘Šè­¦
   - è®¾ç½®é”™è¯¯ç‡é˜ˆå€¼
   - ç›‘æ§èµ„æºä½¿ç”¨ç‡
   - é…ç½®é€šçŸ¥æ¸ é“ï¼ˆé‚®ä»¶/é’‰é’‰/Slackï¼‰

## é¡¹ç›®å®Œæˆ

è‡³æ­¤ï¼Œæ—…æ¸¸å¹³å°æ•°æ®çˆ¬è™«ç³»ç»Ÿçš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½å·²ç»å®ç°å®Œæˆï¼ŒåŒ…æ‹¬ï¼š

1. âœ… åŒå¼•æ“æ¶æ„ï¼ˆCrawl4AI + MediaCrawlï¼‰
2. âœ… 8ä¸ªå¹³å°é€‚é…å™¨
3. âœ… åçˆ¬å¯¹æŠ—ç³»ç»Ÿ
4. âœ… æ•°æ®å¤„ç†ç®¡é“
5. âœ… APIæœåŠ¡
6. âœ… ç›‘æ§è¿ç»´ç³»ç»Ÿ
7. âœ… å®¹å™¨åŒ–éƒ¨ç½²

ç³»ç»Ÿå·²ç»å¯ä»¥æŠ•å…¥ç”Ÿäº§ä½¿ç”¨ï¼