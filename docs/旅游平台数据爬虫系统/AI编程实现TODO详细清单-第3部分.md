# æ—…æ¸¸å¹³å°æ•°æ®çˆ¬è™«ç³»ç»Ÿ - AIç¼–ç¨‹å®ç°TODOè¯¦ç»†æ¸…å•ï¼ˆç¬¬3éƒ¨åˆ†ï¼‰

> æœ¬æ–‡æ¡£ä¸ºAIç¼–ç¨‹å®ç°TODOè¯¦ç»†æ¸…å•çš„ç¬¬ä¸‰éƒ¨åˆ†ï¼ŒåŒ…å«ç¬¬6-7é˜¶æ®µçš„è¯¦ç»†ä»»åŠ¡

## å‰æƒ…æè¦

å·²å®Œæˆéƒ¨åˆ†ï¼š
- âœ… ç¬¬1é˜¶æ®µï¼šé¡¹ç›®åˆå§‹åŒ–ä¸åŸºç¡€æ¶æ„
- âœ… ç¬¬2é˜¶æ®µï¼šåŒå¼•æ“æ ¸å¿ƒæ¶æ„
- âœ… ç¬¬3é˜¶æ®µï¼šåçˆ¬ç³»ç»Ÿå®ç°
- âœ… ç¬¬4é˜¶æ®µï¼šå¹³å°é€‚é…å™¨å®ç°ï¼ˆç¬¬ä¸€æ‰¹ï¼‰
- âœ… ç¬¬5é˜¶æ®µï¼šæ•°æ®å¤„ç†ä¸APIæœåŠ¡

æœ¬æ–‡æ¡£ç»§ç»­ï¼š
- ğŸ“ ç¬¬6é˜¶æ®µï¼šé«˜çº§å¹³å°é€‚é…å™¨å®ç°
- ğŸ“ ç¬¬7é˜¶æ®µï¼šç›‘æ§è¿ç»´ä¸éƒ¨ç½²

---

# ç¬¬å…­é˜¶æ®µï¼šé«˜çº§å¹³å°é€‚é…å™¨å®ç° [Priority: HIGH] [Time: 7-8å¤©]

## 6.1 å°çº¢ä¹¦é€‚é…å™¨

### 6.1.1 å°çº¢ä¹¦æ•°æ®çˆ¬å–

#### 6.1.1.1 å®ç°å°çº¢ä¹¦é€‚é…å™¨åŸºç¡€ [Time: 5h]
```python
# src/adapters/xiaohongshu/adapter.py
import asyncio
import json
import hashlib
import time
from typing import List, Dict, Any, Optional
from urllib.parse import urlencode, quote
from src.adapters.base.adapter_interface import PlatformAdapter
from src.engines.base.engine_interface import CrawlTask, EngineType
from src.core.anti_detection.fingerprint_manager import FingerprintManager
from src.core.anti_detection.behavior_simulator import BehaviorSimulator
from src.utils.logger.logger import get_logger

logger = get_logger("adapter.xiaohongshu")

class XiaohongshuAdapter(PlatformAdapter):
    """å°çº¢ä¹¦é€‚é…å™¨"""
    
    def __init__(self):
        super().__init__()
        self.platform_name = "xiaohongshu"
        self.base_url = "https://www.xiaohongshu.com"
        self.api_base = "https://edith.xiaohongshu.com/api/sns/web/v1"
        
        # åçˆ¬é…ç½®
        self.fingerprint_manager = FingerprintManager()
        self.behavior_simulator = BehaviorSimulator()
        
        # è¯·æ±‚å¤´é…ç½®
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": "https://www.xiaohongshu.com/",
            "Accept": "application/json, text/plain, */*",
            "Accept-Language": "zh-CN,zh;q=0.9",
            "Origin": "https://www.xiaohongshu.com"
        }
    
    async def search_notes(
        self, 
        keyword: str, 
        page: int = 1, 
        sort_type: str = "general"
    ) -> List[Dict[str, Any]]:
        """æœç´¢ç¬”è®°"""
        try:
            # ç”Ÿæˆç­¾å
            params = {
                "keyword": keyword,
                "page": page,
                "page_size": 20,
                "search_id": self._generate_search_id(),
                "sort": sort_type,
                "note_type": 0
            }
            
            # æ·»åŠ ç­¾åå‚æ•°
            params.update(self._generate_signature(params))
            
            # æ„å»ºURL
            search_url = f"{self.api_base}/homefeed"
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡ - ä½¿ç”¨MediaCrawlå¼•æ“
            task = CrawlTask(
                task_id=f"xhs_search_{keyword}_{page}",
                platform="xiaohongshu",
                url=search_url,
                headers=self.headers,
                params=params,
                engine_type=EngineType.MEDIACRAWL,
                timeout=30
            )
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl_engine.crawl(task)
            
            if result.success and result.json_data:
                return await self._parse_search_results(result.json_data)
            else:
                logger.error(f"Xiaohongshu search failed: {result.error_message}")
                return []
                
        except Exception as e:
            logger.error(f"Xiaohongshu search error: {e}")
            return []
    
    async def get_note_detail(self, note_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¬”è®°è¯¦æƒ…"""
        try:
            # æ„å»ºè¯¦æƒ…URL
            detail_url = f"{self.base_url}/discovery/item/{note_id}"
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡
            task = CrawlTask(
                task_id=f"xhs_detail_{note_id}",
                platform="xiaohongshu",
                url=detail_url,
                headers=self.headers,
                engine_type=EngineType.MEDIACRAWL,
                extraction_rules={
                    "text_selectors": {
                        "title": "h1.title",
                        "content": "div.content",
                        "tags": "span.tag"
                    },
                    "attr_selectors": {
                        "images": {
                            "selector": "div.swiper-slide img",
                            "attribute": "src"
                        }
                    },
                    "js_extractors": {
                        "interaction_data": """
                        (() => {
                            const likeElement = document.querySelector('.like-wrapper');
                            const collectElement = document.querySelector('.collect-wrapper');
                            const commentElement = document.querySelector('.comment-wrapper');
                            
                            return {
                                likes: likeElement ? likeElement.innerText : '0',
                                collects: collectElement ? collectElement.innerText : '0',
                                comments: commentElement ? commentElement.innerText : '0'
                            };
                        })()
                        """
                    }
                },
                timeout=30
            )
            
            # æ¨¡æ‹Ÿäººç±»è¡Œä¸º
            await self.behavior_simulator.simulate_human_behavior(
                task.metadata.get("page"), 
                "scroll"
            )
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl_engine.crawl(task)
            
            if result.success:
                return await self._parse_note_detail(result)
            else:
                logger.error(f"Xiaohongshu detail failed: {result.error_message}")
                return None
                
        except Exception as e:
            logger.error(f"Xiaohongshu detail error: {e}")
            return None
    
    def _generate_search_id(self) -> str:
        """ç”Ÿæˆæœç´¢ID"""
        timestamp = str(int(time.time() * 1000))
        random_str = hashlib.md5(timestamp.encode()).hexdigest()[:8]
        return f"{timestamp}_{random_str}"
    
    def _generate_signature(self, params: Dict[str, Any]) -> Dict[str, str]:
        """ç”Ÿæˆç­¾åå‚æ•°"""
        # å°çº¢ä¹¦çš„ç­¾åç®—æ³•æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
        # å®é™…éœ€è¦é€†å‘å·¥ç¨‹è·å–å®Œæ•´ç®—æ³•
        
        # X-Sç­¾å
        x_s = self._calculate_x_s(params)
        
        # X-Tæ—¶é—´æˆ³
        x_t = str(int(time.time() * 1000))
        
        return {
            "X-S": x_s,
            "X-T": x_t,
            "X-S-Common": self._calculate_x_s_common()
        }
    
    def _calculate_x_s(self, params: Dict[str, Any]) -> str:
        """è®¡ç®—X-Sç­¾å"""
        # ç®€åŒ–çš„ç­¾åç®—æ³•ç¤ºä¾‹
        sorted_params = sorted(params.items())
        param_str = "&".join([f"{k}={v}" for k, v in sorted_params])
        
        # æ·»åŠ ç›å€¼
        salt = "xiaohongshu_salt_2024"
        sign_str = f"{param_str}{salt}"
        
        # è®¡ç®—MD5
        return hashlib.md5(sign_str.encode()).hexdigest()
    
    def _calculate_x_s_common(self) -> str:
        """è®¡ç®—X-S-Commonå‚æ•°"""
        # åŒ…å«è®¾å¤‡ä¿¡æ¯ç­‰
        common_data = {
            "s0": "0",  # å¹³å°ç±»å‹
            "s1": "",   # ä¼šè¯ID
            "x0": "1",  # ç‰ˆæœ¬å·
            "x1": "3.6.8",  # åº”ç”¨ç‰ˆæœ¬
            "x2": "Windows",
            "x3": "xhs-pc-web",
            "x4": "1.4.4",
            "x5": self._generate_device_id(),
            "x6": "main_1.4.4",
            "x7": "0",
            "x8": "0",
            "x9": "10.15.7",
            "x10": "undefined"
        }
        
        return ";".join([f"{k}={v}" for k, v in common_data.items()])
    
    def _generate_device_id(self) -> str:
        """ç”Ÿæˆè®¾å¤‡ID"""
        return hashlib.md5(str(time.time()).encode()).hexdigest()[:16]
    
    async def _parse_search_results(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£ææœç´¢ç»“æœ"""
        notes = []
        
        try:
            if "data" in data and "items" in data["data"]:
                for item in data["data"]["items"]:
                    note_card = item.get("note_card", {})
                    note = await self._extract_note_info(note_card)
                    if note:
                        notes.append(note)
            
            return notes
            
        except Exception as e:
            logger.error(f"Parse search results error: {e}")
            return notes
    
    async def _extract_note_info(self, note_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æå–ç¬”è®°ä¿¡æ¯"""
        try:
            note = {
                "platform": "xiaohongshu",
                "note_id": note_data.get("note_id", ""),
                "title": note_data.get("title", ""),
                "desc": note_data.get("desc", ""),
                "type": note_data.get("type", "normal"),
                "user": {
                    "user_id": note_data.get("user", {}).get("user_id", ""),
                    "nickname": note_data.get("user", {}).get("nickname", ""),
                    "avatar": note_data.get("user", {}).get("avatar", "")
                },
                "interact_info": {
                    "liked_count": note_data.get("interact_info", {}).get("liked_count", 0),
                    "collected_count": note_data.get("interact_info", {}).get("collected_count", 0),
                    "comment_count": note_data.get("interact_info", {}).get("comment_count", 0)
                },
                "image_list": [img.get("url", "") for img in note_data.get("image_list", [])],
                "tag_list": [tag.get("name", "") for tag in note_data.get("tag_list", [])],
                "at_user_list": note_data.get("at_user_list", []),
                "time": note_data.get("time", 0),
                "last_update_time": note_data.get("last_update_time", 0)
            }
            
            # å¦‚æœæ˜¯è§†é¢‘ç¬”è®°ï¼Œæ·»åŠ è§†é¢‘ä¿¡æ¯
            if note["type"] == "video":
                note["video_info"] = {
                    "url": note_data.get("video", {}).get("url", ""),
                    "cover": note_data.get("video", {}).get("cover", {}).get("url", ""),
                    "duration": note_data.get("video", {}).get("duration", 0)
                }
            
            return note
            
        except Exception as e:
            logger.error(f"Extract note info error: {e}")
            return None
    
    async def _parse_note_detail(self, crawl_result) -> Dict[str, Any]:
        """è§£æç¬”è®°è¯¦æƒ…"""
        try:
            detail = {
                "platform": "xiaohongshu",
                "extracted_data": crawl_result.extracted_data,
                "html_content": crawl_result.html,
                "url": crawl_result.url
            }
            
            # ä»æå–çš„æ•°æ®ä¸­è·å–è¯¦ç»†ä¿¡æ¯
            if crawl_result.extracted_data:
                detail.update({
                    "title": crawl_result.extracted_data.get("title", [""])[0],
                    "content": "\n".join(crawl_result.extracted_data.get("content", [])),
                    "tags": crawl_result.extracted_data.get("tags", []),
                    "images": crawl_result.extracted_data.get("images", []),
                    "interaction": crawl_result.extracted_data.get("interaction_data", {})
                })
            
            return detail
            
        except Exception as e:
            logger.error(f"Parse note detail error: {e}")
            return {}
```

**éªŒè¯æ ‡å‡†**ï¼šå°çº¢ä¹¦é€‚é…å™¨åŸºç¡€åŠŸèƒ½å®ç°å®Œæˆï¼Œæ”¯æŒç¬”è®°æœç´¢å’Œè¯¦æƒ…è·å–

#### 6.1.1.2 å®ç°å°çº¢ä¹¦æ•°æ®å¤„ç† [Time: 3h]
```python
# src/adapters/xiaohongshu/processor.py
import re
from typing import Dict, Any, List
from datetime import datetime
from src.processors.cleaner.base_cleaner import DataCleaner
from src.utils.logger.logger import get_logger

logger = get_logger("adapter.xiaohongshu.processor")

class XiaohongshuDataProcessor:
    """å°çº¢ä¹¦æ•°æ®å¤„ç†å™¨"""
    
    def __init__(self):
        self.emoji_pattern = re.compile("[\U00010000-\U0010ffff]", flags=re.UNICODE)
        self.mention_pattern = re.compile(r'@[\w\u4e00-\u9fa5]+')
        self.topic_pattern = re.compile(r'#[^#]+#')
    
    async def process_note(self, note_data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†ç¬”è®°æ•°æ®"""
        try:
            # åŸºç¡€æ¸…æ´—
            processed = await self._clean_basic_info(note_data)
            
            # æå–ç»“æ„åŒ–ä¿¡æ¯
            processed = await self._extract_structured_info(processed)
            
            # è®¡ç®—è´¨é‡åˆ†æ•°
            processed["quality_score"] = await self._calculate_quality_score(processed)
            
            # åˆ†ç±»æ ‡è®°
            processed["categories"] = await self._categorize_note(processed)
            
            return processed
            
        except Exception as e:
            logger.error(f"Process note error: {e}")
            return note_data
    
    async def _clean_basic_info(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…æ´—åŸºç¡€ä¿¡æ¯"""
        # æ¸…ç†æ ‡é¢˜
        if "title" in data:
            data["title"] = self._clean_text(data["title"])
        
        # æ¸…ç†æè¿°
        if "desc" in data:
            data["desc"] = self._clean_text(data["desc"])
        
        # æ¸…ç†å†…å®¹
        if "content" in data:
            data["content"] = self._clean_text(data["content"])
        
        # æ ‡å‡†åŒ–æ—¶é—´
        if "time" in data and isinstance(data["time"], (int, float)):
            data["created_at"] = datetime.fromtimestamp(data["time"] / 1000).isoformat()
        
        return data
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬"""
        if not text:
            return ""
        
        # ä¿ç•™emojiï¼ˆå°çº¢ä¹¦ç‰¹è‰²ï¼‰
        # text = self.emoji_pattern.sub("", text)
        
        # æ¸…ç†å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        
        # æ¸…ç†ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[\x00-\x1F\x7F-\x9F]', '', text)
        
        return text.strip()
    
    async def _extract_structured_info(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æå–ç»“æ„åŒ–ä¿¡æ¯"""
        content = data.get("content", "") + " " + data.get("desc", "")
        
        # æå–@ç”¨æˆ·
        mentions = self.mention_pattern.findall(content)
        data["mentioned_users"] = list(set(mentions))
        
        # æå–è¯é¢˜æ ‡ç­¾
        topics = self.topic_pattern.findall(content)
        data["topics"] = list(set(topics))
        
        # æå–POIä¿¡æ¯
        poi_info = await self._extract_poi_info(content, data.get("tags", []))
        if poi_info:
            data["poi_info"] = poi_info
        
        # æå–ä»·æ ¼ä¿¡æ¯
        prices = await self._extract_prices(content)
        if prices:
            data["mentioned_prices"] = prices
        
        return data
    
    async def _extract_poi_info(self, content: str, tags: List[str]) -> Optional[Dict[str, Any]]:
        """æå–POIä¿¡æ¯"""
        poi_info = {}
        
        # ä»æ ‡ç­¾ä¸­æå–ä½ç½®
        location_keywords = ["ä½äº", "åœ°å€", "åæ ‡", "åœ°ç‚¹"]
        for tag in tags:
            for keyword in location_keywords:
                if keyword in tag:
                    poi_info["location_tag"] = tag
                    break
        
        # ä»å†…å®¹ä¸­æå–åœ°å€
        address_pattern = r'(?:åœ°å€|ä½äº|åœ¨)[ï¼š:]?\s*([^ï¼Œã€‚,\n]+)'
        match = re.search(address_pattern, content)
        if match:
            poi_info["address"] = match.group(1).strip()
        
        # æå–åº—å
        shop_pattern = r'(?:åº—å|é¤å…|é…’åº—|æ°‘å®¿)[ï¼š:]?\s*([^ï¼Œã€‚,\n]+)'
        match = re.search(shop_pattern, content)
        if match:
            poi_info["name"] = match.group(1).strip()
        
        return poi_info if poi_info else None
    
    async def _extract_prices(self, content: str) -> List[Dict[str, Any]]:
        """æå–ä»·æ ¼ä¿¡æ¯"""
        prices = []
        
        # ä»·æ ¼æ¨¡å¼
        price_patterns = [
            r'(?:ï¿¥|Â¥|äººæ°‘å¸|RMB|rmb)\s*(\d+(?:\.\d{1,2})?)',
            r'(\d+(?:\.\d{1,2})?)\s*(?:å…ƒ|å—)',
            r'(?:ä»·æ ¼|ä»·ä½|äººå‡)[ï¼š:]?\s*(\d+(?:\.\d{1,2})?)'
        ]
        
        for pattern in price_patterns:
            matches = re.findall(pattern, content)
            for match in matches:
                try:
                    price = float(match)
                    prices.append({
                        "amount": price,
                        "currency": "CNY",
                        "context": self._get_price_context(content, match)
                    })
                except ValueError:
                    continue
        
        return prices
    
    def _get_price_context(self, content: str, price_str: str) -> str:
        """è·å–ä»·æ ¼ä¸Šä¸‹æ–‡"""
        try:
            index = content.find(price_str)
            start = max(0, index - 20)
            end = min(len(content), index + len(price_str) + 20)
            return content[start:end].strip()
        except:
            return ""
    
    async def _calculate_quality_score(self, data: Dict[str, Any]) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡åˆ†æ•°"""
        score = 0.0
        
        # å†…å®¹é•¿åº¦ï¼ˆ20%ï¼‰
        content_length = len(data.get("content", "")) + len(data.get("desc", ""))
        if content_length > 500:
            score += 0.2
        elif content_length > 200:
            score += 0.15
        elif content_length > 100:
            score += 0.1
        
        # å›¾ç‰‡æ•°é‡ï¼ˆ20%ï¼‰
        image_count = len(data.get("image_list", []))
        if image_count >= 9:
            score += 0.2
        elif image_count >= 6:
            score += 0.15
        elif image_count >= 3:
            score += 0.1
        
        # äº’åŠ¨æ•°æ®ï¼ˆ30%ï¼‰
        interact = data.get("interact_info", {})
        total_interact = (
            interact.get("liked_count", 0) + 
            interact.get("collected_count", 0) * 2 + 
            interact.get("comment_count", 0) * 3
        )
        
        if total_interact > 10000:
            score += 0.3
        elif total_interact > 5000:
            score += 0.25
        elif total_interact > 1000:
            score += 0.2
        elif total_interact > 100:
            score += 0.1
        
        # æ ‡ç­¾å’Œè¯é¢˜ï¼ˆ15%ï¼‰
        tag_count = len(data.get("tag_list", [])) + len(data.get("topics", []))
        if tag_count >= 5:
            score += 0.15
        elif tag_count >= 3:
            score += 0.1
        elif tag_count >= 1:
            score += 0.05
        
        # POIä¿¡æ¯ï¼ˆ15%ï¼‰
        if data.get("poi_info"):
            score += 0.15
        
        return round(score, 2)
    
    async def _categorize_note(self, data: Dict[str, Any]) -> List[str]:
        """åˆ†ç±»ç¬”è®°"""
        categories = []
        
        content = (data.get("content", "") + " " + 
                  data.get("title", "") + " " + 
                  " ".join(data.get("tag_list", [])))
        
        # æ—…æ¸¸ç›¸å…³
        travel_keywords = ["æ—…æ¸¸", "æ—…è¡Œ", "æ”»ç•¥", "æ™¯ç‚¹", "æ‰“å¡", "å‡ºæ¸¸", "è¡Œç¨‹"]
        if any(keyword in content for keyword in travel_keywords):
            categories.append("travel")
        
        # ç¾é£Ÿç›¸å…³
        food_keywords = ["ç¾é£Ÿ", "é¤å…", "å¥½åƒ", "æ¨è", "äººå‡", "å£å‘³", "å¿…åƒ"]
        if any(keyword in content for keyword in food_keywords):
            categories.append("food")
        
        # ä½å®¿ç›¸å…³
        hotel_keywords = ["é…’åº—", "æ°‘å®¿", "ä½å®¿", "å®¢æˆ¿", "å…¥ä½", "é¢„è®¢"]
        if any(keyword in content for keyword in hotel_keywords):
            categories.append("accommodation")
        
        # è´­ç‰©ç›¸å…³
        shopping_keywords = ["è´­ç‰©", "ä¹°", "åº—", "å•†åœº", "ç‰¹äº§", "ä¼´æ‰‹ç¤¼"]
        if any(keyword in content for keyword in shopping_keywords):
            categories.append("shopping")
        
        # é»˜è®¤åˆ†ç±»
        if not categories:
            categories.append("other")
        
        return categories
```

**éªŒè¯æ ‡å‡†**ï¼šå°çº¢ä¹¦æ•°æ®å¤„ç†å™¨å®ç°å®Œæˆï¼Œæ”¯æŒå†…å®¹æ¸…æ´—å’Œç»“æ„åŒ–æå–

## 6.2 æŠ–éŸ³é€‚é…å™¨

### 6.2.1 æŠ–éŸ³è§†é¢‘æ•°æ®çˆ¬å–

#### 6.2.1.1 å®ç°æŠ–éŸ³é€‚é…å™¨åŸºç¡€ [Time: 5h]
```python
# src/adapters/douyin/adapter.py
import asyncio
import json
import re
import time
from typing import List, Dict, Any, Optional
from urllib.parse import urlencode, quote
from src.adapters.base.adapter_interface import PlatformAdapter
from src.engines.base.engine_interface import CrawlTask, EngineType
from src.utils.logger.logger import get_logger

logger = get_logger("adapter.douyin")

class DouyinAdapter(PlatformAdapter):
    """æŠ–éŸ³é€‚é…å™¨"""
    
    def __init__(self):
        super().__init__()
        self.platform_name = "douyin"
        self.base_url = "https://www.douyin.com"
        self.api_base = "https://www.douyin.com/aweme/v1/web"
        
        # è¯·æ±‚å¤´é…ç½®
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": "https://www.douyin.com/",
            "Accept": "application/json, text/plain, */*",
            "Cookie": ""  # éœ€è¦æœ‰æ•ˆçš„cookie
        }
    
    async def search_videos(
        self, 
        keyword: str, 
        offset: int = 0, 
        count: int = 20
    ) -> List[Dict[str, Any]]:
        """æœç´¢è§†é¢‘"""
        try:
            # æ„å»ºæœç´¢å‚æ•°
            params = {
                "device_platform": "webapp",
                "aid": "6383",
                "channel": "channel_pc_web",
                "search_channel": "aweme_video_web",
                "keyword": keyword,
                "search_source": "normal_search",
                "query_correct_type": "1",
                "is_filter_search": "0",
                "from_group_id": "",
                "offset": offset,
                "count": count,
                "version_code": "190600",
                "version_name": "19.6.0"
            }
            
            # æ·»åŠ ç­¾å
            params.update(await self._generate_signature(params))
            
            # æ„å»ºURL
            search_url = f"{self.api_base}/search/item/?{urlencode(params)}"
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡
            task = CrawlTask(
                task_id=f"douyin_search_{keyword}_{offset}",
                platform="douyin",
                url=search_url,
                headers=self.headers,
                engine_type=EngineType.MEDIACRAWL,
                timeout=30
            )
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl_engine.crawl(task)
            
            if result.success and result.json_data:
                return await self._parse_search_results(result.json_data)
            else:
                logger.error(f"Douyin search failed: {result.error_message}")
                return []
                
        except Exception as e:
            logger.error(f"Douyin search error: {e}")
            return []
    
    async def get_video_detail(self, aweme_id: str) -> Optional[Dict[str, Any]]:
        """è·å–è§†é¢‘è¯¦æƒ…"""
        try:
            # æ„å»ºè¯¦æƒ…URL
            detail_url = f"{self.base_url}/video/{aweme_id}"
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡
            task = CrawlTask(
                task_id=f"douyin_detail_{aweme_id}",
                platform="douyin",
                url=detail_url,
                headers=self.headers,
                engine_type=EngineType.MEDIACRAWL,
                extraction_rules={
                    "js_extractors": {
                        "video_data": """
                        (() => {
                            // ä»é¡µé¢ä¸­æå–è§†é¢‘æ•°æ®
                            const scripts = document.querySelectorAll('script');
                            for (let script of scripts) {
                                if (script.innerHTML.includes('RENDER_DATA')) {
                                    const match = script.innerHTML.match(/window\.__RENDER_DATA__\s*=\s*(.+?);\s*<\/script>/);
                                    if (match) {
                                        try {
                                            return JSON.parse(decodeURIComponent(match[1]));
                                        } catch (e) {
                                            console.error('Parse error:', e);
                                        }
                                    }
                                }
                            }
                            return null;
                        })()
                        """
                    }
                },
                timeout=30
            )
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl_engine.crawl(task)
            
            if result.success:
                return await self._parse_video_detail(result)
            else:
                logger.error(f"Douyin detail failed: {result.error_message}")
                return None
                
        except Exception as e:
            logger.error(f"Douyin detail error: {e}")
            return None
    
    async def get_user_videos(
        self, 
        user_id: str, 
        max_cursor: int = 0, 
        count: int = 20
    ) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·è§†é¢‘åˆ—è¡¨"""
        try:
            params = {
                "device_platform": "webapp",
                "aid": "6383",
                "sec_user_id": user_id,
                "max_cursor": max_cursor,
                "count": count,
                "version_code": "190600",
                "version_name": "19.6.0"
            }
            
            # æ·»åŠ ç­¾å
            params.update(await self._generate_signature(params))
            
            # æ„å»ºURL
            user_url = f"{self.api_base}/post/?{urlencode(params)}"
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡
            task = CrawlTask(
                task_id=f"douyin_user_{user_id}_{max_cursor}",
                platform="douyin",
                url=user_url,
                headers=self.headers,
                engine_type=EngineType.MEDIACRAWL,
                timeout=30
            )
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl_engine.crawl(task)
            
            if result.success and result.json_data:
                return await self._parse_user_videos(result.json_data)
            else:
                logger.error(f"Get user videos failed: {result.error_message}")
                return []
                
        except Exception as e:
            logger.error(f"Get user videos error: {e}")
            return []
    
    async def _generate_signature(self, params: Dict[str, Any]) -> Dict[str, str]:
        """ç”Ÿæˆç­¾åå‚æ•°"""
        # æŠ–éŸ³çš„ç­¾åç®—æ³•æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦é€†å‘å·¥ç¨‹
        # è¿™é‡Œç®€åŒ–å¤„ç†
        
        # X-Boguså‚æ•°
        x_bogus = await self._calculate_x_bogus(params)
        
        # msToken
        ms_token = self._generate_ms_token()
        
        return {
            "X-Bogus": x_bogus,
            "msToken": ms_token,
            "_signature": await self._calculate_signature(params)
        }
    
    async def _calculate_x_bogus(self, params: Dict[str, Any]) -> str:
        """è®¡ç®—X-Boguså‚æ•°"""
        # ç®€åŒ–çš„ç®—æ³•ç¤ºä¾‹
        import base64
        
        param_str = urlencode(sorted(params.items()))
        timestamp = str(int(time.time()))
        
        # æ··åˆå‚æ•°å’Œæ—¶é—´æˆ³
        mixed = f"{param_str}{timestamp}"
        
        # Base64ç¼–ç 
        encoded = base64.b64encode(mixed.encode()).decode()
        
        # æ·»åŠ ç‰¹æ®Šå­—ç¬¦
        return f"DFSzswVOQXs7ANkSNTkTNGATOBU0{encoded[:20]}"
    
    def _generate_ms_token(self) -> str:
        """ç”ŸæˆmsToken"""
        import random
        import string
        
        # ç”Ÿæˆ128ä½éšæœºå­—ç¬¦ä¸²
        return ''.join(random.choices(string.ascii_letters + string.digits, k=128))
    
    async def _calculate_signature(self, params: Dict[str, Any]) -> str:
        """è®¡ç®—ç­¾å"""
        # ç®€åŒ–çš„ç­¾åç®—æ³•
        import hashlib
        
        # æŒ‰ç…§ç‰¹å®šé¡ºåºæ’åˆ—å‚æ•°
        sign_params = []
        for key in sorted(params.keys()):
            sign_params.append(f"{key}={params[key]}")
        
        sign_str = "&".join(sign_params)
        
        # æ·»åŠ å¯†é’¥
        secret = "douyin_secret_2024"
        sign_str = f"{sign_str}{secret}"
        
        # è®¡ç®—SHA256
        return hashlib.sha256(sign_str.encode()).hexdigest()
    
    async def _parse_search_results(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£ææœç´¢ç»“æœ"""
        videos = []
        
        try:
            if "data" in data:
                for item in data["data"]:
                    if item.get("type") == 1:  # è§†é¢‘ç±»å‹
                        video = await self._extract_video_info(item.get("aweme_info", {}))
                        if video:
                            videos.append(video)
            
            return videos
            
        except Exception as e:
            logger.error(f"Parse search results error: {e}")
            return videos
    
    async def _extract_video_info(self, video_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æå–è§†é¢‘ä¿¡æ¯"""
        try:
            video = {
                "platform": "douyin",
                "aweme_id": video_data.get("aweme_id", ""),
                "desc": video_data.get("desc", ""),
                "create_time": video_data.get("create_time", 0),
                "author": {
                    "uid": video_data.get("author", {}).get("uid", ""),
                    "nickname": video_data.get("author", {}).get("nickname", ""),
                    "signature": video_data.get("author", {}).get("signature", ""),
                    "avatar": video_data.get("author", {}).get("avatar_thumb", {}).get("url_list", [""])[0]
                },
                "music": {
                    "id": video_data.get("music", {}).get("id", ""),
                    "title": video_data.get("music", {}).get("title", ""),
                    "author": video_data.get("music", {}).get("author", "")
                },
                "statistics": {
                    "comment_count": video_data.get("statistics", {}).get("comment_count", 0),
                    "digg_count": video_data.get("statistics", {}).get("digg_count", 0),
                    "share_count": video_data.get("statistics", {}).get("share_count", 0),
                    "play_count": video_data.get("statistics", {}).get("play_count", 0)
                },
                "video": {
                    "play_addr": video_data.get("video", {}).get("play_addr", {}).get("url_list", [""])[0],
                    "cover": video_data.get("video", {}).get("cover", {}).get("url_list", [""])[0],
                    "duration": video_data.get("video", {}).get("duration", 0),
                    "width": video_data.get("video", {}).get("width", 0),
                    "height": video_data.get("video", {}).get("height", 0)
                },
                "text_extra": video_data.get("text_extra", []),
                "video_labels": video_data.get("video_labels", [])
            }
            
            # æå–è¯é¢˜æ ‡ç­¾
            video["hashtags"] = []
            for extra in video["text_extra"]:
                if extra.get("type") == 1:  # è¯é¢˜ç±»å‹
                    video["hashtags"].append(extra.get("hashtag_name", ""))
            
            return video
            
        except Exception as e:
            logger.error(f"Extract video info error: {e}")
            return None
    
    async def _parse_video_detail(self, crawl_result) -> Dict[str, Any]:
        """è§£æè§†é¢‘è¯¦æƒ…"""
        try:
            detail = {
                "platform": "douyin",
                "url": crawl_result.url
            }
            
            # ä»æå–çš„æ•°æ®ä¸­è·å–è¯¦ç»†ä¿¡æ¯
            if crawl_result.extracted_data and crawl_result.extracted_data.get("video_data"):
                render_data = crawl_result.extracted_data["video_data"]
                
                # è§£ææ¸²æŸ“æ•°æ®
                if render_data and isinstance(render_data, dict):
                    # æå–è§†é¢‘ä¿¡æ¯
                    video_detail = render_data.get("aweme", {}).get("detail", {})
                    if video_detail:
                        detail["video_info"] = await self._extract_video_info(video_detail)
                    
                    # æå–è¯„è®º
                    comments = render_data.get("comment", {}).get("comments", [])
                    detail["top_comments"] = await self._extract_comments(comments[:10])
            
            return detail
            
        except Exception as e:
            logger.error(f"Parse video detail error: {e}")
            return {}
    
    async def _extract_comments(self, comments: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–è¯„è®ºä¿¡æ¯"""
        extracted_comments = []
        
        for comment in comments:
            try:
                extracted = {
                    "cid": comment.get("cid", ""),
                    "text": comment.get("text", ""),
                    "create_time": comment.get("create_time", 0),
                    "digg_count": comment.get("digg_count", 0),
                    "user": {
                        "uid": comment.get("user", {}).get("uid", ""),
                        "nickname": comment.get("user", {}).get("nickname", ""),
                        "avatar": comment.get("user", {}).get("avatar_thumb", {}).get("url_list", [""])[0]
                    },
                    "reply_count": comment.get("reply_comment_count", 0)
                }
                extracted_comments.append(extracted)
            except Exception as e:
                logger.debug(f"Extract comment error: {e}")
                continue
        
        return extracted_comments
    
    async def _parse_user_videos(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£æç”¨æˆ·è§†é¢‘åˆ—è¡¨"""
        videos = []
        
        try:
            if "aweme_list" in data:
                for video_data in data["aweme_list"]:
                    video = await self._extract_video_info(video_data)
                    if video:
                        videos.append(video)
            
            return videos
            
        except Exception as e:
            logger.error(f"Parse user videos error: {e}")
            return videos
```

**éªŒè¯æ ‡å‡†**ï¼šæŠ–éŸ³é€‚é…å™¨åŸºç¡€åŠŸèƒ½å®ç°å®Œæˆï¼Œæ”¯æŒè§†é¢‘æœç´¢ã€è¯¦æƒ…å’Œç”¨æˆ·è§†é¢‘è·å–

## 6.3 å¾®åšé€‚é…å™¨

### 6.3.1 å¾®åšæ•°æ®çˆ¬å–

#### 6.3.1.1 å®ç°å¾®åšé€‚é…å™¨åŸºç¡€ [Time: 4h]
```python
# src/adapters/weibo/adapter.py
import asyncio
import json
import re
import time
from typing import List, Dict, Any, Optional
from urllib.parse import urlencode, quote
from src.adapters.base.adapter_interface import PlatformAdapter
from src.engines.base.engine_interface import CrawlTask, EngineType
from src.utils.logger.logger import get_logger

logger = get_logger("adapter.weibo")

class WeiboAdapter(PlatformAdapter):
    """å¾®åšé€‚é…å™¨"""
    
    def __init__(self):
        super().__init__()
        self.platform_name = "weibo"
        self.base_url = "https://weibo.com"
        self.api_base = "https://weibo.com/ajax"
        
        # è¯·æ±‚å¤´é…ç½®
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": "https://weibo.com/",
            "Accept": "application/json, text/plain, */*",
            "X-Requested-With": "XMLHttpRequest"
        }
    
    async def search_weibo(
        self, 
        keyword: str, 
        page: int = 1,
        search_type: str = "realtime"  # realtime/hot
    ) -> List[Dict[str, Any]]:
        """æœç´¢å¾®åš"""
        try:
            # æ„å»ºæœç´¢å‚æ•°
            params = {
                "q": keyword,
                "page": page,
                "typeall": "1",
                "suball": "1",
                "timescope": "custom:2024-01-01:2024-12-31",
                "Refer": "g",
                "type": "wb" if search_type == "realtime" else "hot"
            }
            
            # æ„å»ºURL
            search_url = f"{self.api_base}/side/search?{urlencode(params)}"
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡
            task = CrawlTask(
                task_id=f"weibo_search_{keyword}_{page}",
                platform="weibo",
                url=search_url,
                headers=self.headers,
                engine_type=EngineType.MEDIACRAWL,
                timeout=30
            )
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl_engine.crawl(task)
            
            if result.success and result.json_data:
                return await self._parse_search_results(result.json_data)
            else:
                logger.error(f"Weibo search failed: {result.error_message}")
                return []
                
        except Exception as e:
            logger.error(f"Weibo search error: {e}")
            return []
    
    async def get_weibo_detail(self, weibo_id: str) -> Optional[Dict[str, Any]]:
        """è·å–å¾®åšè¯¦æƒ…"""
        try:
            # æ„å»ºè¯¦æƒ…URL
            detail_url = f"{self.api_base}/statuses/show?id={weibo_id}"
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡
            task = CrawlTask(
                task_id=f"weibo_detail_{weibo_id}",
                platform="weibo",
                url=detail_url,
                headers=self.headers,
                engine_type=EngineType.CRAWL4AI,
                timeout=30
            )
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl_engine.crawl(task)
            
            if result.success and result.json_data:
                return await self._parse_weibo_detail(result.json_data)
            else:
                logger.error(f"Weibo detail failed: {result.error_message}")
                return None
                
        except Exception as e:
            logger.error(f"Weibo detail error: {e}")
            return None
    
    async def get_user_weibo(
        self, 
        uid: str, 
        page: int = 1,
        feature: int = 0  # 0:å…¨éƒ¨ 1:åŸåˆ› 2:å›¾ç‰‡ 3:è§†é¢‘
    ) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·å¾®åš"""
        try:
            params = {
                "uid": uid,
                "page": page,
                "feature": feature,
                "since_id": ""
            }
            
            # æ„å»ºURL
            user_url = f"{self.api_base}/statuses/mymblog?{urlencode(params)}"
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡
            task = CrawlTask(
                task_id=f"weibo_user_{uid}_{page}",
                platform="weibo",
                url=user_url,
                headers=self.headers,
                engine_type=EngineType.CRAWL4AI,
                timeout=30
            )
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl_engine.crawl(task)
            
            if result.success and result.json_data:
                return await self._parse_user_weibo(result.json_data)
            else:
                logger.error(f"Get user weibo failed: {result.error_message}")
                return []
                
        except Exception as e:
            logger.error(f"Get user weibo error: {e}")
            return []
    
    async def get_comments(
        self, 
        weibo_id: str, 
        max_id: int = 0
    ) -> List[Dict[str, Any]]:
        """è·å–å¾®åšè¯„è®º"""
        try:
            params = {
                "id": weibo_id,
                "mid": weibo_id,
                "max_id": max_id,
                "count": 20,
                "flow": "0"
            }
            
            # æ„å»ºURL
            comment_url = f"{self.api_base}/statuses/buildComments?{urlencode(params)}"
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡
            task = CrawlTask(
                task_id=f"weibo_comments_{weibo_id}_{max_id}",
                platform="weibo",
                url=comment_url,
                headers=self.headers,
                engine_type=EngineType.CRAWL4AI,
                timeout=30
            )
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl_engine.crawl(task)
            
            if result.success and result.json_data:
                return await self._parse_comments(result.json_data)
            else:
                logger.error(f"Get comments failed: {result.error_message}")
                return []
                
        except Exception as e:
            logger.error(f"Get comments error: {e}")
            return []
    
    async def _parse_search_results(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£ææœç´¢ç»“æœ"""
        weibos = []
        
        try:
            if "data" in data and "cards" in data["data"]:
                for card in data["data"]["cards"]:
                    if card.get("card_type") == 9:  # å¾®åšå¡ç‰‡
                        mblog = card.get("mblog", {})
                        weibo = await self._extract_weibo_info(mblog)
                        if weibo:
                            weibos.append(weibo)
            
            return weibos
            
        except Exception as e:
            logger.error(f"Parse search results error: {e}")
            return weibos
    
    async def _extract_weibo_info(self, mblog: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æå–å¾®åšä¿¡æ¯"""
        try:
            # è§£æå¾®åšæ–‡æœ¬
            text, text_length = await self._parse_weibo_text(mblog.get("text", ""))
            
            weibo = {
                "platform": "weibo",
                "id": mblog.get("id", ""),
                "mid": mblog.get("mid", ""),
                "text": text,
                "text_length": text_length,
                "source": mblog.get("source", ""),
                "created_at": mblog.get("created_at", ""),
                "user": {
                    "id": mblog.get("user", {}).get("id", ""),
                    "screen_name": mblog.get("user", {}).get("screen_name", ""),
                    "profile_image_url": mblog.get("user", {}).get("profile_image_url", ""),
                    "verified": mblog.get("user", {}).get("verified", False),
                    "verified_type": mblog.get("user", {}).get("verified_type", -1),
                    "followers_count": mblog.get("user", {}).get("followers_count", 0),
                    "description": mblog.get("user", {}).get("description", "")
                },
                "reposts_count": mblog.get("reposts_count", 0),
                "comments_count": mblog.get("comments_count", 0),
                "attitudes_count": mblog.get("attitudes_count", 0),
                "pic_ids": mblog.get("pic_ids", []),
                "is_long_text": mblog.get("isLongText", False),
                "topics": await self._extract_topics(mblog.get("text", "")),
                "at_users": await self._extract_mentions(mblog.get("text", ""))
            }
            
            # å¤„ç†å›¾ç‰‡
            if mblog.get("pics"):
                weibo["pics"] = [
                    {
                        "pid": pic.get("pid", ""),
                        "url": pic.get("large", {}).get("url", ""),
                        "thumbnail": pic.get("thumbnail", {}).get("url", "")
                    }
                    for pic in mblog.get("pics", [])
                ]
            
            # å¤„ç†è§†é¢‘
            if mblog.get("page_info") and mblog["page_info"].get("type") == "video":
                weibo["video"] = {
                    "url": mblog["page_info"].get("media_info", {}).get("stream_url", ""),
                    "cover": mblog["page_info"].get("page_pic", {}).get("url", ""),
                    "duration": mblog["page_info"].get("media_info", {}).get("duration", 0)
                }
            
            # å¤„ç†è½¬å‘å¾®åš
            if mblog.get("retweeted_status"):
                weibo["retweeted_status"] = await self._extract_weibo_info(mblog["retweeted_status"])
            
            return weibo
            
        except Exception as e:
            logger.error(f"Extract weibo info error: {e}")
            return None
    
    async def _parse_weibo_text(self, html_text: str) -> tuple:
        """è§£æå¾®åšæ–‡æœ¬"""
        # ç§»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', html_text)
        
        # è§£ç HTMLå®ä½“
        import html
        text = html.unescape(text)
        
        # è®¡ç®—æ–‡æœ¬é•¿åº¦ï¼ˆä¸­æ–‡ç®—2ä¸ªå­—ç¬¦ï¼‰
        length = 0
        for char in text:
            if '\u4e00' <= char <= '\u9fff':
                length += 2
            else:
                length += 1
        
        return text, length
    
    async def _extract_topics(self, text: str) -> List[str]:
        """æå–è¯é¢˜"""
        # åŒ¹é…#è¯é¢˜#æ ¼å¼
        pattern = r'#([^#]+)#'
        topics = re.findall(pattern, text)
        return list(set(topics))
    
    async def _extract_mentions(self, text: str) -> List[str]:
        """æå–@ç”¨æˆ·"""
        # åŒ¹é…@ç”¨æˆ·æ ¼å¼
        pattern = r'@([\w\u4e00-\u9fa5-]+)'
        mentions = re.findall(pattern, text)
        return list(set(mentions))
    
    async def _parse_weibo_detail(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """è§£æå¾®åšè¯¦æƒ…"""
        try:
            if not data.get("ok"):
                return {}
            
            mblog = data.get("data", {})
            detail = await self._extract_weibo_info(mblog)
            
            # æ·»åŠ é¢å¤–çš„è¯¦æƒ…ä¿¡æ¯
            if detail:
                detail["long_text"] = mblog.get("longText", {}).get("longTextContent", "")
                detail["edit_count"] = mblog.get("edit_count", 0)
                detail["region_name"] = mblog.get("region_name", "")
                detail["reward_scheme"] = mblog.get("reward_scheme", "")
            
            return detail
            
        except Exception as e:
            logger.error(f"Parse weibo detail error: {e}")
            return {}
    
    async def _parse_user_weibo(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£æç”¨æˆ·å¾®åš"""
        weibos = []
        
        try:
            if "data" in data and "list" in data["data"]:
                for mblog in data["data"]["list"]:
                    weibo = await self._extract_weibo_info(mblog)
                    if weibo:
                        weibos.append(weibo)
            
            return weibos
            
        except Exception as e:
            logger.error(f"Parse user weibo error: {e}")
            return weibos
    
    async def _parse_comments(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£æè¯„è®º"""
        comments = []
        
        try:
            if "data" in data:
                for comment in data["data"]:
                    parsed = {
                        "id": comment.get("id", ""),
                        "mid": comment.get("mid", ""),
                        "text": await self._parse_weibo_text(comment.get("text", ""))[0],
                        "created_at": comment.get("created_at", ""),
                        "source": comment.get("source", ""),
                        "user": {
                            "id": comment.get("user", {}).get("id", ""),
                            "screen_name": comment.get("user", {}).get("screen_name", ""),
                            "profile_image_url": comment.get("user", {}).get("profile_image_url", "")
                        },
                        "like_count": comment.get("like_count", 0),
                        "total_number": comment.get("total_number", 0)
                    }
                    comments.append(parsed)
            
            return comments
            
        except Exception as e:
            logger.error(f"Parse comments error: {e}")
            return comments
```

**éªŒè¯æ ‡å‡†**ï¼šå¾®åšé€‚é…å™¨åŸºç¡€åŠŸèƒ½å®ç°å®Œæˆï¼Œæ”¯æŒå¾®åšæœç´¢ã€è¯¦æƒ…ã€ç”¨æˆ·å¾®åšå’Œè¯„è®ºè·å–

## 6.4 Bç«™é€‚é…å™¨

### 6.4.1 Bç«™è§†é¢‘æ•°æ®çˆ¬å–

#### 6.4.1.1 å®ç°Bç«™é€‚é…å™¨åŸºç¡€ [Time: 4h]
```python
# src/adapters/bilibili/adapter.py
import asyncio
import json
import re
import time
import hashlib
from typing import List, Dict, Any, Optional
from urllib.parse import urlencode, quote
from src.adapters.base.adapter_interface import PlatformAdapter
from src.engines.base.engine_interface import CrawlTask, EngineType
from src.utils.logger.logger import get_logger

logger = get_logger("adapter.bilibili")

class BilibiliAdapter(PlatformAdapter):
    """Bç«™é€‚é…å™¨"""
    
    def __init__(self):
        super().__init__()
        self.platform_name = "bilibili"
        self.base_url = "https://www.bilibili.com"
        self.api_base = "https://api.bilibili.com"
        
        # è¯·æ±‚å¤´é…ç½®
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Referer": "https://www.bilibili.com/",
            "Accept": "application/json, text/plain, */*",
            "Origin": "https://www.bilibili.com"
        }
        
        # WBIç­¾åå¯†é’¥ï¼ˆéœ€è¦å®šæœŸæ›´æ–°ï¼‰
        self.img_key = "7cd084941338484aae1ad9425b84077c"
        self.sub_key = "4932caff0ff746eab6f01bf08b70ac45"
    
    async def search_videos(
        self, 
        keyword: str, 
        page: int = 1,
        order: str = "totalrank",  # ç»¼åˆæ’åº
        duration: int = 0  # æ—¶é•¿ç­›é€‰
    ) -> List[Dict[str, Any]]:
        """æœç´¢è§†é¢‘"""
        try:
            # æ„å»ºæœç´¢å‚æ•°
            params = {
                "keyword": keyword,
                "page": page,
                "page_size": 20,
                "order": order,
                "duration": duration,
                "search_type": "video",
                "platform": "pc"
            }
            
            # æ·»åŠ WBIç­¾å
            params = await self._add_wbi_sign(params)
            
            # æ„å»ºURL
            search_url = f"{self.api_base}/x/web-interface/search/type?{urlencode(params)}"
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡
            task = CrawlTask(
                task_id=f"bilibili_search_{keyword}_{page}",
                platform="bilibili",
                url=search_url,
                headers=self.headers,
                engine_type=EngineType.CRAWL4AI,
                timeout=30
            )
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl_engine.crawl(task)
            
            if result.success and result.json_data:
                return await self._parse_search_results(result.json_data)
            else:
                logger.error(f"Bilibili search failed: {result.error_message}")
                return []
                
        except Exception as e:
            logger.error(f"Bilibili search error: {e}")
            return []
    
    async def get_video_detail(self, bvid: str) -> Optional[Dict[str, Any]]:
        """è·å–è§†é¢‘è¯¦æƒ…"""
        try:
            # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
            params = {"bvid": bvid}
            params = await self._add_wbi_sign(params)
            
            detail_url = f"{self.api_base}/x/web-interface/view?{urlencode(params)}"
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡
            task = CrawlTask(
                task_id=f"bilibili_detail_{bvid}",
                platform="bilibili",
                url=detail_url,
                headers=self.headers,
                engine_type=EngineType.CRAWL4AI,
                timeout=30
            )
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl_engine.crawl(task)
            
            if result.success and result.json_data:
                detail = await self._parse_video_detail(result.json_data)
                
                # è·å–è¯„è®º
                if detail:
                    detail["comments"] = await self.get_comments(detail["aid"])
                
                return detail
            else:
                logger.error(f"Bilibili detail failed: {result.error_message}")
                return None
                
        except Exception as e:
            logger.error(f"Bilibili detail error: {e}")
            return None
    
    async def get_user_videos(
        self, 
        mid: str, 
        pn: int = 1,
        ps: int = 20
    ) -> List[Dict[str, Any]]:
        """è·å–ç”¨æˆ·æŠ•ç¨¿è§†é¢‘"""
        try:
            params = {
                "mid": mid,
                "pn": pn,
                "ps": ps,
                "order": "pubdate",
                "tid": 0
            }
            
            params = await self._add_wbi_sign(params)
            
            user_url = f"{self.api_base}/x/space/arc/search?{urlencode(params)}"
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡
            task = CrawlTask(
                task_id=f"bilibili_user_{mid}_{pn}",
                platform="bilibili",
                url=user_url,
                headers=self.headers,
                engine_type=EngineType.CRAWL4AI,
                timeout=30
            )
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl_engine.crawl(task)
            
            if result.success and result.json_data:
                return await self._parse_user_videos(result.json_data)
            else:
                logger.error(f"Get user videos failed: {result.error_message}")
                return []
                
        except Exception as e:
            logger.error(f"Get user videos error: {e}")
            return []
    
    async def get_comments(
        self, 
        aid: int, 
        pn: int = 1,
        sort: int = 0  # 0:æŒ‰æ—¶é—´ 2:æŒ‰çƒ­åº¦
    ) -> List[Dict[str, Any]]:
        """è·å–è§†é¢‘è¯„è®º"""
        try:
            params = {
                "type": 1,
                "oid": aid,
                "pn": pn,
                "ps": 20,
                "sort": sort,
                "nohot": 0
            }
            
            comment_url = f"{self.api_base}/x/v2/reply/main?{urlencode(params)}"
            
            # åˆ›å»ºçˆ¬å–ä»»åŠ¡
            task = CrawlTask(
                task_id=f"bilibili_comments_{aid}_{pn}",
                platform="bilibili",
                url=comment_url,
                headers=self.headers,
                engine_type=EngineType.CRAWL4AI,
                timeout=30
            )
            
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl_engine.crawl(task)
            
            if result.success and result.json_data:
                return await self._parse_comments(result.json_data)
            else:
                logger.error(f"Get comments failed: {result.error_message}")
                return []
                
        except Exception as e:
            logger.error(f"Get comments error: {e}")
            return []
    
    async def _add_wbi_sign(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """æ·»åŠ WBIç­¾å"""
        # æ·»åŠ æ—¶é—´æˆ³
        wts = str(int(time.time()))
        params["wts"] = wts
        
        # æŒ‰ç…§ç‰¹å®šè§„åˆ™æ’åºå‚æ•°
        sorted_params = sorted(params.items())
        
        # æ„å»ºå¾…ç­¾åå­—ç¬¦ä¸²
        query = urlencode(sorted_params)
        
        # è®¡ç®—ç­¾å
        w_rid = self._calculate_wrid(query)
        
        # æ·»åŠ ç­¾å
        params["w_rid"] = w_rid
        
        return params
    
    def _calculate_wrid(self, query: str) -> str:
        """è®¡ç®—w_ridç­¾å"""
        # æ··æ·†å¯†é’¥
        mixin_key = self._get_mixin_key()
        
        # æ„å»ºç­¾åå­—ç¬¦ä¸²
        sign_str = f"{query}{mixin_key}"
        
        # è®¡ç®—MD5
        return hashlib.md5(sign_str.encode()).hexdigest()
    
    def _get_mixin_key(self) -> str:
        """è·å–æ··æ·†å¯†é’¥"""
        # WBIç­¾åçš„å¯†é’¥æ··æ·†ç®—æ³•
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦æ ¹æ®Bç«™çš„ç®—æ³•åŠ¨æ€ç”Ÿæˆ
        orig = self.img_key + self.sub_key
        
        # å­—ç¬¦æ˜ å°„è¡¨
        char_map = [
            46, 47, 18, 2, 53, 8, 23, 32, 15, 50, 10, 31, 58, 3, 45, 35,
            27, 43, 5, 49, 33, 9, 42, 19, 29, 28, 14, 39, 12, 38, 41, 13,
            37, 48, 7, 16, 24, 55, 40, 61, 26, 17, 0, 1, 60, 51, 30, 4,
            22, 25, 54, 21, 56, 59, 6, 63, 57, 62, 11, 36, 20, 34, 44, 52
        ]
        
        # æ ¹æ®æ˜ å°„è¡¨é‡æ’å­—ç¬¦
        mixin = ""
        for i in char_map[:32]:
            if i < len(orig):
                mixin += orig[i]
        
        return mixin
    
    async def _parse_search_results(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£ææœç´¢ç»“æœ"""
        videos = []
        
        try:
            if data.get("code") == 0 and "result" in data["data"]:
                for item in data["data"]["result"]:
                    video = await self._extract_video_info(item)
                    if video:
                        videos.append(video)
            
            return videos
            
        except Exception as e:
            logger.error(f"Parse search results error: {e}")
            return videos
    
    async def _extract_video_info(self, video_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æå–è§†é¢‘ä¿¡æ¯"""
        try:
            # å¤„ç†æ—¶é•¿æ ¼å¼
            duration_str = video_data.get("duration", "00:00")
            duration_seconds = await self._parse_duration(duration_str)
            
            video = {
                "platform": "bilibili",
                "bvid": video_data.get("bvid", ""),
                "aid": video_data.get("aid", 0),
                "title": re.sub(r'<[^>]+>', '', video_data.get("title", "")),
                "description": video_data.get("description", ""),
                "pic": f"https:{video_data.get('pic', '')}" if not video_data.get('pic', '').startswith('http') else video_data.get('pic', ''),
                "author": video_data.get("author", ""),
                "mid": video_data.get("mid", 0),
                "play": video_data.get("play", 0),
                "danmaku": video_data.get("danmaku", 0),
                "reply": video_data.get("reply", 0),
                "favorite": video_data.get("favorite", 0),
                "coin": video_data.get("coin", 0),
                "share": video_data.get("share", 0),
                "like": video_data.get("like", 0),
                "duration": duration_seconds,
                "pubdate": video_data.get("pubdate", 0),
                "tag": video_data.get("tag", "").split(",") if video_data.get("tag") else [],
                "tname": video_data.get("tname", ""),
                "tid": video_data.get("tid", 0)
            }
            
            return video
            
        except Exception as e:
            logger.error(f"Extract video info error: {e}")
            return None
    
    async def _parse_duration(self, duration_str: str) -> int:
        """è§£ææ—¶é•¿å­—ç¬¦ä¸²ä¸ºç§’æ•°"""
        try:
            parts = duration_str.split(":")
            if len(parts) == 2:
                return int(parts[0]) * 60 + int(parts[1])
            elif len(parts) == 3:
                return int(parts[0]) * 3600 + int(parts[1]) * 60 + int(parts[2])
            else:
                return 0
        except:
            return 0
    
    async def _parse_video_detail(self, data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """è§£æè§†é¢‘è¯¦æƒ…"""
        try:
            if data.get("code") != 0:
                return None
            
            video_data = data["data"]
            
            detail = {
                "platform": "bilibili",
                "bvid": video_data.get("bvid", ""),
                "aid": video_data.get("aid", 0),
                "videos": video_data.get("videos", 1),
                "tid": video_data.get("tid", 0),
                "tname": video_data.get("tname", ""),
                "pic": video_data.get("pic", ""),
                "title": video_data.get("title", ""),
                "pubdate": video_data.get("pubdate", 0),
                "ctime": video_data.get("ctime", 0),
                "desc": video_data.get("desc", ""),
                "duration": video_data.get("duration", 0),
                "owner": {
                    "mid": video_data.get("owner", {}).get("mid", 0),
                    "name": video_data.get("owner", {}).get("name", ""),
                    "face": video_data.get("owner", {}).get("face", "")
                },
                "stat": {
                    "aid": video_data.get("stat", {}).get("aid", 0),
                    "view": video_data.get("stat", {}).get("view", 0),
                    "danmaku": video_data.get("stat", {}).get("danmaku", 0),
                    "reply": video_data.get("stat", {}).get("reply", 0),
                    "favorite": video_data.get("stat", {}).get("favorite", 0),
                    "coin": video_data.get("stat", {}).get("coin", 0),
                    "share": video_data.get("stat", {}).get("share", 0),
                    "like": video_data.get("stat", {}).get("like", 0),
                    "dislike": video_data.get("stat", {}).get("dislike", 0)
                },
                "pages": [
                    {
                        "cid": page.get("cid", 0),
                        "page": page.get("page", 1),
                        "part": page.get("part", ""),
                        "duration": page.get("duration", 0)
                    }
                    for page in video_data.get("pages", [])
                ],
                "tags": [
                    {
                        "tag_id": tag.get("tag_id", 0),
                        "tag_name": tag.get("tag_name", "")
                    }
                    for tag in video_data.get("tag", [])
                ] if isinstance(video_data.get("tag"), list) else []
            }
            
            return detail
            
        except Exception as e:
            logger.error(f"Parse video detail error: {e}")
            return None
    
    async def _parse_user_videos(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£æç”¨æˆ·è§†é¢‘"""
        videos = []
        
        try:
            if data.get("code") == 0 and "vlist" in data["data"]["list"]:
                for video_data in data["data"]["list"]["vlist"]:
                    video = {
                        "platform": "bilibili",
                        "bvid": video_data.get("bvid", ""),
                        "aid": video_data.get("aid", 0),
                        "title": video_data.get("title", ""),
                        "description": video_data.get("description", ""),
                        "pic": f"https:{video_data.get('pic', '')}" if not video_data.get('pic', '').startswith('http') else video_data.get('pic', ''),
                        "author": video_data.get("author", ""),
                        "mid": video_data.get("mid", 0),
                        "play": video_data.get("play", 0),
                        "video_review": video_data.get("video_review", 0),
                        "favorites": video_data.get("favorites", 0),
                        "length": video_data.get("length", ""),
                        "created": video_data.get("created", 0),
                        "comment": video_data.get("comment", 0)
                    }
                    videos.append(video)
            
            return videos
            
        except Exception as e:
            logger.error(f"Parse user videos error: {e}")
            return videos
    
    async def _parse_comments(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """è§£æè¯„è®º"""
        comments = []
        
        try:
            if data.get("code") == 0 and "replies" in data["data"]:
                for reply in data["data"]["replies"]:
                    comment = {
                        "rpid": reply.get("rpid", 0),
                        "oid": reply.get("oid", 0),
                        "type": reply.get("type", 1),
                        "mid": reply.get("mid", 0),
                        "uname": reply.get("member", {}).get("uname", ""),
                        "avatar": reply.get("member", {}).get("avatar", ""),
                        "content": reply.get("content", {}).get("message", ""),
                        "ctime": reply.get("ctime", 0),
                        "like": reply.get("like", 0),
                        "count": reply.get("count", 0),
                        "rcount": reply.get("rcount", 0)
                    }
                    comments.append(comment)
            
            return comments
            
        except Exception as e:
            logger.error(f"Parse comments error: {e}")
            return comments
```

**éªŒè¯æ ‡å‡†**ï¼šBç«™é€‚é…å™¨åŸºç¡€åŠŸèƒ½å®ç°å®Œæˆï¼Œæ”¯æŒè§†é¢‘æœç´¢ã€è¯¦æƒ…ã€ç”¨æˆ·è§†é¢‘å’Œè¯„è®ºè·å–

---

## éªŒè¯å’Œæµ‹è¯•è¦æ±‚

### é˜¶æ®µå…­éªŒè¯æ¸…å•
- [ ] å°çº¢ä¹¦é€‚é…å™¨æµ‹è¯•é€šè¿‡
- [ ] æŠ–éŸ³é€‚é…å™¨æµ‹è¯•é€šè¿‡
- [ ] å¾®åšé€‚é…å™¨æµ‹è¯•é€šè¿‡
- [ ] Bç«™é€‚é…å™¨æµ‹è¯•é€šè¿‡
- [ ] åçˆ¬ç­–ç•¥æœ‰æ•ˆæ€§éªŒè¯

### ä»£ç è´¨é‡è¦æ±‚
- ä»£ç è¦†ç›–ç‡ > 80%
- é€‚é…å™¨æ¥å£ç»Ÿä¸€
- é”™è¯¯å¤„ç†å®Œå–„
- æ•°æ®æ ¼å¼æ ‡å‡†åŒ–

### æ€§èƒ½è¦æ±‚
- å•å¹³å°çˆ¬å–é€Ÿåº¦ > 100æ¡/åˆ†é’Ÿ
- å¹¶å‘æ”¯æŒ > 10ä¸ªä»»åŠ¡
- åçˆ¬æˆåŠŸç‡ > 90%

---

**æ³¨æ„**ï¼šæœ¬æ–‡æ¡£ä¸ºç¬¬ä¸‰éƒ¨åˆ†ï¼ŒåŒ…å«ç¬¬6é˜¶æ®µçš„è¯¦ç»†å®ç°ã€‚ç¬¬7é˜¶æ®µï¼ˆç›‘æ§è¿ç»´ä¸éƒ¨ç½²ï¼‰å°†åœ¨ç¬¬4éƒ¨åˆ†ä¸­è¯¦ç»†è¯´æ˜ã€‚

æ¯ä¸ªä»»åŠ¡éƒ½åŒ…å«å…·ä½“çš„å®ç°ä»£ç ã€éªŒè¯æ ‡å‡†å’Œé¢„ä¼°å·¥æ—¶ï¼Œå¯ä»¥ç›´æ¥ç”¨äºæŒ‡å¯¼AIç¼–ç¨‹åŠ©æ‰‹è¿›è¡Œå¼€å‘å·¥ä½œã€‚