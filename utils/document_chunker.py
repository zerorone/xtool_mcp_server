"""
Document Chunker - æ™ºèƒ½æ–‡æ¡£åˆ†ç‰‡å·¥å…·

å®ç°æ–‡æ¡£çš„æ™ºèƒ½åˆ†ç‰‡åŠŸèƒ½ï¼Œå½“è¾“å‡ºè¶…è¿‡æŒ‡å®š token é™åˆ¶æ—¶ï¼Œ
è‡ªåŠ¨å°†æ–‡æ¡£åˆ†æˆå¤šä¸ªéƒ¨åˆ†ï¼ŒåŒæ—¶ä¿æŒå†…å®¹çš„å®Œæ•´æ€§å’Œå¯è¯»æ€§ã€‚

ä¸»è¦ç‰¹æ€§ï¼š
1. æ™ºèƒ½æ–­ç‚¹æ£€æµ‹ - åœ¨æ®µè½ã€ç« èŠ‚è¾¹ç•Œåˆ†å‰²
2. ä»£ç å—ä¿æŠ¤ - ç¡®ä¿ä»£ç å—ä¸è¢«ä¸­æ–­
3. åˆ—è¡¨å®Œæ•´æ€§ - ä¿æŒåˆ—è¡¨é¡¹çš„å®Œæ•´
4. æ ‡è®°æ·»åŠ  - è‡ªåŠ¨æ·»åŠ åˆ†ç‰‡æ ‡è®°ï¼ˆPart X/Yï¼‰
5. Token è®¡ç®— - å‡†ç¡®è®¡ç®—æ¯ä¸ªåˆ†ç‰‡çš„ token æ•°
"""

import logging
import re
from dataclasses import dataclass

from utils.token_utils import estimate_tokens

logger = logging.getLogger(__name__)


@dataclass
class DocumentChunk:
    """è¡¨ç¤ºæ–‡æ¡£çš„ä¸€ä¸ªåˆ†ç‰‡"""

    content: str
    part_number: int
    total_parts: int
    token_count: int
    start_line: int
    end_line: int

    @property
    def header(self) -> str:
        """ç”Ÿæˆåˆ†ç‰‡å¤´éƒ¨æ ‡è®°"""
        return f"\n{'=' * 60}\nğŸ“„ Document Part {self.part_number}/{self.total_parts}\n{'=' * 60}\n"

    @property
    def footer(self) -> str:
        """ç”Ÿæˆåˆ†ç‰‡å°¾éƒ¨æ ‡è®°"""
        if self.part_number < self.total_parts:
            return f"\n{'=' * 60}\nâ¡ï¸ Continued in Part {self.part_number + 1}/{self.total_parts}\n{'=' * 60}\n"
        else:
            return f"\n{'=' * 60}\nâœ… End of Document (Total {self.total_parts} parts)\n{'=' * 60}\n"

    def format(self) -> str:
        """æ ¼å¼åŒ–åˆ†ç‰‡å†…å®¹ï¼ŒåŒ…å«å¤´éƒ¨å’Œå°¾éƒ¨æ ‡è®°"""
        return f"{self.header}{self.content}{self.footer}"


class DocumentChunker:
    """
    æ™ºèƒ½æ–‡æ¡£åˆ†ç‰‡å™¨

    å°†é•¿æ–‡æ¡£æ™ºèƒ½åˆ†å‰²æˆå¤šä¸ªéƒ¨åˆ†ï¼Œä¿æŒå†…å®¹å®Œæ•´æ€§ã€‚
    """

    # é»˜è®¤é…ç½®
    DEFAULT_MAX_TOKENS = 30000  # é»˜è®¤æœ€å¤§ token æ•°
    MIN_CHUNK_TOKENS = 5000  # æœ€å°åˆ†ç‰‡ token æ•°
    HEADER_FOOTER_TOKENS = 100  # é¢„ç•™ç»™å¤´éƒ¨å’Œå°¾éƒ¨æ ‡è®°çš„ token æ•°

    # ä»£ç å—å’Œç‰¹æ®Šå—çš„æ­£åˆ™è¡¨è¾¾å¼
    CODE_BLOCK_PATTERN = re.compile(r"```[\s\S]*?```", re.MULTILINE)
    FENCED_BLOCK_PATTERN = re.compile(r"(^|\n)(```|~~~)[\s\S]*?\2", re.MULTILINE)
    TABLE_PATTERN = re.compile(r"^\|.*\|$", re.MULTILINE)

    def __init__(self, max_tokens: int = DEFAULT_MAX_TOKENS):
        """
        åˆå§‹åŒ–æ–‡æ¡£åˆ†ç‰‡å™¨

        Args:
            max_tokens: æ¯ä¸ªåˆ†ç‰‡çš„æœ€å¤§ token æ•°
        """
        self.max_tokens = max_tokens
        self.effective_max_tokens = max_tokens - self.HEADER_FOOTER_TOKENS

    def should_chunk(self, content: str) -> bool:
        """
        åˆ¤æ–­å†…å®¹æ˜¯å¦éœ€è¦åˆ†ç‰‡

        Args:
            content: è¦æ£€æŸ¥çš„å†…å®¹

        Returns:
            bool: True å¦‚æœéœ€è¦åˆ†ç‰‡ï¼ŒFalse å¦åˆ™
        """
        token_count = estimate_tokens(content)
        return token_count > self.max_tokens

    def chunk_document(self, content: str) -> list[DocumentChunk]:
        """
        å°†æ–‡æ¡£åˆ†æˆå¤šä¸ªæ™ºèƒ½åˆ†ç‰‡

        Args:
            content: è¦åˆ†ç‰‡çš„æ–‡æ¡£å†…å®¹

        Returns:
            List[DocumentChunk]: æ–‡æ¡£åˆ†ç‰‡åˆ—è¡¨
        """
        # å¦‚æœä¸éœ€è¦åˆ†ç‰‡ï¼Œç›´æ¥è¿”å›
        if not self.should_chunk(content):
            token_count = estimate_tokens(content)
            return [
                DocumentChunk(
                    content=content,
                    part_number=1,
                    total_parts=1,
                    token_count=token_count,
                    start_line=1,
                    end_line=len(content.splitlines()),
                )
            ]

        # å°†å†…å®¹æŒ‰è¡Œåˆ†å‰²
        lines = content.splitlines(keepends=True)

        # è¯†åˆ«ç‰¹æ®Šå—ï¼ˆä»£ç å—ã€è¡¨æ ¼ç­‰ï¼‰
        special_blocks = self._identify_special_blocks(lines)

        # æ™ºèƒ½åˆ†ç‰‡
        chunks = self._create_chunks(lines, special_blocks)

        # è®¾ç½®æ€»ç‰‡æ•°
        total_parts = len(chunks)
        for chunk in chunks:
            chunk.total_parts = total_parts

        logger.info(f"Document chunked into {total_parts} parts")
        return chunks

    def _identify_special_blocks(self, lines: list[str]) -> list[tuple[int, int, str]]:
        """
        è¯†åˆ«æ–‡æ¡£ä¸­çš„ç‰¹æ®Šå—ï¼ˆä»£ç å—ã€è¡¨æ ¼ç­‰ï¼‰

        Args:
            lines: æ–‡æ¡£è¡Œåˆ—è¡¨

        Returns:
            List[Tuple[int, int, str]]: (å¼€å§‹è¡Œ, ç»“æŸè¡Œ, ç±»å‹) çš„åˆ—è¡¨
        """
        special_blocks = []
        i = 0

        while i < len(lines):
            line = lines[i]

            # æ£€æŸ¥ä»£ç å—
            if line.strip().startswith("```") or line.strip().startswith("~~~"):
                fence = line.strip()[:3]
                start = i
                i += 1
                while i < len(lines) and not lines[i].strip().startswith(fence):
                    i += 1
                if i < len(lines):
                    special_blocks.append((start, i, "code"))

            # æ£€æŸ¥è¡¨æ ¼
            elif "|" in line and i + 1 < len(lines) and re.match(r"^[\s\-\|]+$", lines[i + 1]):
                start = i
                i += 2  # è·³è¿‡è¡¨å¤´å’Œåˆ†éš”ç¬¦
                while i < len(lines) and "|" in lines[i]:
                    i += 1
                special_blocks.append((start, i - 1, "table"))
                continue

            i += 1

        return special_blocks

    def _create_chunks(self, lines: list[str], special_blocks: list[tuple[int, int, str]]) -> list[DocumentChunk]:
        """
        æ ¹æ®æ™ºèƒ½æ–­ç‚¹åˆ›å»ºæ–‡æ¡£åˆ†ç‰‡

        Args:
            lines: æ–‡æ¡£è¡Œåˆ—è¡¨
            special_blocks: ç‰¹æ®Šå—åˆ—è¡¨

        Returns:
            List[DocumentChunk]: åˆ†ç‰‡åˆ—è¡¨
        """
        chunks = []
        current_chunk_lines = []
        current_tokens = 0
        chunk_start_line = 1
        part_number = 1

        for i, line in enumerate(lines):
            line_tokens = estimate_tokens(line)

            # æ£€æŸ¥æ˜¯å¦åœ¨ç‰¹æ®Šå—å†…
            in_special_block = any(start <= i <= end for start, end, _ in special_blocks)

            # å¦‚æœæ·»åŠ è¿™ä¸€è¡Œä¼šè¶…è¿‡é™åˆ¶
            if current_tokens + line_tokens > self.effective_max_tokens and current_chunk_lines:
                # å¦‚æœåœ¨ç‰¹æ®Šå—å†…ï¼Œéœ€è¦åŒ…å«æ•´ä¸ªå—
                if in_special_block:
                    # æ‰¾åˆ°åŒ…å«å½“å‰è¡Œçš„ç‰¹æ®Šå—
                    for start, end, _block_type in special_blocks:
                        if start <= i <= end:
                            # å¦‚æœå½“å‰åˆ†ç‰‡å·²ç»åŒ…å«äº†å—çš„å¼€å§‹ï¼Œç»§ç»­æ·»åŠ åˆ°å—ç»“æŸ
                            if any(
                                start <= j < len(current_chunk_lines) + chunk_start_line - 1 for j in range(start, i)
                            ):
                                current_chunk_lines.append(line)
                                current_tokens += line_tokens
                                continue
                            else:
                                # å¦åˆ™ï¼Œåœ¨å—ä¹‹å‰æ–­å¼€
                                break

                # æŸ¥æ‰¾æœ€ä½³æ–­ç‚¹
                if not in_special_block:
                    best_break = self._find_best_break_point(current_chunk_lines)

                    # åˆ›å»ºåˆ†ç‰‡
                    chunk_content = "".join(current_chunk_lines[:best_break])
                    chunk = DocumentChunk(
                        content=chunk_content.rstrip(),
                        part_number=part_number,
                        total_parts=0,  # ç¨åè®¾ç½®
                        token_count=estimate_tokens(chunk_content),
                        start_line=chunk_start_line,
                        end_line=chunk_start_line + best_break - 1,
                    )
                    chunks.append(chunk)

                    # å‡†å¤‡ä¸‹ä¸€ä¸ªåˆ†ç‰‡
                    part_number += 1
                    current_chunk_lines = current_chunk_lines[best_break:] + [line]
                    chunk_start_line = chunk_start_line + best_break
                    current_tokens = estimate_tokens("".join(current_chunk_lines))
                    continue

            # æ·»åŠ å½“å‰è¡Œ
            current_chunk_lines.append(line)
            current_tokens += line_tokens

        # å¤„ç†å‰©ä½™çš„å†…å®¹
        if current_chunk_lines:
            chunk_content = "".join(current_chunk_lines)
            chunk = DocumentChunk(
                content=chunk_content.rstrip(),
                part_number=part_number,
                total_parts=0,  # ç¨åè®¾ç½®
                token_count=estimate_tokens(chunk_content),
                start_line=chunk_start_line,
                end_line=chunk_start_line + len(current_chunk_lines) - 1,
            )
            chunks.append(chunk)

        return chunks

    def _find_best_break_point(self, lines: list[str]) -> int:
        """
        åœ¨è¡Œåˆ—è¡¨ä¸­æ‰¾åˆ°æœ€ä½³æ–­ç‚¹

        ä¼˜å…ˆçº§ï¼š
        1. ç« èŠ‚è¾¹ç•Œï¼ˆ# æ ‡é¢˜ï¼‰
        2. ç©ºè¡Œ
        3. æ®µè½ç»“æŸï¼ˆå¥å·ã€é—®å·ã€æ„Ÿå¹å·ï¼‰
        4. åˆ—è¡¨é¡¹è¾¹ç•Œ
        5. ä»»æ„è¡Œ

        Args:
            lines: è¦æ£€æŸ¥çš„è¡Œåˆ—è¡¨

        Returns:
            int: æœ€ä½³æ–­ç‚¹ç´¢å¼•ï¼ˆåœ¨æ­¤ä¹‹å‰æ–­å¼€ï¼‰
        """
        # ä»åå‘å‰æŸ¥æ‰¾ï¼Œä½†ä¸è¦å¤ªé å‰ï¼ˆè‡³å°‘ä¿ç•™ 20% çš„å†…å®¹ï¼‰
        min_index = max(1, len(lines) // 5)

        # ä¼˜å…ˆçº§ 1: ç« èŠ‚è¾¹ç•Œ
        for i in range(len(lines) - 1, min_index - 1, -1):
            if lines[i].strip().startswith("#") and i > 0 and lines[i - 1].strip() == "":
                return i

        # ä¼˜å…ˆçº§ 2: ç©ºè¡Œ
        for i in range(len(lines) - 1, min_index - 1, -1):
            if lines[i].strip() == "" and i < len(lines) - 1:
                return i + 1

        # ä¼˜å…ˆçº§ 3: æ®µè½ç»“æŸ
        for i in range(len(lines) - 1, min_index - 1, -1):
            line = lines[i].strip()
            if line and (
                line.endswith(".")
                or line.endswith("ã€‚")
                or line.endswith("?")
                or line.endswith("ï¼Ÿ")
                or line.endswith("!")
                or line.endswith("ï¼")
            ):
                return i + 1

        # ä¼˜å…ˆçº§ 4: åˆ—è¡¨é¡¹è¾¹ç•Œ
        for i in range(len(lines) - 1, min_index - 1, -1):
            if re.match(r"^[\s]*[-*+â€¢]\s", lines[i]) or re.match(r"^[\s]*\d+\.\s", lines[i]):
                return i

        # é»˜è®¤ï¼šåœ¨ 80% å¤„æ–­å¼€
        return int(len(lines) * 0.8)

    def format_chunks(self, chunks: list[DocumentChunk]) -> list[str]:
        """
        æ ¼å¼åŒ–æ‰€æœ‰åˆ†ç‰‡ï¼Œæ·»åŠ æ ‡è®°

        Args:
            chunks: åˆ†ç‰‡åˆ—è¡¨

        Returns:
            List[str]: æ ¼å¼åŒ–åçš„åˆ†ç‰‡å†…å®¹åˆ—è¡¨
        """
        return [chunk.format() for chunk in chunks]


# ä¾¿æ·å‡½æ•°
def chunk_document(content: str, max_tokens: int = 30000) -> list[str]:
    """
    ä¾¿æ·å‡½æ•°ï¼šå°†æ–‡æ¡£åˆ†ç‰‡å¹¶æ ¼å¼åŒ–

    Args:
        content: è¦åˆ†ç‰‡çš„æ–‡æ¡£å†…å®¹
        max_tokens: æ¯ä¸ªåˆ†ç‰‡çš„æœ€å¤§ token æ•°

    Returns:
        List[str]: æ ¼å¼åŒ–åçš„åˆ†ç‰‡å†…å®¹åˆ—è¡¨
    """
    chunker = DocumentChunker(max_tokens)
    chunks = chunker.chunk_document(content)
    return chunker.format_chunks(chunks)


def estimate_chunk_count(content: str, max_tokens: int = 30000) -> int:
    """
    ä¼°ç®—æ–‡æ¡£éœ€è¦åˆ†æˆå¤šå°‘ç‰‡

    Args:
        content: æ–‡æ¡£å†…å®¹
        max_tokens: æ¯ä¸ªåˆ†ç‰‡çš„æœ€å¤§ token æ•°

    Returns:
        int: ä¼°è®¡çš„åˆ†ç‰‡æ•°
    """
    total_tokens = estimate_tokens(content)
    effective_max = max_tokens - DocumentChunker.HEADER_FOOTER_TOKENS
    return max(1, (total_tokens + effective_max - 1) // effective_max)
