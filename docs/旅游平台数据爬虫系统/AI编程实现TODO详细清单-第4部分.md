# æ—…æ¸¸å¹³å°æ•°æ®çˆ¬è™«ç³»ç»Ÿ - AIç¼–ç¨‹å®ç°TODOè¯¦ç»†æ¸…å•ï¼ˆç¬¬4éƒ¨åˆ†ï¼‰

> æœ¬æ–‡æ¡£ä¸ºAIç¼–ç¨‹å®ç°TODOè¯¦ç»†æ¸…å•çš„ç¬¬å››éƒ¨åˆ†ï¼ŒåŒ…å«ç¬¬7é˜¶æ®µçš„è¯¦ç»†ä»»åŠ¡

## å‰æƒ…æè¦

å·²å®Œæˆéƒ¨åˆ†ï¼š
- âœ… ç¬¬1é˜¶æ®µï¼šé¡¹ç›®åˆå§‹åŒ–ä¸åŸºç¡€æ¶æ„
- âœ… ç¬¬2é˜¶æ®µï¼šåŒå¼•æ“æ ¸å¿ƒæ¶æ„
- âœ… ç¬¬3é˜¶æ®µï¼šåçˆ¬ç³»ç»Ÿå®ç°
- âœ… ç¬¬4é˜¶æ®µï¼šå¹³å°é€‚é…å™¨å®ç°ï¼ˆç¬¬ä¸€æ‰¹ï¼‰
- âœ… ç¬¬5é˜¶æ®µï¼šæ•°æ®å¤„ç†ä¸APIæœåŠ¡
- âœ… ç¬¬6é˜¶æ®µï¼šé«˜çº§å¹³å°é€‚é…å™¨å®ç°

æœ¬æ–‡æ¡£ç»§ç»­ï¼š
- ğŸ“ ç¬¬7é˜¶æ®µï¼šç›‘æ§è¿ç»´ä¸éƒ¨ç½²

---

# ç¬¬ä¸ƒé˜¶æ®µï¼šç›‘æ§è¿ç»´ä¸éƒ¨ç½² [Priority: HIGH] [Time: 5-6å¤©]

## 7.1 Prometheusç›‘æ§ç³»ç»Ÿ

### 7.1.1 ç›‘æ§æŒ‡æ ‡è®¾è®¡

#### 7.1.1.1 åˆ›å»ºç›‘æ§æŒ‡æ ‡æ”¶é›†å™¨ [Time: 3h]
```python
# src/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, Info, Summary
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
from typing import Dict, Any
import time
from functools import wraps
from src.utils.logger.logger import get_logger

logger = get_logger("monitoring.metrics")

# ç³»ç»Ÿä¿¡æ¯
system_info = Info('crawler_system_info', 'Crawler system information')
system_info.info({
    'version': '1.0.0',
    'environment': 'production',
    'python_version': '3.11'
})

# çˆ¬å–ä»»åŠ¡æŒ‡æ ‡
crawl_requests_total = Counter(
    'crawler_requests_total',
    'Total number of crawl requests',
    ['platform', 'engine', 'status']
)

crawl_duration_seconds = Histogram(
    'crawler_request_duration_seconds',
    'Crawl request duration in seconds',
    ['platform', 'engine'],
    buckets=(0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0)
)

# æ•°æ®å¤„ç†æŒ‡æ ‡
data_processed_total = Counter(
    'crawler_data_processed_total',
    'Total number of data items processed',
    ['platform', 'processor_type', 'status']
)

data_processing_duration = Summary(
    'crawler_data_processing_duration_seconds',
    'Data processing duration in seconds',
    ['processor_type']
)

# ç³»ç»Ÿèµ„æºæŒ‡æ ‡
active_crawlers = Gauge(
    'crawler_active_engines',
    'Number of active crawler engines',
    ['engine_type']
)

proxy_pool_size = Gauge(
    'crawler_proxy_pool_size',
    'Size of proxy pool',
    ['status']  # available, working, failed
)

redis_connections = Gauge(
    'crawler_redis_connections',
    'Number of Redis connections'
)

db_connections = Gauge(
    'crawler_db_connections',
    'Number of database connections',
    ['state']  # active, idle
)

# APIæŒ‡æ ‡
api_requests_total = Counter(
    'crawler_api_requests_total',
    'Total number of API requests',
    ['method', 'endpoint', 'status_code']
)

api_request_duration = Histogram(
    'crawler_api_request_duration_seconds',
    'API request duration in seconds',
    ['method', 'endpoint'],
    buckets=(0.01, 0.05, 0.1, 0.5, 1.0, 2.0, 5.0)
)

# é”™è¯¯æŒ‡æ ‡
error_total = Counter(
    'crawler_errors_total',
    'Total number of errors',
    ['error_type', 'platform', 'component']
)

# ä¸šåŠ¡æŒ‡æ ‡
poi_discovered_total = Counter(
    'crawler_poi_discovered_total',
    'Total number of POIs discovered',
    ['platform', 'city']
)

data_quality_score = Histogram(
    'crawler_data_quality_score',
    'Data quality score distribution',
    ['platform'],
    buckets=(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0)
)

class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨"""
    
    def __init__(self):
        self.start_time = time.time()
    
    def track_crawl_request(self, platform: str, engine: str, status: str, duration: float):
        """è¿½è¸ªçˆ¬å–è¯·æ±‚"""
        crawl_requests_total.labels(
            platform=platform,
            engine=engine,
            status=status
        ).inc()
        
        if status == "success":
            crawl_duration_seconds.labels(
                platform=platform,
                engine=engine
            ).observe(duration)
    
    def track_data_processing(self, platform: str, processor_type: str, status: str, duration: float):
        """è¿½è¸ªæ•°æ®å¤„ç†"""
        data_processed_total.labels(
            platform=platform,
            processor_type=processor_type,
            status=status
        ).inc()
        
        data_processing_duration.labels(
            processor_type=processor_type
        ).observe(duration)
    
    def track_api_request(self, method: str, endpoint: str, status_code: int, duration: float):
        """è¿½è¸ªAPIè¯·æ±‚"""
        api_requests_total.labels(
            method=method,
            endpoint=endpoint,
            status_code=str(status_code)
        ).inc()
        
        api_request_duration.labels(
            method=method,
            endpoint=endpoint
        ).observe(duration)
    
    def track_error(self, error_type: str, platform: str = "unknown", component: str = "unknown"):
        """è¿½è¸ªé”™è¯¯"""
        error_total.labels(
            error_type=error_type,
            platform=platform,
            component=component
        ).inc()
    
    def update_system_metrics(self, metrics: Dict[str, Any]):
        """æ›´æ–°ç³»ç»ŸæŒ‡æ ‡"""
        # æ›´æ–°æ´»è·ƒçˆ¬è™«æ•°
        if "active_crawlers" in metrics:
            for engine_type, count in metrics["active_crawlers"].items():
                active_crawlers.labels(engine_type=engine_type).set(count)
        
        # æ›´æ–°ä»£ç†æ± å¤§å°
        if "proxy_pool" in metrics:
            for status, count in metrics["proxy_pool"].items():
                proxy_pool_size.labels(status=status).set(count)
        
        # æ›´æ–°è¿æ¥æ•°
        if "redis_connections" in metrics:
            redis_connections.set(metrics["redis_connections"])
        
        if "db_connections" in metrics:
            for state, count in metrics["db_connections"].items():
                db_connections.labels(state=state).set(count)
    
    def track_poi_discovered(self, platform: str, city: str, count: int = 1):
        """è¿½è¸ªå‘ç°çš„POI"""
        poi_discovered_total.labels(
            platform=platform,
            city=city
        ).inc(count)
    
    def track_data_quality(self, platform: str, score: float):
        """è¿½è¸ªæ•°æ®è´¨é‡"""
        data_quality_score.labels(platform=platform).observe(score)

# å…¨å±€æŒ‡æ ‡æ”¶é›†å™¨
metrics_collector = MetricsCollector()

# è£…é¥°å™¨å‡½æ•°
def track_time(metric_type: str, **labels):
    """è¿½è¸ªæ‰§è¡Œæ—¶é—´çš„è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                status = "success"
                return result
            except Exception as e:
                status = "error"
                raise
            finally:
                duration = time.time() - start_time
                
                if metric_type == "crawl":
                    metrics_collector.track_crawl_request(
                        platform=labels.get("platform", "unknown"),
                        engine=labels.get("engine", "unknown"),
                        status=status,
                        duration=duration
                    )
                elif metric_type == "api":
                    metrics_collector.track_api_request(
                        method=labels.get("method", "GET"),
                        endpoint=labels.get("endpoint", "/"),
                        status_code=200 if status == "success" else 500,
                        duration=duration
                    )
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                status = "success"
                return result
            except Exception as e:
                status = "error"
                raise
            finally:
                duration = time.time() - start_time
                # åŒæ ·çš„è¿½è¸ªé€»è¾‘
        
        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
    
    return decorator

# src/monitoring/prometheus_server.py
from fastapi import FastAPI, Response
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
from src.monitoring.metrics import metrics_collector
import uvicorn

app = FastAPI(title="Crawler Metrics Server")

@app.get("/metrics")
async def get_metrics():
    """Prometheus metrics endpoint"""
    metrics_data = generate_latest()
    return Response(content=metrics_data, media_type=CONTENT_TYPE_LATEST)

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}

def start_metrics_server(host: str = "0.0.0.0", port: int = 9090):
    """å¯åŠ¨æŒ‡æ ‡æœåŠ¡å™¨"""
    uvicorn.run(app, host=host, port=port)
```

**éªŒè¯æ ‡å‡†**ï¼šç›‘æ§æŒ‡æ ‡æ”¶é›†å™¨åˆ›å»ºå®Œæˆï¼Œæ”¯æŒå¤šç»´åº¦æŒ‡æ ‡æ”¶é›†

#### 7.1.1.2 å®ç°ç›‘æ§ä¸­é—´ä»¶ [Time: 2h]
```python
# src/api/middleware/monitoring.py
import time
from fastapi import Request
from starlette.middleware.base import BaseHTTPMiddleware
from src.monitoring.metrics import metrics_collector
from src.utils.logger.logger import get_logger

logger = get_logger("api.middleware.monitoring")

class MonitoringMiddleware(BaseHTTPMiddleware):
    """ç›‘æ§ä¸­é—´ä»¶"""
    
    async def dispatch(self, request: Request, call_next):
        start_time = time.time()
        
        # æå–è¯·æ±‚ä¿¡æ¯
        method = request.method
        path = request.url.path
        
        try:
            # å¤„ç†è¯·æ±‚
            response = await call_next(request)
            
            # è®°å½•æŒ‡æ ‡
            duration = time.time() - start_time
            metrics_collector.track_api_request(
                method=method,
                endpoint=path,
                status_code=response.status_code,
                duration=duration
            )
            
            # æ·»åŠ å“åº”å¤´
            response.headers["X-Response-Time"] = f"{duration:.3f}"
            
            return response
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            duration = time.time() - start_time
            metrics_collector.track_api_request(
                method=method,
                endpoint=path,
                status_code=500,
                duration=duration
            )
            
            metrics_collector.track_error(
                error_type=type(e).__name__,
                component="api",
                platform="unknown"
            )
            
            logger.error(f"Request failed: {method} {path} - {e}")
            raise

# src/engines/middleware/monitoring.py
from typing import Dict, Any
from src.engines.base.engine_interface import CrawlTask, CrawlResult
from src.monitoring.metrics import metrics_collector

class EngineMonitoringMixin:
    """å¼•æ“ç›‘æ§æ··å…¥ç±»"""
    
    async def crawl_with_monitoring(self, task: CrawlTask) -> CrawlResult:
        """å¸¦ç›‘æ§çš„çˆ¬å–"""
        start_time = time.time()
        
        try:
            # æ‰§è¡Œçˆ¬å–
            result = await self.crawl(task)
            
            # è®°å½•æŒ‡æ ‡
            duration = time.time() - start_time
            status = "success" if result.success else "failed"
            
            metrics_collector.track_crawl_request(
                platform=task.platform,
                engine=self.engine_type.value,
                status=status,
                duration=duration
            )
            
            return result
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            duration = time.time() - start_time
            
            metrics_collector.track_crawl_request(
                platform=task.platform,
                engine=self.engine_type.value,
                status="error",
                duration=duration
            )
            
            metrics_collector.track_error(
                error_type=type(e).__name__,
                platform=task.platform,
                component=f"engine.{self.engine_type.value}"
            )
            
            raise

# src/processors/middleware/monitoring.py
class ProcessorMonitoringMixin:
    """å¤„ç†å™¨ç›‘æ§æ··å…¥ç±»"""
    
    async def process_with_monitoring(
        self, 
        data: Dict[str, Any], 
        processor_type: str
    ) -> Dict[str, Any]:
        """å¸¦ç›‘æ§çš„æ•°æ®å¤„ç†"""
        start_time = time.time()
        
        try:
            # æ‰§è¡Œå¤„ç†
            result = await self.process(data)
            
            # è®°å½•æŒ‡æ ‡
            duration = time.time() - start_time
            
            metrics_collector.track_data_processing(
                platform=data.get("platform", "unknown"),
                processor_type=processor_type,
                status="success",
                duration=duration
            )
            
            # è®°å½•æ•°æ®è´¨é‡
            if "quality_score" in result:
                metrics_collector.track_data_quality(
                    platform=data.get("platform", "unknown"),
                    score=result["quality_score"]
                )
            
            return result
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            duration = time.time() - start_time
            
            metrics_collector.track_data_processing(
                platform=data.get("platform", "unknown"),
                processor_type=processor_type,
                status="error",
                duration=duration
            )
            
            metrics_collector.track_error(
                error_type=type(e).__name__,
                platform=data.get("platform", "unknown"),
                component=f"processor.{processor_type}"
            )
            
            raise
```

**éªŒè¯æ ‡å‡†**ï¼šç›‘æ§ä¸­é—´ä»¶å®ç°å®Œæˆï¼Œæ”¯æŒè¯·æ±‚å’Œå¤„ç†è¿‡ç¨‹ç›‘æ§

## 7.2 Grafanaä»ªè¡¨ç›˜

### 7.2.1 ä»ªè¡¨ç›˜é…ç½®

#### 7.2.1.1 åˆ›å»ºGrafanaé…ç½® [Time: 3h]
```yaml
# docker/grafana/provisioning/dashboards/crawler-dashboard.json
{
  "dashboard": {
    "id": null,
    "uid": "crawler-dashboard",
    "title": "æ—…æ¸¸çˆ¬è™«ç³»ç»Ÿç›‘æ§é¢æ¿",
    "tags": ["crawler", "monitoring"],
    "timezone": "browser",
    "schemaVersion": 30,
    "version": 1,
    "refresh": "5s",
    "panels": [
      {
        "id": 1,
        "gridPos": {"x": 0, "y": 0, "w": 8, "h": 8},
        "type": "graph",
        "title": "çˆ¬å–è¯·æ±‚é€Ÿç‡",
        "targets": [
          {
            "expr": "rate(crawler_requests_total[5m])",
            "legendFormat": "{{platform}} - {{status}}",
            "refId": "A"
          }
        ],
        "yaxes": [
          {
            "format": "reqps",
            "label": "è¯·æ±‚/ç§’"
          }
        ]
      },
      {
        "id": 2,
        "gridPos": {"x": 8, "y": 0, "w": 8, "h": 8},
        "type": "graph",
        "title": "çˆ¬å–æˆåŠŸç‡",
        "targets": [
          {
            "expr": "rate(crawler_requests_total{status=\"success\"}[5m]) / rate(crawler_requests_total[5m]) * 100",
            "legendFormat": "{{platform}}",
            "refId": "A"
          }
        ],
        "yaxes": [
          {
            "format": "percent",
            "label": "æˆåŠŸç‡",
            "min": 0,
            "max": 100
          }
        ]
      },
      {
        "id": 3,
        "gridPos": {"x": 16, "y": 0, "w": 8, "h": 8},
        "type": "graph",
        "title": "å“åº”æ—¶é—´åˆ†å¸ƒ",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(crawler_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P95 - {{platform}}",
            "refId": "A"
          },
          {
            "expr": "histogram_quantile(0.99, rate(crawler_request_duration_seconds_bucket[5m]))",
            "legendFormat": "P99 - {{platform}}",
            "refId": "B"
          }
        ],
        "yaxes": [
          {
            "format": "s",
            "label": "å“åº”æ—¶é—´"
          }
        ]
      },
      {
        "id": 4,
        "gridPos": {"x": 0, "y": 8, "w": 12, "h": 8},
        "type": "graph",
        "title": "APIè¯·æ±‚ç›‘æ§",
        "targets": [
          {
            "expr": "rate(crawler_api_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}} - {{status_code}}",
            "refId": "A"
          }
        ]
      },
      {
        "id": 5,
        "gridPos": {"x": 12, "y": 8, "w": 12, "h": 8},
        "type": "graph",
        "title": "é”™è¯¯ç‡è¶‹åŠ¿",
        "targets": [
          {
            "expr": "rate(crawler_errors_total[5m])",
            "legendFormat": "{{error_type}} - {{component}}",
            "refId": "A"
          }
        ],
        "alert": {
          "name": "é«˜é”™è¯¯ç‡è­¦æŠ¥",
          "conditions": [
            {
              "evaluator": {
                "params": [0.1],
                "type": "gt"
              },
              "query": {
                "params": ["A", "5m", "now"]
              },
              "reducer": {
                "params": [],
                "type": "avg"
              },
              "type": "query"
            }
          ],
          "frequency": "60s",
          "handler": 1,
          "message": "é”™è¯¯ç‡è¶…è¿‡é˜ˆå€¼",
          "noDataState": "no_data",
          "notifications": []
        }
      },
      {
        "id": 6,
        "gridPos": {"x": 0, "y": 16, "w": 8, "h": 8},
        "type": "stat",
        "title": "ä»£ç†æ± çŠ¶æ€",
        "targets": [
          {
            "expr": "crawler_proxy_pool_size",
            "legendFormat": "{{status}}",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 10},
                {"color": "green", "value": 50}
              ]
            }
          }
        }
      },
      {
        "id": 7,
        "gridPos": {"x": 8, "y": 16, "w": 8, "h": 8},
        "type": "gauge",
        "title": "æ•°æ®è´¨é‡åˆ†æ•°",
        "targets": [
          {
            "expr": "avg(rate(crawler_data_quality_score_sum[5m]) / rate(crawler_data_quality_score_count[5m]))",
            "refId": "A"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "min": 0,
            "max": 1,
            "thresholds": {
              "mode": "absolute",
              "steps": [
                {"color": "red", "value": 0},
                {"color": "yellow", "value": 0.6},
                {"color": "green", "value": 0.8}
              ]
            }
          }
        }
      },
      {
        "id": 8,
        "gridPos": {"x": 16, "y": 16, "w": 8, "h": 8},
        "type": "table",
        "title": "å¹³å°æ•°æ®ç»Ÿè®¡",
        "targets": [
          {
            "expr": "sum(crawler_poi_discovered_total) by (platform)",
            "format": "table",
            "instant": true,
            "refId": "A"
          }
        ],
        "transformations": [
          {
            "id": "organize",
            "options": {
              "excludeByName": {},
              "indexByName": {},
              "renameByName": {
                "platform": "å¹³å°",
                "Value": "POIæ€»æ•°"
              }
            }
          }
        ]
      }
    ]
  }
}

# docker/grafana/provisioning/datasources/prometheus.yml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
    jsonData:
      httpMethod: POST
      exemplarTraceIdDestinations:
        - name: trace_id
          url: http://jaeger:16686/trace/$${__value.raw}

# docker/grafana/provisioning/alerting/alerts.yml
apiVersion: 1

groups:
  - name: crawler_alerts
    interval: 1m
    rules:
      - alert: HighErrorRate
        expr: rate(crawler_errors_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "é«˜é”™è¯¯ç‡è­¦æŠ¥"
          description: "{{ $labels.component }} ç»„ä»¶çš„é”™è¯¯ç‡è¶…è¿‡ 10%"
      
      - alert: LowProxyPoolSize
        expr: crawler_proxy_pool_size{status="available"} < 10
        for: 10m
        labels:
          severity: critical
        annotations:
          summary: "å¯ç”¨ä»£ç†æ•°é‡è¿‡ä½"
          description: "å¯ç”¨ä»£ç†æ•°é‡ä»…å‰© {{ $value }} ä¸ª"
      
      - alert: SlowResponseTime
        expr: histogram_quantile(0.95, rate(crawler_request_duration_seconds_bucket[5m])) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "çˆ¬å–å“åº”æ—¶é—´è¿‡é•¿"
          description: "{{ $labels.platform }} å¹³å°çš„P95å“åº”æ—¶é—´è¶…è¿‡10ç§’"
      
      - alert: APIHighLatency
        expr: histogram_quantile(0.99, rate(crawler_api_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "APIå“åº”å»¶è¿Ÿè¿‡é«˜"
          description: "APIç«¯ç‚¹ {{ $labels.endpoint }} çš„P99å»¶è¿Ÿè¶…è¿‡2ç§’"
```

**éªŒè¯æ ‡å‡†**ï¼šGrafanaä»ªè¡¨ç›˜é…ç½®å®Œæˆï¼Œæ”¯æŒå¤šç»´åº¦å¯è§†åŒ–ç›‘æ§

#### 7.2.1.2 åˆ›å»ºè‡ªå®šä¹‰é¢æ¿ [Time: 2h]
```python
# src/monitoring/grafana_client.py
import requests
import json
from typing import Dict, Any, List
from src.core.config.settings import settings
from src.utils.logger.logger import get_logger

logger = get_logger("monitoring.grafana")

class GrafanaClient:
    """Grafana APIå®¢æˆ·ç«¯"""
    
    def __init__(self):
        self.base_url = settings.GRAFANA_URL
        self.api_key = settings.GRAFANA_API_KEY
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
    
    async def create_dashboard(self, dashboard_config: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºä»ªè¡¨ç›˜"""
        try:
            response = requests.post(
                f"{self.base_url}/api/dashboards/db",
                headers=self.headers,
                json=dashboard_config
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                logger.error(f"Create dashboard failed: {response.text}")
                return {}
                
        except Exception as e:
            logger.error(f"Grafana API error: {e}")
            return {}
    
    async def create_custom_panels(self) -> List[Dict[str, Any]]:
        """åˆ›å»ºè‡ªå®šä¹‰é¢æ¿"""
        panels = []
        
        # å®æ—¶çˆ¬å–çŠ¶æ€é¢æ¿
        real_time_panel = {
            "type": "graph",
            "title": "å®æ—¶çˆ¬å–çŠ¶æ€",
            "gridPos": {"x": 0, "y": 0, "w": 24, "h": 8},
            "targets": [
                {
                    "expr": 'sum(rate(crawler_requests_total[1m])) by (platform)',
                    "legendFormat": "{{platform}}",
                    "refId": "A"
                }
            ],
            "options": {
                "alerting": {},
                "dataLinks": [],
                "renderer": "flot",
                "tooltipOptions": {
                    "mode": "multi",
                    "sort": "desc"
                }
            },
            "xaxis": {
                "mode": "time",
                "show": true
            },
            "yaxes": [
                {
                    "format": "short",
                    "label": "è¯·æ±‚/åˆ†é’Ÿ",
                    "show": true
                }
            ]
        }
        panels.append(real_time_panel)
        
        # å¹³å°å¥åº·åº¦è¯„åˆ†é¢æ¿
        health_score_panel = {
            "type": "heatmap",
            "title": "å¹³å°å¥åº·åº¦çƒ­åŠ›å›¾",
            "gridPos": {"x": 0, "y": 8, "w": 12, "h": 8},
            "targets": [
                {
                    "expr": '''
                        (
                            rate(crawler_requests_total{status="success"}[5m]) / 
                            rate(crawler_requests_total[5m])
                        ) * 100
                    ''',
                    "format": "heatmap",
                    "legendFormat": "{{platform}}",
                    "refId": "A"
                }
            ],
            "dataFormat": "timeseries",
            "yAxis": {
                "format": "short",
                "decimals": 0,
                "logBase": 1,
                "min": 0,
                "max": 100
            }
        }
        panels.append(health_score_panel)
        
        # æ•°æ®æµé‡ç»Ÿè®¡é¢æ¿
        data_flow_panel = {
            "type": "graph",
            "title": "æ•°æ®æµé‡ç»Ÿè®¡",
            "gridPos": {"x": 12, "y": 8, "w": 12, "h": 8},
            "targets": [
                {
                    "expr": 'sum(rate(crawler_data_processed_total[5m])) by (processor_type)',
                    "legendFormat": "{{processor_type}}",
                    "refId": "A"
                }
            ],
            "seriesOverrides": [
                {
                    "alias": "cleaner",
                    "color": "#70DBED"
                },
                {
                    "alias": "deduplicator",
                    "color": "#F4D598"
                },
                {
                    "alias": "enhancer",
                    "color": "#96D98D"
                }
            ]
        }
        panels.append(data_flow_panel)
        
        # ç³»ç»Ÿèµ„æºä½¿ç”¨é¢æ¿
        resource_panel = {
            "type": "graph",
            "title": "ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ",
            "gridPos": {"x": 0, "y": 16, "w": 24, "h": 8},
            "targets": [
                {
                    "expr": 'crawler_active_engines',
                    "legendFormat": "æ´»è·ƒå¼•æ“ - {{engine_type}}",
                    "refId": "A"
                },
                {
                    "expr": 'crawler_redis_connections',
                    "legendFormat": "Redisè¿æ¥æ•°",
                    "refId": "B"
                },
                {
                    "expr": 'sum(crawler_db_connections)',
                    "legendFormat": "æ•°æ®åº“è¿æ¥æ€»æ•°",
                    "refId": "C"
                }
            ],
            "yaxes": [
                {
                    "format": "short",
                    "label": "æ•°é‡"
                }
            ]
        }
        panels.append(resource_panel)
        
        return panels
    
    async def setup_alerts(self) -> List[Dict[str, Any]]:
        """è®¾ç½®å‘Šè­¦è§„åˆ™"""
        alerts = []
        
        # çˆ¬å–å¤±è´¥ç‡å‘Šè­¦
        crawl_failure_alert = {
            "uid": "crawl_failure_alert",
            "title": "çˆ¬å–å¤±è´¥ç‡è¿‡é«˜",
            "condition": "A",
            "data": [
                {
                    "refId": "A",
                    "queryType": "",
                    "model": {
                        "expr": '''
                            rate(crawler_requests_total{status="failed"}[5m]) / 
                            rate(crawler_requests_total[5m]) > 0.2
                        ''',
                        "refId": "A"
                    },
                    "datasourceUid": "prometheus",
                    "relativeTimeRange": {
                        "from": 600,
                        "to": 0
                    }
                }
            ],
            "noDataState": "NoData",
            "execErrState": "Alerting",
            "for": "5m",
            "annotations": {
                "description": "å¹³å° {{ $labels.platform }} çš„çˆ¬å–å¤±è´¥ç‡è¶…è¿‡20%",
                "runbook_url": "https://wiki.company.com/crawler-troubleshooting",
                "summary": "çˆ¬å–å¤±è´¥ç‡è¿‡é«˜"
            },
            "labels": {
                "severity": "warning"
            }
        }
        alerts.append(crawl_failure_alert)
        
        # æ•°æ®è´¨é‡å‘Šè­¦
        data_quality_alert = {
            "uid": "data_quality_alert",
            "title": "æ•°æ®è´¨é‡ä¸‹é™",
            "condition": "A",
            "data": [
                {
                    "refId": "A",
                    "model": {
                        "expr": '''
                            avg(rate(crawler_data_quality_score_sum[5m]) / 
                            rate(crawler_data_quality_score_count[5m])) < 0.6
                        ''',
                        "refId": "A"
                    }
                }
            ],
            "for": "10m",
            "annotations": {
                "description": "å¹³å‡æ•°æ®è´¨é‡åˆ†æ•°ä½äº0.6",
                "summary": "æ•°æ®è´¨é‡å‘Šè­¦"
            },
            "labels": {
                "severity": "critical"
            }
        }
        alerts.append(data_quality_alert)
        
        return alerts

# src/monitoring/dashboard_generator.py
from typing import Dict, Any, List
import json

class DashboardGenerator:
    """ä»ªè¡¨ç›˜ç”Ÿæˆå™¨"""
    
    def generate_crawler_dashboard(self) -> Dict[str, Any]:
        """ç”Ÿæˆçˆ¬è™«ç›‘æ§ä»ªè¡¨ç›˜"""
        return {
            "dashboard": {
                "title": "çˆ¬è™«ç³»ç»Ÿå®æ—¶ç›‘æ§",
                "refresh": "5s",
                "time": {
                    "from": "now-6h",
                    "to": "now"
                },
                "panels": self._generate_all_panels(),
                "templating": {
                    "list": [
                        {
                            "name": "platform",
                            "type": "query",
                            "query": 'label_values(crawler_requests_total, platform)',
                            "multi": True,
                            "includeAll": True,
                            "current": {
                                "selected": True,
                                "text": "All",
                                "value": "$__all"
                            }
                        }
                    ]
                }
            }
        }
    
    def _generate_all_panels(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ‰€æœ‰é¢æ¿"""
        panels = []
        
        # æ·»åŠ å„ç§ç±»å‹çš„é¢æ¿
        panels.extend(self._generate_overview_panels())
        panels.extend(self._generate_performance_panels())
        panels.extend(self._generate_business_panels())
        panels.extend(self._generate_alert_panels())
        
        return panels
    
    def _generate_overview_panels(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¦‚è§ˆé¢æ¿"""
        return [
            {
                "id": 1,
                "type": "stat",
                "title": "æ€»è¯·æ±‚æ•°",
                "gridPos": {"x": 0, "y": 0, "w": 6, "h": 4},
                "targets": [{
                    "expr": 'sum(crawler_requests_total)',
                    "refId": "A"
                }]
            },
            {
                "id": 2,
                "type": "stat",
                "title": "æˆåŠŸç‡",
                "gridPos": {"x": 6, "y": 0, "w": 6, "h": 4},
                "targets": [{
                    "expr": '''
                        sum(crawler_requests_total{status="success"}) / 
                        sum(crawler_requests_total) * 100
                    ''',
                    "refId": "A"
                }],
                "fieldConfig": {
                    "defaults": {
                        "unit": "percent",
                        "thresholds": {
                            "mode": "absolute",
                            "steps": [
                                {"color": "red", "value": 0},
                                {"color": "yellow", "value": 80},
                                {"color": "green", "value": 95}
                            ]
                        }
                    }
                }
            }
        ]
    
    def _generate_performance_panels(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ€§èƒ½é¢æ¿"""
        return [
            {
                "id": 10,
                "type": "graph",
                "title": "å“åº”æ—¶é—´è¶‹åŠ¿",
                "gridPos": {"x": 0, "y": 4, "w": 12, "h": 8},
                "targets": [
                    {
                        "expr": 'histogram_quantile(0.5, rate(crawler_request_duration_seconds_bucket[5m]))',
                        "legendFormat": "P50",
                        "refId": "A"
                    },
                    {
                        "expr": 'histogram_quantile(0.95, rate(crawler_request_duration_seconds_bucket[5m]))',
                        "legendFormat": "P95",
                        "refId": "B"
                    },
                    {
                        "expr": 'histogram_quantile(0.99, rate(crawler_request_duration_seconds_bucket[5m]))',
                        "legendFormat": "P99",
                        "refId": "C"
                    }
                ]
            }
        ]
    
    def _generate_business_panels(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¸šåŠ¡é¢æ¿"""
        return [
            {
                "id": 20,
                "type": "bargauge",
                "title": "å„å¹³å°POIå‘ç°æ•°",
                "gridPos": {"x": 0, "y": 12, "w": 12, "h": 8},
                "targets": [{
                    "expr": 'sum(crawler_poi_discovered_total) by (platform)',
                    "legendFormat": "{{platform}}",
                    "refId": "A"
                }]
            }
        ]
    
    def _generate_alert_panels(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå‘Šè­¦é¢æ¿"""
        return [
            {
                "id": 30,
                "type": "alertlist",
                "title": "æ´»è·ƒå‘Šè­¦",
                "gridPos": {"x": 12, "y": 12, "w": 12, "h": 8},
                "options": {
                    "showOptions": "current",
                    "maxItems": 10,
                    "sortOrder": 1,
                    "dashboardAlerts": True,
                    "alertName": "",
                    "dashboardTitle": "",
                    "tags": []
                }
            }
        ]
```

**éªŒè¯æ ‡å‡†**ï¼šè‡ªå®šä¹‰é¢æ¿åˆ›å»ºå®Œæˆï¼Œæ”¯æŒä¸šåŠ¡æŒ‡æ ‡å¯è§†åŒ–

## 7.3 DockeråŒ–éƒ¨ç½²

### 7.3.1 å®¹å™¨åŒ–é…ç½®

#### 7.3.1.1 åˆ›å»ºDockeré…ç½®æ–‡ä»¶ [Time: 3h]
```dockerfile
# Dockerfile
FROM python:3.11-slim as builder

# å®‰è£…æ„å»ºä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libpq-dev \
    libxml2-dev \
    libxslt-dev \
    libffi-dev \
    libssl-dev \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY pyproject.toml poetry.lock ./

# å®‰è£…Poetry
RUN pip install poetry==1.3.0

# å¯¼å‡ºä¾èµ–åˆ°requirements.txt
RUN poetry export -f requirements.txt --output requirements.txt --without-hashes

# æœ€ç»ˆé•œåƒ
FROM python:3.11-slim

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apt-get update && apt-get install -y \
    libpq5 \
    libxml2 \
    libxslt1.1 \
    wget \
    gnupg \
    && rm -rf /var/lib/apt/lists/*

# å®‰è£…Chromeç”¨äºMediaCrawlå¼•æ“
RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \
    && echo "deb http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list \
    && apt-get update \
    && apt-get install -y google-chrome-stable \
    && rm -rf /var/lib/apt/lists/*

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# ä»æ„å»ºé˜¶æ®µå¤åˆ¶requirements.txt
COPY --from=builder /app/requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å®‰è£…Playwrightæµè§ˆå™¨
RUN playwright install chromium

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºérootç”¨æˆ·
RUN useradd -m -u 1000 crawler && chown -R crawler:crawler /app

# åˆ‡æ¢åˆ°érootç”¨æˆ·
USER crawler

# æš´éœ²ç«¯å£
EXPOSE 8000 9090

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]

# docker-compose.yml
version: '3.8'

services:
  postgres:
    image: postgis/postgis:15-3.3
    container_name: crawler_postgres
    environment:
      POSTGRES_DB: crawler_db
      POSTGRES_USER: crawler
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U crawler"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - crawler_network

  redis:
    image: redis:7-alpine
    container_name: crawler_redis
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - crawler_network

  crawler_app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: crawler_app
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql+asyncpg://crawler:${DB_PASSWORD}@postgres:5432/crawler_db
      REDIS_URL: redis://redis:6379/0
      APP_ENV: production
    volumes:
      - ./logs:/app/logs
      - ./data:/app/data
    ports:
      - "8000:8000"
      - "9090:9090"
    networks:
      - crawler_network
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    container_name: crawler_prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9091:9090"
    networks:
      - crawler_network
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: crawler_grafana
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - ./docker/grafana/provisioning:/etc/grafana/provisioning
      - grafana_data:/var/lib/grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    networks:
      - crawler_network
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    container_name: crawler_nginx
    volumes:
      - ./docker/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./docker/nginx/conf.d:/etc/nginx/conf.d
    ports:
      - "80:80"
      - "443:443"
    depends_on:
      - crawler_app
    networks:
      - crawler_network
    restart: unless-stopped

networks:
  crawler_network:
    driver: bridge

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

# docker/postgres/init.sql
-- å¯ç”¨PostGISæ‰©å±•
CREATE EXTENSION IF NOT EXISTS postgis;
CREATE EXTENSION IF NOT EXISTS pg_trgm;

-- åˆ›å»ºç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_poi_location_gist ON pois USING GIST (location);
CREATE INDEX IF NOT EXISTS idx_poi_name_trgm ON pois USING GIN (name gin_trgm_ops);
CREATE INDEX IF NOT EXISTS idx_poi_address_trgm ON pois USING GIN (address gin_trgm_ops);

-- åˆ›å»ºåˆ†åŒºè¡¨ï¼ˆæŒ‰æœˆåˆ†åŒºï¼‰
CREATE TABLE IF NOT EXISTS crawl_tasks_partitioned (
    LIKE crawl_tasks INCLUDING ALL
) PARTITION BY RANGE (created_at);

-- åˆ›å»ºåˆå§‹åˆ†åŒº
CREATE TABLE IF NOT EXISTS crawl_tasks_y2024m01 PARTITION OF crawl_tasks_partitioned
    FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE IF NOT EXISTS crawl_tasks_y2024m02 PARTITION OF crawl_tasks_partitioned
    FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

# docker/nginx/conf.d/crawler.conf
upstream crawler_backend {
    server crawler_app:8000;
}

upstream metrics_backend {
    server crawler_app:9090;
}

server {
    listen 80;
    server_name crawler.example.com;

    # APIè·¯ç”±
    location /api/ {
        proxy_pass http://crawler_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # WebSocketæ”¯æŒ
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        
        # è¶…æ—¶è®¾ç½®
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }

    # Metricsè·¯ç”±
    location /metrics {
        proxy_pass http://metrics_backend;
        
        # åŸºç¡€è®¤è¯
        auth_basic "Metrics Access";
        auth_basic_user_file /etc/nginx/.htpasswd;
    }

    # é™æ€æ–‡ä»¶
    location /static/ {
        alias /app/static/;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }

    # å¥åº·æ£€æŸ¥
    location /health {
        proxy_pass http://crawler_backend/health;
        access_log off;
    }
}
```

**éªŒè¯æ ‡å‡†**ï¼šDockeré…ç½®æ–‡ä»¶åˆ›å»ºå®Œæˆï¼Œæ”¯æŒå®Œæ•´çš„å®¹å™¨åŒ–éƒ¨ç½²

#### 7.3.1.2 åˆ›å»ºéƒ¨ç½²è„šæœ¬ [Time: 2h]
```bash
#!/bin/bash
# deploy.sh

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

# å‡½æ•°å®šä¹‰
log_info() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ£€æŸ¥ç¯å¢ƒå˜é‡
check_env() {
    log_info "æ£€æŸ¥ç¯å¢ƒå˜é‡..."
    
    required_vars=("DB_PASSWORD" "GRAFANA_PASSWORD" "SECRET_KEY")
    
    for var in "${required_vars[@]}"; do
        if [ -z "${!var}" ]; then
            log_error "ç¯å¢ƒå˜é‡ $var æœªè®¾ç½®"
            exit 1
        fi
    done
    
    log_info "ç¯å¢ƒå˜é‡æ£€æŸ¥é€šè¿‡"
}

# æ„å»ºé•œåƒ
build_images() {
    log_info "æ„å»ºDockeré•œåƒ..."
    
    docker-compose build --no-cache
    
    if [ $? -eq 0 ]; then
        log_info "é•œåƒæ„å»ºæˆåŠŸ"
    else
        log_error "é•œåƒæ„å»ºå¤±è´¥"
        exit 1
    fi
}

# å¯åŠ¨æœåŠ¡
start_services() {
    log_info "å¯åŠ¨æœåŠ¡..."
    
    docker-compose up -d
    
    if [ $? -eq 0 ]; then
        log_info "æœåŠ¡å¯åŠ¨æˆåŠŸ"
    else
        log_error "æœåŠ¡å¯åŠ¨å¤±è´¥"
        exit 1
    fi
}

# ç­‰å¾…æœåŠ¡å°±ç»ª
wait_for_services() {
    log_info "ç­‰å¾…æœåŠ¡å°±ç»ª..."
    
    services=("postgres:5432" "redis:6379" "crawler_app:8000")
    
    for service in "${services[@]}"; do
        host=$(echo $service | cut -d: -f1)
        port=$(echo $service | cut -d: -f2)
        
        log_info "ç­‰å¾… $host:$port..."
        
        timeout=60
        while ! nc -z $host $port; do
            sleep 1
            timeout=$((timeout - 1))
            if [ $timeout -eq 0 ]; then
                log_error "$host:$port å¯åŠ¨è¶…æ—¶"
                exit 1
            fi
        done
        
        log_info "$host:$port å·²å°±ç»ª"
    done
}

# è¿è¡Œæ•°æ®åº“è¿ç§»
run_migrations() {
    log_info "è¿è¡Œæ•°æ®åº“è¿ç§»..."
    
    docker-compose exec -T crawler_app alembic upgrade head
    
    if [ $? -eq 0 ]; then
        log_info "æ•°æ®åº“è¿ç§»æˆåŠŸ"
    else
        log_error "æ•°æ®åº“è¿ç§»å¤±è´¥"
        exit 1
    fi
}

# å¥åº·æ£€æŸ¥
health_check() {
    log_info "æ‰§è¡Œå¥åº·æ£€æŸ¥..."
    
    response=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8000/health)
    
    if [ $response -eq 200 ]; then
        log_info "å¥åº·æ£€æŸ¥é€šè¿‡"
    else
        log_error "å¥åº·æ£€æŸ¥å¤±è´¥ï¼ŒHTTPçŠ¶æ€ç : $response"
        exit 1
    fi
}

# æ˜¾ç¤ºæœåŠ¡çŠ¶æ€
show_status() {
    log_info "æœåŠ¡çŠ¶æ€:"
    docker-compose ps
    
    echo ""
    log_info "è®¿é—®åœ°å€:"
    echo "  - APIæ–‡æ¡£: http://localhost:8000/docs"
    echo "  - Grafana: http://localhost:3000 (admin/${GRAFANA_PASSWORD})"
    echo "  - Prometheus: http://localhost:9091"
}

# ä¸»å‡½æ•°
main() {
    log_info "å¼€å§‹éƒ¨ç½²çˆ¬è™«ç³»ç»Ÿ..."
    
    check_env
    build_images
    start_services
    wait_for_services
    run_migrations
    health_check
    show_status
    
    log_info "éƒ¨ç½²å®Œæˆï¼"
}

# æ‰§è¡Œä¸»å‡½æ•°
main

# docker/scripts/backup.sh
#!/bin/bash
# å¤‡ä»½è„šæœ¬

BACKUP_DIR="/backup/crawler"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# å¤‡ä»½æ•°æ®åº“
log_info "å¤‡ä»½æ•°æ®åº“..."
docker-compose exec -T postgres pg_dump -U crawler crawler_db | gzip > $BACKUP_DIR/db_backup_$TIMESTAMP.sql.gz

# å¤‡ä»½Redis
log_info "å¤‡ä»½Redis..."
docker-compose exec -T redis redis-cli BGSAVE
sleep 5
docker cp crawler_redis:/data/dump.rdb $BACKUP_DIR/redis_backup_$TIMESTAMP.rdb

# å¤‡ä»½åº”ç”¨æ•°æ®
log_info "å¤‡ä»½åº”ç”¨æ•°æ®..."
tar -czf $BACKUP_DIR/app_data_$TIMESTAMP.tar.gz ./data ./logs

# æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™7å¤©ï¼‰
find $BACKUP_DIR -type f -mtime +7 -delete

log_info "å¤‡ä»½å®Œæˆï¼"

# docker/scripts/restore.sh
#!/bin/bash
# æ¢å¤è„šæœ¬

if [ $# -eq 0 ]; then
    echo "Usage: $0 <backup_timestamp>"
    exit 1
fi

TIMESTAMP=$1
BACKUP_DIR="/backup/crawler"

# æ¢å¤æ•°æ®åº“
log_info "æ¢å¤æ•°æ®åº“..."
gunzip -c $BACKUP_DIR/db_backup_$TIMESTAMP.sql.gz | docker-compose exec -T postgres psql -U crawler crawler_db

# æ¢å¤Redis
log_info "æ¢å¤Redis..."
docker cp $BACKUP_DIR/redis_backup_$TIMESTAMP.rdb crawler_redis:/data/dump.rdb
docker-compose restart redis

# æ¢å¤åº”ç”¨æ•°æ®
log_info "æ¢å¤åº”ç”¨æ•°æ®..."
tar -xzf $BACKUP_DIR/app_data_$TIMESTAMP.tar.gz -C ./

log_info "æ¢å¤å®Œæˆï¼"
```

**éªŒè¯æ ‡å‡†**ï¼šéƒ¨ç½²è„šæœ¬åˆ›å»ºå®Œæˆï¼Œæ”¯æŒè‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹

## 7.4 Kubernetesç¼–æ’

### 7.4.1 K8sé…ç½®æ–‡ä»¶

#### 7.4.1.1 åˆ›å»ºK8séƒ¨ç½²é…ç½® [Time: 4h]
```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: crawler-system

---
# k8s/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: crawler-config
  namespace: crawler-system
data:
  APP_ENV: "production"
  DEBUG: "false"
  LOG_LEVEL: "INFO"
  REDIS_URL: "redis://redis-service:6379/0"
  DATABASE_URL: "postgresql+asyncpg://crawler:password@postgres-service:5432/crawler_db"

---
# k8s/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: crawler-secret
  namespace: crawler-system
type: Opaque
stringData:
  SECRET_KEY: "your-secret-key-here"
  DB_PASSWORD: "your-db-password"
  GRAFANA_PASSWORD: "your-grafana-password"

---
# k8s/postgres.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: crawler-system
spec:
  serviceName: postgres-service
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgis/postgis:15-3.3
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_DB
          value: crawler_db
        - name: POSTGRES_USER
          value: crawler
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: crawler-secret
              key: DB_PASSWORD
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        livenessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - crawler
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - crawler
          initialDelaySeconds: 5
          periodSeconds: 5
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi

---
# k8s/postgres-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: postgres-service
  namespace: crawler-system
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
  clusterIP: None

---
# k8s/redis.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: crawler-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        command:
        - redis-server
        - --appendonly
        - "yes"
        - --maxmemory
        - "2gb"
        - --maxmemory-policy
        - "allkeys-lru"
        volumeMounts:
        - name: redis-storage
          mountPath: /data
        livenessProbe:
          tcpSocket:
            port: 6379
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - redis-cli
            - ping
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: redis-storage
        persistentVolumeClaim:
          claimName: redis-pvc

---
# k8s/redis-pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: redis-pvc
  namespace: crawler-system
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

---
# k8s/redis-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: redis-service
  namespace: crawler-system
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379

---
# k8s/crawler-app.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: crawler-app
  namespace: crawler-system
spec:
  replicas: 3
  selector:
    matchLabels:
      app: crawler-app
  template:
    metadata:
      labels:
        app: crawler-app
    spec:
      initContainers:
      - name: wait-for-db
        image: busybox:1.35
        command: ['sh', '-c', 'until nc -z postgres-service 5432; do echo waiting for db; sleep 2; done;']
      - name: wait-for-redis
        image: busybox:1.35
        command: ['sh', '-c', 'until nc -z redis-service 6379; do echo waiting for redis; sleep 2; done;']
      containers:
      - name: crawler
        image: your-registry/crawler-app:latest
        ports:
        - containerPort: 8000
          name: api
        - containerPort: 9090
          name: metrics
        env:
        - name: APP_ENV
          valueFrom:
            configMapKeyRef:
              name: crawler-config
              key: APP_ENV
        - name: DATABASE_URL
          valueFrom:
            configMapKeyRef:
              name: crawler-config
              key: DATABASE_URL
        - name: REDIS_URL
          valueFrom:
            configMapKeyRef:
              name: crawler-config
              key: REDIS_URL
        - name: SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: crawler-secret
              key: SECRET_KEY
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 60
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        volumeMounts:
        - name: logs
          mountPath: /app/logs
        - name: data
          mountPath: /app/data
      volumes:
      - name: logs
        emptyDir: {}
      - name: data
        persistentVolumeClaim:
          claimName: crawler-data-pvc

---
# k8s/crawler-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: crawler-service
  namespace: crawler-system
spec:
  selector:
    app: crawler-app
  ports:
  - name: api
    port: 8000
    targetPort: 8000
  - name: metrics
    port: 9090
    targetPort: 9090

---
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: crawler-ingress
  namespace: crawler-system
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - crawler.example.com
    secretName: crawler-tls
  rules:
  - host: crawler.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: crawler-service
            port:
              number: 8000

---
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: crawler-hpa
  namespace: crawler-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: crawler-app
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: crawler_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"

---
# k8s/prometheus.yaml
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: crawler-monitor
  namespace: crawler-system
spec:
  selector:
    matchLabels:
      app: crawler-app
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
```

**éªŒè¯æ ‡å‡†**ï¼šK8séƒ¨ç½²é…ç½®åˆ›å»ºå®Œæˆï¼Œæ”¯æŒç”Ÿäº§çº§å®¹å™¨ç¼–æ’

#### 7.4.1.2 åˆ›å»ºHelm Chart [Time: 2h]
```yaml
# helm/crawler/Chart.yaml
apiVersion: v2
name: crawler-system
description: A Helm chart for Travel Crawler System
type: application
version: 1.0.0
appVersion: "1.0.0"
dependencies:
  - name: postgresql
    version: 12.1.0
    repository: https://charts.bitnami.com/bitnami
  - name: redis
    version: 17.3.0
    repository: https://charts.bitnami.com/bitnami

# helm/crawler/values.yaml
replicaCount: 3

image:
  repository: your-registry/crawler-app
  pullPolicy: IfNotPresent
  tag: "latest"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "9090"
  prometheus.io/path: "/metrics"

podSecurityContext:
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000

securityContext:
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: false
  runAsNonRoot: true
  runAsUser: 1000

service:
  type: ClusterIP
  port: 8000
  metricsPort: 9090

ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
  hosts:
    - host: crawler.example.com
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls:
    - secretName: crawler-tls
      hosts:
        - crawler.example.com

resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values:
            - crawler-app
        topologyKey: kubernetes.io/hostname

postgresql:
  enabled: true
  auth:
    postgresPassword: "postgres"
    username: "crawler"
    password: "crawler"
    database: "crawler_db"
  primary:
    persistence:
      enabled: true
      size: 10Gi

redis:
  enabled: true
  auth:
    enabled: false
  master:
    persistence:
      enabled: true
      size: 5Gi

config:
  appEnv: "production"
  debug: false
  logLevel: "INFO"

secrets:
  secretKey: "change-me-in-production"
  grafanaPassword: "admin"

# helm/crawler/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "crawler.fullname" . }}
  labels:
    {{- include "crawler.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "crawler.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "crawler.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "crawler.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      initContainers:
        - name: wait-for-db
          image: busybox:1.35
          command: ['sh', '-c', 'until nc -z {{ .Release.Name }}-postgresql 5432; do echo waiting for db; sleep 2; done;']
        - name: wait-for-redis
          image: busybox:1.35
          command: ['sh', '-c', 'until nc -z {{ .Release.Name }}-redis-master 6379; do echo waiting for redis; sleep 2; done;']
        - name: run-migrations
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          command: ['alembic', 'upgrade', 'head']
          env:
            - name: DATABASE_URL
              value: "postgresql+asyncpg://{{ .Values.postgresql.auth.username }}:{{ .Values.postgresql.auth.password }}@{{ .Release.Name }}-postgresql:5432/{{ .Values.postgresql.auth.database }}"
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
            - name: metrics
              containerPort: 9090
              protocol: TCP
          env:
            - name: APP_ENV
              value: {{ .Values.config.appEnv }}
            - name: DATABASE_URL
              value: "postgresql+asyncpg://{{ .Values.postgresql.auth.username }}:{{ .Values.postgresql.auth.password }}@{{ .Release.Name }}-postgresql:5432/{{ .Values.postgresql.auth.database }}"
            - name: REDIS_URL
              value: "redis://{{ .Release.Name }}-redis-master:6379/0"
            - name: SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ include "crawler.fullname" . }}-secret
                  key: secret-key
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 60
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
```

**éªŒè¯æ ‡å‡†**ï¼šHelm Chartåˆ›å»ºå®Œæˆï¼Œæ”¯æŒä¸€é”®éƒ¨ç½²å’Œç‰ˆæœ¬ç®¡ç†

---

## éªŒè¯å’Œæµ‹è¯•è¦æ±‚

### é˜¶æ®µä¸ƒéªŒè¯æ¸…å•
- [ ] PrometheusæŒ‡æ ‡æ”¶é›†æ­£å¸¸
- [ ] Grafanaä»ªè¡¨ç›˜æ˜¾ç¤ºæ­£ç¡®
- [ ] Dockerå®¹å™¨è¿è¡Œç¨³å®š
- [ ] K8sé›†ç¾¤éƒ¨ç½²æˆåŠŸ
- [ ] è‡ªåŠ¨æ‰©ç¼©å®¹åŠŸèƒ½æ­£å¸¸

### ä»£ç è´¨é‡è¦æ±‚
- ç›‘æ§è¦†ç›–ç‡ > 90%
- éƒ¨ç½²è„šæœ¬æ— é”™è¯¯
- é…ç½®æ–‡ä»¶è§„èŒƒ
- æ–‡æ¡£å®Œæ•´è¯¦ç»†

### æ€§èƒ½è¦æ±‚
- ç›‘æ§å»¶è¿Ÿ < 10ç§’
- éƒ¨ç½²æ—¶é—´ < 10åˆ†é’Ÿ
- æœåŠ¡å¯ç”¨æ€§ > 99.9%

---

## é¡¹ç›®æ€»ç»“

### å®Œæˆçš„é˜¶æ®µ
1. âœ… **ç¬¬1é˜¶æ®µ**ï¼šé¡¹ç›®åˆå§‹åŒ–ä¸åŸºç¡€æ¶æ„
2. âœ… **ç¬¬2é˜¶æ®µ**ï¼šåŒå¼•æ“æ ¸å¿ƒæ¶æ„
3. âœ… **ç¬¬3é˜¶æ®µ**ï¼šåçˆ¬ç³»ç»Ÿå®ç°
4. âœ… **ç¬¬4é˜¶æ®µ**ï¼šå¹³å°é€‚é…å™¨å®ç°ï¼ˆç¬¬ä¸€æ‰¹ï¼‰
5. âœ… **ç¬¬5é˜¶æ®µ**ï¼šæ•°æ®å¤„ç†ä¸APIæœåŠ¡
6. âœ… **ç¬¬6é˜¶æ®µ**ï¼šé«˜çº§å¹³å°é€‚é…å™¨å®ç°
7. âœ… **ç¬¬7é˜¶æ®µ**ï¼šç›‘æ§è¿ç»´ä¸éƒ¨ç½²

### æ€»è®¡ä»»åŠ¡ç»Ÿè®¡
- **æ€»ä»»åŠ¡æ•°**ï¼š318ä¸ªè¯¦ç»†ä»»åŠ¡
- **ä»£ç è¡Œæ•°**ï¼šçº¦50,000è¡Œ
- **é…ç½®æ–‡ä»¶**ï¼š30+ä¸ª
- **æµ‹è¯•ç”¨ä¾‹**ï¼š200+ä¸ª
- **æ–‡æ¡£é¡µæ•°**ï¼š1000+é¡µ

### æŠ€æœ¯æ ˆè¦†ç›–
- **åç«¯**ï¼šPython, FastAPI, SQLAlchemy, Celery
- **æ•°æ®åº“**ï¼šPostgreSQL, Redis
- **çˆ¬è™«**ï¼šCrawl4AI, Playwright
- **ç›‘æ§**ï¼šPrometheus, Grafana
- **éƒ¨ç½²**ï¼šDocker, Kubernetes, Helm

### é¡¹ç›®ç‰¹è‰²
1. **åŒå¼•æ“æ¶æ„**ï¼šå…¼é¡¾æ•ˆç‡ä¸åŠŸèƒ½
2. **å®Œå–„çš„åçˆ¬ç³»ç»Ÿ**ï¼šä¸‰å±‚é˜²æŠ¤æœºåˆ¶
3. **å¤šå¹³å°æ”¯æŒ**ï¼šè¦†ç›–8å¤§ä¸»æµå¹³å°
4. **æ™ºèƒ½æ•°æ®å¤„ç†**ï¼šæ¸…æ´—ã€å»é‡ã€å¢å¼º
5. **ç”Ÿäº§çº§éƒ¨ç½²**ï¼šå®¹å™¨åŒ–ã€è‡ªåŠ¨æ‰©ç¼©å®¹

---

**é‡è¦æé†’**ï¼š
- æœ¬æ–‡æ¡£ä¸ºç¬¬å››éƒ¨åˆ†ï¼Œå®Œæˆäº†å…¨éƒ¨7ä¸ªé˜¶æ®µçš„è¯¦ç»†TODOæ¸…å•
- æ€»è®¡318ä¸ªä»»åŠ¡ï¼Œæ¯ä¸ªéƒ½åŒ…å«å…·ä½“å®ç°ä»£ç 
- å¯ç›´æ¥ç”¨äºæŒ‡å¯¼AIç¼–ç¨‹åŠ©æ‰‹å®Œæˆæ•´ä¸ªé¡¹ç›®å¼€å‘
- å»ºè®®æŒ‰ç…§é˜¶æ®µé¡ºåºæ‰§è¡Œï¼Œç¡®ä¿è´¨é‡å’Œè¿›åº¦

**ä½¿ç”¨å»ºè®®**ï¼š
1. ä¸¥æ ¼æŒ‰ç…§TODOé¡ºåºæ‰§è¡Œ
2. æ¯ä¸ªä»»åŠ¡å®Œæˆåè¿›è¡ŒéªŒè¯
3. å®šæœŸè¿›è¡Œä»£ç å®¡æŸ¥
4. ä¿æŒæ–‡æ¡£åŒæ­¥æ›´æ–°
5. é‡è§†ç›‘æ§å’Œè¿ç»´

é€šè¿‡è¿™ä¸ªè¯¦ç»†çš„TODOæ¸…å•ï¼ŒAIç¼–ç¨‹åŠ©æ‰‹å¯ä»¥ç³»ç»Ÿåœ°å®Œæˆæ•´ä¸ªæ—…æ¸¸å¹³å°æ•°æ®çˆ¬è™«ç³»ç»Ÿçš„å¼€å‘å·¥ä½œã€‚