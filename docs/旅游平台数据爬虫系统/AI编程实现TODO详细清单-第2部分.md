# æ—…æ¸¸å¹³å°æ•°æ®çˆ¬è™«ç³»ç»Ÿ - AIç¼–ç¨‹å®ç°TODOè¯¦ç»†æ¸…å•ï¼ˆç¬¬2éƒ¨åˆ†ï¼‰

> æœ¬æ–‡æ¡£ä¸ºAIç¼–ç¨‹å®ç°TODOè¯¦ç»†æ¸…å•çš„ç¬¬äºŒéƒ¨åˆ†ï¼ŒåŒ…å«ç¬¬5-7é˜¶æ®µçš„è¯¦ç»†ä»»åŠ¡

## å‰æƒ…æè¦

ç¬¬ä¸€éƒ¨åˆ†å·²å®Œæˆï¼š
- âœ… ç¬¬1é˜¶æ®µï¼šé¡¹ç›®åˆå§‹åŒ–ä¸åŸºç¡€æ¶æ„
- âœ… ç¬¬2é˜¶æ®µï¼šåŒå¼•æ“æ ¸å¿ƒæ¶æ„
- âœ… ç¬¬3é˜¶æ®µï¼šåçˆ¬ç³»ç»Ÿå®ç°
- âœ… ç¬¬4é˜¶æ®µï¼šå¹³å°é€‚é…å™¨å®ç°ï¼ˆç¬¬ä¸€æ‰¹ï¼‰

æœ¬æ–‡æ¡£ç»§ç»­ï¼š
- ğŸ“ ç¬¬5é˜¶æ®µï¼šæ•°æ®å¤„ç†ä¸APIæœåŠ¡
- ğŸ“ ç¬¬6é˜¶æ®µï¼šé«˜çº§å¹³å°é€‚é…å™¨å®ç°
- ğŸ“ ç¬¬7é˜¶æ®µï¼šç›‘æ§è¿ç»´ä¸éƒ¨ç½²

---

# ç¬¬äº”é˜¶æ®µï¼šæ•°æ®å¤„ç†ä¸APIæœåŠ¡ [Priority: HIGH] [Time: 5-6å¤©]

## 5.1 æ•°æ®æ¸…æ´—ç³»ç»Ÿ

### 5.1.1 æ•°æ®æ¸…æ´—æ¡†æ¶

#### 5.1.1.1 åˆ›å»ºæ•°æ®æ¸…æ´—åŸºç¡€ç±» [Time: 3h]
```python
# src/processors/cleaner/base_cleaner.py
import re
import html
from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional
from src.utils.logger.logger import get_logger

logger = get_logger("processor.cleaner")

class DataCleaner(ABC):
    """æ•°æ®æ¸…æ´—åŸºç¡€ç±»"""
    
    def __init__(self):
        self.cleaning_rules = []
        self._init_rules()
    
    @abstractmethod
    def _init_rules(self):
        """åˆå§‹åŒ–æ¸…æ´—è§„åˆ™"""
        pass
    
    async def clean(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®æ¸…æ´—"""
        try:
            # åº”ç”¨æ‰€æœ‰æ¸…æ´—è§„åˆ™
            cleaned_data = data.copy()
            
            for rule in self.cleaning_rules:
                cleaned_data = await rule(cleaned_data)
            
            # åŸºç¡€æ¸…æ´—
            cleaned_data = await self._basic_clean(cleaned_data)
            
            # ç‰¹å®šå¹³å°æ¸…æ´—
            cleaned_data = await self._platform_specific_clean(cleaned_data)
            
            return cleaned_data
            
        except Exception as e:
            logger.error(f"Data cleaning failed: {e}")
            return data
    
    async def _basic_clean(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºç¡€æ¸…æ´—"""
        # æ¸…ç†å­—ç¬¦ä¸²å­—æ®µ
        for key, value in data.items():
            if isinstance(value, str):
                data[key] = await self._clean_string(value)
            elif isinstance(value, dict):
                data[key] = await self._basic_clean(value)
            elif isinstance(value, list):
                data[key] = [
                    await self._clean_string(item) if isinstance(item, str) else item
                    for item in value
                ]
        
        return data
    
    async def _clean_string(self, text: str) -> str:
        """æ¸…ç†å­—ç¬¦ä¸²"""
        if not text:
            return ""
        
        # HTMLè§£ç 
        text = html.unescape(text)
        
        # ç§»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        
        # ç§»é™¤æ§åˆ¶å­—ç¬¦
        text = re.sub(r'[\x00-\x1F\x7F-\x9F]', '', text)
        
        # å»é™¤é¦–å°¾ç©ºç™½
        text = text.strip()
        
        return text
    
    @abstractmethod
    async def _platform_specific_clean(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¹³å°ç‰¹å®šæ¸…æ´—"""
        pass

class POICleaner(DataCleaner):
    """POIæ•°æ®æ¸…æ´—å™¨"""
    
    def _init_rules(self):
        """åˆå§‹åŒ–POIæ¸…æ´—è§„åˆ™"""
        self.cleaning_rules = [
            self._clean_name,
            self._clean_address,
            self._clean_phone,
            self._normalize_rating,
            self._validate_coordinates
        ]
    
    async def _clean_name(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…æ´—åç§°"""
        if "name" in data:
            # ç§»é™¤æ‹¬å·å†…å®¹
            name = re.sub(r'\([^)]*\)', '', data["name"])
            
            # ç§»é™¤ç‰¹æ®Šæ ‡è®°
            name = re.sub(r'ã€[^ã€‘]*ã€‘', '', name)
            
            # æ ‡å‡†åŒ–
            data["name"] = name.strip()
        
        return data
    
    async def _clean_address(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…æ´—åœ°å€"""
        if "address" in data:
            address = data["address"]
            
            # ç§»é™¤é‡å¤çš„åŸå¸‚/åŒºåŸŸä¿¡æ¯
            if "city" in data and data["city"] in address:
                address = address.replace(data["city"], "", 1)
            
            if "district" in data and data["district"] in address:
                address = address.replace(data["district"], "", 1)
            
            data["address"] = address.strip()
        
        return data
    
    async def _clean_phone(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…æ´—ç”µè¯å·ç """
        if "phone" in data and data["phone"]:
            phone = data["phone"]
            
            # ç§»é™¤éæ•°å­—å­—ç¬¦
            phone = re.sub(r'[^\d\-\+\s,;]', '', phone)
            
            # æ ‡å‡†åŒ–åˆ†éš”ç¬¦
            phone = re.sub(r'[,;]', ' ', phone)
            
            # éªŒè¯ç”µè¯æ ¼å¼
            phones = []
            for p in phone.split():
                p = p.strip()
                if re.match(r'^(\+86)?1[3-9]\d{9}$', p):  # æ‰‹æœºå·
                    phones.append(p)
                elif re.match(r'^(\d{3,4}-)?\d{7,8}$', p):  # åº§æœº
                    phones.append(p)
            
            data["phone"] = " ".join(phones)
        
        return data
    
    async def _normalize_rating(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–è¯„åˆ†"""
        if "rating" in data:
            try:
                rating = float(data["rating"])
                
                # ç»Ÿä¸€åˆ°5åˆ†åˆ¶
                if rating > 5:
                    rating = rating / 2  # 10åˆ†åˆ¶è½¬5åˆ†åˆ¶
                
                # ä¿ç•™ä¸€ä½å°æ•°
                data["rating"] = round(rating, 1)
                
            except (ValueError, TypeError):
                data["rating"] = 0.0
        
        return data
    
    async def _validate_coordinates(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯åæ ‡"""
        if "latitude" in data and "longitude" in data:
            try:
                lat = float(data["latitude"])
                lng = float(data["longitude"])
                
                # ä¸­å›½åœ°ç†èŒƒå›´éªŒè¯
                if not (3.86 <= lat <= 53.55 and 73.66 <= lng <= 135.05):
                    logger.warning(f"Invalid coordinates: {lat}, {lng}")
                    data["latitude"] = None
                    data["longitude"] = None
                
            except (ValueError, TypeError):
                data["latitude"] = None
                data["longitude"] = None
        
        return data
    
    async def _platform_specific_clean(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¹³å°ç‰¹å®šæ¸…æ´—"""
        platform = data.get("platform", "")
        
        if platform == "amap":
            return await self._clean_amap_data(data)
        elif platform == "dianping":
            return await self._clean_dianping_data(data)
        elif platform == "mafengwo":
            return await self._clean_mafengwo_data(data)
        
        return data
    
    async def _clean_amap_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """é«˜å¾·åœ°å›¾æ•°æ®æ¸…æ´—"""
        # å¤„ç†é«˜å¾·ç‰¹å®šçš„ç±»å‹ç¼–ç 
        if "category" in data:
            category_map = {
                "050000": "é¤é¥®æœåŠ¡",
                "100000": "ä½å®¿æœåŠ¡",
                "110000": "é£æ™¯åèƒœ",
                "060000": "è´­ç‰©æœåŠ¡"
            }
            
            if data["category"] in category_map:
                data["category"] = category_map[data["category"]]
        
        return data
    
    async def _clean_dianping_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¤§ä¼—ç‚¹è¯„æ•°æ®æ¸…æ´—"""
        # å¤„ç†è¯„åˆ†
        if "rating" in data and isinstance(data["rating"], str):
            # å¤§ä¼—ç‚¹è¯„å¯èƒ½è¿”å›æ˜Ÿçº§æè¿°
            star_map = {
                "äº”æ˜Ÿå•†æˆ·": 5.0,
                "å››æ˜Ÿå•†æˆ·": 4.0,
                "ä¸‰æ˜Ÿå•†æˆ·": 3.0,
                "å‡†å››æ˜Ÿå•†æˆ·": 3.5,
                "å‡†äº”æ˜Ÿå•†æˆ·": 4.5
            }
            
            data["rating"] = star_map.get(data["rating"], 0.0)
        
        return data
    
    async def _clean_mafengwo_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """é©¬èœ‚çªæ•°æ®æ¸…æ´—"""
        # å¤„ç†é©¬èœ‚çªçš„ç‰¹æ®Šå­—æ®µ
        if "poi_type" in data:
            data["category"] = data.pop("poi_type")
        
        return data
```

**éªŒè¯æ ‡å‡†**ï¼šæ•°æ®æ¸…æ´—æ¡†æ¶åˆ›å»ºå®Œæˆï¼Œæ”¯æŒå¤šè§„åˆ™é“¾å¼æ¸…æ´—

#### 5.1.1.2 å®ç°æ‰¹é‡æ¸…æ´—å¤„ç†å™¨ [Time: 2h]
```python
# src/processors/cleaner/batch_cleaner.py
import asyncio
from typing import List, Dict, Any, Type
from src.processors.cleaner.base_cleaner import DataCleaner, POICleaner
from src.utils.logger.logger import get_logger

logger = get_logger("processor.batch_cleaner")

class BatchDataCleaner:
    """æ‰¹é‡æ•°æ®æ¸…æ´—å™¨"""
    
    def __init__(self):
        self.cleaner_registry = {
            "poi": POICleaner,
            "review": ReviewCleaner,
            "media": MediaCleaner
        }
        self.batch_size = 100
        self.max_workers = 10
    
    async def clean_batch(
        self, 
        data_list: List[Dict[str, Any]], 
        data_type: str = "poi"
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡æ¸…æ´—æ•°æ®"""
        try:
            # è·å–æ¸…æ´—å™¨
            cleaner_class = self.cleaner_registry.get(data_type, POICleaner)
            cleaner = cleaner_class()
            
            # åˆ†æ‰¹å¤„ç†
            cleaned_data = []
            
            for i in range(0, len(data_list), self.batch_size):
                batch = data_list[i:i + self.batch_size]
                
                # å¹¶å‘æ¸…æ´—
                tasks = [cleaner.clean(data) for data in batch]
                batch_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # å¤„ç†ç»“æœ
                for j, result in enumerate(batch_results):
                    if isinstance(result, Exception):
                        logger.error(f"Cleaning failed for item {i+j}: {result}")
                        cleaned_data.append(batch[j])  # è¿”å›åŸæ•°æ®
                    else:
                        cleaned_data.append(result)
                
                # è¿›åº¦æ—¥å¿—
                progress = min(i + self.batch_size, len(data_list))
                logger.info(f"Cleaned {progress}/{len(data_list)} items")
            
            return cleaned_data
            
        except Exception as e:
            logger.error(f"Batch cleaning failed: {e}")
            return data_list
    
    def register_cleaner(self, data_type: str, cleaner_class: Type[DataCleaner]):
        """æ³¨å†Œæ–°çš„æ¸…æ´—å™¨"""
        self.cleaner_registry[data_type] = cleaner_class
        logger.info(f"Registered cleaner for {data_type}")

class ReviewCleaner(DataCleaner):
    """è¯„è®ºæ•°æ®æ¸…æ´—å™¨"""
    
    def _init_rules(self):
        self.cleaning_rules = [
            self._clean_review_content,
            self._clean_reviewer_info,
            self._normalize_rating
        ]
    
    async def _clean_review_content(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…æ´—è¯„è®ºå†…å®¹"""
        if "content" in data:
            content = data["content"]
            
            # ç§»é™¤è¡¨æƒ…ç¬¦å·
            content = re.sub(r'[\U00010000-\U0010ffff]', '', content)
            
            # ç§»é™¤é‡å¤å­—ç¬¦
            content = re.sub(r'(.)\1{3,}', r'\1\1\1', content)
            
            # ç§»é™¤å¹¿å‘Šé“¾æ¥
            content = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', content)
            
            data["content"] = content.strip()
        
        return data
    
    async def _clean_reviewer_info(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…æ´—è¯„è®ºè€…ä¿¡æ¯"""
        if "reviewer_name" in data:
            # è„±æ•å¤„ç†
            name = data["reviewer_name"]
            if len(name) > 1:
                data["reviewer_name"] = name[0] + "*" * (len(name) - 1)
        
        return data
    
    async def _normalize_rating(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–è¯„åˆ†"""
        if "rating" in data:
            try:
                rating = float(data["rating"])
                data["rating"] = max(0, min(5, rating))  # é™åˆ¶åœ¨0-5èŒƒå›´
            except:
                data["rating"] = 0
        
        return data
    
    async def _platform_specific_clean(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¹³å°ç‰¹å®šæ¸…æ´—"""
        return data

class MediaCleaner(DataCleaner):
    """åª’ä½“æ•°æ®æ¸…æ´—å™¨"""
    
    def _init_rules(self):
        self.cleaning_rules = [
            self._clean_media_url,
            self._clean_media_metadata,
            self._validate_media_type
        ]
    
    async def _clean_media_url(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…æ´—åª’ä½“URL"""
        if "url" in data:
            url = data["url"]
            
            # ç§»é™¤æŸ¥è¯¢å‚æ•°
            if "?" in url:
                url = url.split("?")[0]
            
            # ç¡®ä¿HTTPS
            if url.startswith("http://"):
                url = url.replace("http://", "https://")
            
            data["url"] = url
        
        return data
    
    async def _clean_media_metadata(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…æ´—åª’ä½“å…ƒæ•°æ®"""
        # æ ‡å‡†åŒ–å°ºå¯¸ä¿¡æ¯
        if "width" in data and "height" in data:
            try:
                data["width"] = int(data["width"])
                data["height"] = int(data["height"])
            except:
                data["width"] = 0
                data["height"] = 0
        
        return data
    
    async def _validate_media_type(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯åª’ä½“ç±»å‹"""
        valid_types = ["image", "video", "audio"]
        
        if "media_type" in data:
            if data["media_type"] not in valid_types:
                data["media_type"] = "unknown"
        
        return data
    
    async def _platform_specific_clean(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¹³å°ç‰¹å®šæ¸…æ´—"""
        return data
```

**éªŒè¯æ ‡å‡†**ï¼šæ‰¹é‡æ¸…æ´—å¤„ç†å™¨å®ç°å®Œæˆï¼Œæ”¯æŒå¹¶å‘æ‰¹é‡å¤„ç†

## 5.2 æ•°æ®å»é‡ç³»ç»Ÿ

### 5.2.1 å»é‡ç®—æ³•å®ç°

#### 5.2.1.1 å®ç°å»é‡ç®¡ç†å™¨ [Time: 3h]
```python
# src/processors/deduplicator/dedup_manager.py
import hashlib
import difflib
from typing import List, Dict, Any, Tuple, Optional
from datetime import datetime, timedelta
from src.core.database.connection import get_db
from src.core.redis.connection import redis_manager
from src.utils.logger.logger import get_logger

logger = get_logger("processor.deduplicator")

class DeduplicationManager:
    """å»é‡ç®¡ç†å™¨"""
    
    def __init__(self):
        self.similarity_threshold = 0.85
        self.hash_fields = ["name", "address", "latitude", "longitude"]
        self.cache_ttl = 3600  # 1å°æ—¶
    
    async def deduplicate(
        self, 
        items: List[Dict[str, Any]], 
        platform: str = None
    ) -> List[Dict[str, Any]]:
        """å»é‡å¤„ç†"""
        try:
            unique_items = []
            seen_hashes = set()
            
            for item in items:
                # ç”Ÿæˆå”¯ä¸€æ ‡è¯†
                item_hash = self._generate_hash(item)
                
                # å¿«é€Ÿå“ˆå¸Œå»é‡
                if item_hash in seen_hashes:
                    continue
                
                # æ•°æ®åº“å»é‡æ£€æŸ¥
                is_duplicate = await self._check_database_duplicate(item, platform)
                if is_duplicate:
                    continue
                
                # ç›¸ä¼¼åº¦å»é‡
                is_similar = await self._check_similarity(item, unique_items)
                if is_similar:
                    continue
                
                # æ·»åŠ åˆ°ç»“æœ
                seen_hashes.add(item_hash)
                unique_items.append(item)
            
            logger.info(f"Deduplication: {len(items)} -> {len(unique_items)} items")
            return unique_items
            
        except Exception as e:
            logger.error(f"Deduplication failed: {e}")
            return items
    
    def _generate_hash(self, item: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ•°æ®å“ˆå¸Œ"""
        hash_parts = []
        
        for field in self.hash_fields:
            if field in item:
                value = str(item[field]).lower().strip()
                hash_parts.append(value)
        
        hash_string = "|".join(hash_parts)
        return hashlib.md5(hash_string.encode()).hexdigest()
    
    async def _check_database_duplicate(
        self, 
        item: Dict[str, Any], 
        platform: str
    ) -> bool:
        """æ£€æŸ¥æ•°æ®åº“ä¸­çš„é‡å¤"""
        try:
            # å…ˆæ£€æŸ¥Redisç¼“å­˜
            cache_key = f"dedup:{platform}:{item.get('platform_poi_id', '')}"
            cached = await redis_manager.get_client().get(cache_key)
            
            if cached:
                return True
            
            # æ•°æ®åº“æŸ¥è¯¢
            async with get_db() as db:
                # ç²¾ç¡®åŒ¹é…
                if "platform_poi_id" in item:
                    existing = await db.query(POI).filter(
                        POI.platform == platform,
                        POI.platform_poi_id == item["platform_poi_id"]
                    ).first()
                    
                    if existing:
                        # ç¼“å­˜ç»“æœ
                        await redis_manager.get_client().setex(
                            cache_key, self.cache_ttl, "1"
                        )
                        return True
                
                # åæ ‡åŒ¹é…
                if "latitude" in item and "longitude" in item:
                    lat, lng = item["latitude"], item["longitude"]
                    
                    # æŸ¥æ‰¾é™„è¿‘100ç±³å†…çš„POI
                    nearby = await db.query(POI).filter(
                        func.ST_DWithin(
                            POI.location,
                            func.ST_MakePoint(lng, lat),
                            0.001  # çº¦100ç±³
                        )
                    ).all()
                    
                    for poi in nearby:
                        if self._is_same_poi(item, poi):
                            return True
            
            return False
            
        except Exception as e:
            logger.error(f"Database duplicate check failed: {e}")
            return False
    
    async def _check_similarity(
        self, 
        item: Dict[str, Any], 
        existing_items: List[Dict[str, Any]]
    ) -> bool:
        """æ£€æŸ¥ç›¸ä¼¼åº¦"""
        for existing in existing_items:
            similarity = self._calculate_similarity(item, existing)
            
            if similarity >= self.similarity_threshold:
                return True
        
        return False
    
    def _calculate_similarity(
        self, 
        item1: Dict[str, Any], 
        item2: Dict[str, Any]
    ) -> float:
        """è®¡ç®—ä¸¤ä¸ªPOIçš„ç›¸ä¼¼åº¦"""
        scores = []
        
        # åç§°ç›¸ä¼¼åº¦ï¼ˆæƒé‡0.4ï¼‰
        if "name" in item1 and "name" in item2:
            name_sim = difflib.SequenceMatcher(
                None, 
                item1["name"].lower(), 
                item2["name"].lower()
            ).ratio()
            scores.append(name_sim * 0.4)
        
        # åœ°å€ç›¸ä¼¼åº¦ï¼ˆæƒé‡0.3ï¼‰
        if "address" in item1 and "address" in item2:
            addr_sim = difflib.SequenceMatcher(
                None,
                item1["address"].lower(),
                item2["address"].lower()
            ).ratio()
            scores.append(addr_sim * 0.3)
        
        # åæ ‡è·ç¦»ï¼ˆæƒé‡0.3ï¼‰
        if all(k in item1 and k in item2 for k in ["latitude", "longitude"]):
            distance = self._calculate_distance(
                item1["latitude"], item1["longitude"],
                item2["latitude"], item2["longitude"]
            )
            
            # è·ç¦»å°äº100ç±³è®¤ä¸ºæ˜¯åŒä¸€åœ°ç‚¹
            if distance < 0.1:
                scores.append(0.3)
            else:
                scores.append(0)
        
        return sum(scores)
    
    def _calculate_distance(
        self, 
        lat1: float, 
        lng1: float, 
        lat2: float, 
        lng2: float
    ) -> float:
        """è®¡ç®—ä¸¤ç‚¹è·ç¦»ï¼ˆç®€åŒ–ç‰ˆï¼Œå•ä½ï¼šå…¬é‡Œï¼‰"""
        import math
        
        R = 6371  # åœ°çƒåŠå¾„
        
        lat1_rad = math.radians(lat1)
        lat2_rad = math.radians(lat2)
        delta_lat = math.radians(lat2 - lat1)
        delta_lng = math.radians(lng2 - lng1)
        
        a = (math.sin(delta_lat / 2) ** 2 + 
             math.cos(lat1_rad) * math.cos(lat2_rad) * 
             math.sin(delta_lng / 2) ** 2)
        
        c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))
        
        return R * c
    
    def _is_same_poi(self, item: Dict[str, Any], poi: POI) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºåŒä¸€POI"""
        # åç§°å®Œå…¨ç›¸åŒ
        if item.get("name") == poi.name:
            return True
        
        # è®¡ç®—ç»¼åˆç›¸ä¼¼åº¦
        poi_dict = {
            "name": poi.name,
            "address": poi.address,
            "latitude": poi.latitude,
            "longitude": poi.longitude
        }
        
        similarity = self._calculate_similarity(item, poi_dict)
        return similarity >= self.similarity_threshold
```

**éªŒè¯æ ‡å‡†**ï¼šå»é‡ç®¡ç†å™¨å®ç°å®Œæˆï¼Œæ”¯æŒå¤šç»´åº¦å»é‡ç­–ç•¥

#### 5.2.1.2 å®ç°å¢é‡å»é‡å¤„ç† [Time: 2h]
```python
# src/processors/deduplicator/incremental_dedup.py
import asyncio
from typing import List, Dict, Any, Set
from datetime import datetime, timedelta
from src.processors.deduplicator.dedup_manager import DeduplicationManager
from src.core.models.poi import POI
from src.utils.logger.logger import get_logger

logger = get_logger("processor.incremental_dedup")

class IncrementalDeduplicator:
    """å¢é‡å»é‡å¤„ç†å™¨"""
    
    def __init__(self):
        self.dedup_manager = DeduplicationManager()
        self.bloom_filter_size = 1000000  # å¸ƒéš†è¿‡æ»¤å™¨å¤§å°
        self.bloom_filter_error_rate = 0.01
        self._init_bloom_filter()
    
    def _init_bloom_filter(self):
        """åˆå§‹åŒ–å¸ƒéš†è¿‡æ»¤å™¨"""
        try:
            from pybloom_live import BloomFilter
            self.bloom_filter = BloomFilter(
                capacity=self.bloom_filter_size,
                error_rate=self.bloom_filter_error_rate
            )
            logger.info("Bloom filter initialized")
        except ImportError:
            logger.warning("pybloom_live not installed, using set instead")
            self.bloom_filter = set()
    
    async def process_incremental(
        self, 
        new_items: List[Dict[str, Any]], 
        platform: str
    ) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """å¤„ç†å¢é‡æ•°æ®"""
        try:
            unique_items = []
            duplicate_items = []
            
            # æ‰¹é‡æ£€æŸ¥
            for item in new_items:
                item_id = self._get_item_id(item)
                
                # å¸ƒéš†è¿‡æ»¤å™¨å¿«é€Ÿæ£€æŸ¥
                if self._bloom_filter_check(item_id):
                    # å¯èƒ½é‡å¤ï¼Œéœ€è¦è¿›ä¸€æ­¥éªŒè¯
                    is_duplicate = await self._verify_duplicate(item, platform)
                    
                    if is_duplicate:
                        duplicate_items.append(item)
                        continue
                
                # æ·»åŠ åˆ°å¸ƒéš†è¿‡æ»¤å™¨
                self._bloom_filter_add(item_id)
                unique_items.append(item)
            
            # äºŒæ¬¡å»é‡
            unique_items = await self.dedup_manager.deduplicate(unique_items, platform)
            
            logger.info(
                f"Incremental dedup: {len(new_items)} items, "
                f"{len(unique_items)} unique, {len(duplicate_items)} duplicates"
            )
            
            return unique_items, duplicate_items
            
        except Exception as e:
            logger.error(f"Incremental deduplication failed: {e}")
            return new_items, []
    
    def _get_item_id(self, item: Dict[str, Any]) -> str:
        """è·å–é¡¹ç›®æ ‡è¯†"""
        # ä¼˜å…ˆä½¿ç”¨å¹³å°ID
        if "platform_poi_id" in item:
            return f"{item.get('platform', '')}:{item['platform_poi_id']}"
        
        # ä½¿ç”¨åç§°å’Œåæ ‡
        parts = [
            item.get("name", ""),
            str(item.get("latitude", "")),
            str(item.get("longitude", ""))
        ]
        
        return ":".join(parts)
    
    def _bloom_filter_check(self, item_id: str) -> bool:
        """å¸ƒéš†è¿‡æ»¤å™¨æ£€æŸ¥"""
        if isinstance(self.bloom_filter, set):
            return item_id in self.bloom_filter
        else:
            return item_id in self.bloom_filter
    
    def _bloom_filter_add(self, item_id: str):
        """æ·»åŠ åˆ°å¸ƒéš†è¿‡æ»¤å™¨"""
        if isinstance(self.bloom_filter, set):
            self.bloom_filter.add(item_id)
        else:
            self.bloom_filter.add(item_id)
    
    async def _verify_duplicate(
        self, 
        item: Dict[str, Any], 
        platform: str
    ) -> bool:
        """éªŒè¯æ˜¯å¦çœŸçš„é‡å¤"""
        # ä½¿ç”¨å»é‡ç®¡ç†å™¨çš„æ•°æ®åº“æ£€æŸ¥
        return await self.dedup_manager._check_database_duplicate(item, platform)
    
    async def sync_bloom_filter(self, platform: str = None):
        """åŒæ­¥å¸ƒéš†è¿‡æ»¤å™¨ä¸æ•°æ®åº“"""
        try:
            async with get_db() as db:
                # æŸ¥è¯¢ç°æœ‰POI
                query = db.query(POI.platform, POI.platform_poi_id)
                
                if platform:
                    query = query.filter(POI.platform == platform)
                
                # æ‰¹é‡å¤„ç†
                offset = 0
                batch_size = 10000
                
                while True:
                    pois = await query.offset(offset).limit(batch_size).all()
                    
                    if not pois:
                        break
                    
                    # æ·»åŠ åˆ°å¸ƒéš†è¿‡æ»¤å™¨
                    for poi in pois:
                        item_id = f"{poi.platform}:{poi.platform_poi_id}"
                        self._bloom_filter_add(item_id)
                    
                    offset += batch_size
                    logger.info(f"Synced {offset} POIs to bloom filter")
            
            logger.info("Bloom filter sync completed")
            
        except Exception as e:
            logger.error(f"Bloom filter sync failed: {e}")
```

**éªŒè¯æ ‡å‡†**ï¼šå¢é‡å»é‡å¤„ç†å®ç°å®Œæˆï¼Œæ”¯æŒå¸ƒéš†è¿‡æ»¤å™¨ä¼˜åŒ–

## 5.3 æ•°æ®å¢å¼ºç³»ç»Ÿ

### 5.3.1 æ•°æ®å¢å¼ºå¤„ç†

#### 5.3.1.1 å®ç°æ•°æ®å¢å¼ºå™¨ [Time: 3h]
```python
# src/processors/enhancer/data_enhancer.py
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime
from src.utils.logger.logger import get_logger

logger = get_logger("processor.enhancer")

class DataEnhancer:
    """æ•°æ®å¢å¼ºå™¨"""
    
    def __init__(self):
        self.enhancement_rules = [
            self._enhance_category,
            self._enhance_business_hours,
            self._enhance_price_level,
            self._enhance_tags,
            self._calculate_popularity_score
        ]
    
    async def enhance(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºæ•°æ®"""
        try:
            enhanced_data = data.copy()
            
            # åº”ç”¨æ‰€æœ‰å¢å¼ºè§„åˆ™
            for rule in self.enhancement_rules:
                enhanced_data = await rule(enhanced_data)
            
            # æ·»åŠ å¢å¼ºå…ƒæ•°æ®
            enhanced_data["enhanced_at"] = datetime.utcnow().isoformat()
            enhanced_data["enhancement_version"] = "1.0"
            
            return enhanced_data
            
        except Exception as e:
            logger.error(f"Data enhancement failed: {e}")
            return data
    
    async def _enhance_category(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºåˆ†ç±»ä¿¡æ¯"""
        if "category" in data:
            # æ ‡å‡†åŒ–åˆ†ç±»
            category_mapping = {
                "é¤é¥®": ["restaurant", "food", "dining"],
                "ä½å®¿": ["hotel", "accommodation", "lodging"],
                "æ™¯ç‚¹": ["attraction", "scenic_spot", "tourist_attraction"],
                "è´­ç‰©": ["shopping", "mall", "store"],
                "å¨±ä¹": ["entertainment", "recreation", "leisure"]
            }
            
            # æ·»åŠ è‹±æ–‡æ ‡ç­¾
            for cn_cat, en_cats in category_mapping.items():
                if cn_cat in data["category"]:
                    if "category_tags" not in data:
                        data["category_tags"] = []
                    data["category_tags"].extend(en_cats)
                    break
        
        return data
    
    async def _enhance_business_hours(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºè¥ä¸šæ—¶é—´"""
        if "business_hours" in data and data["business_hours"]:
            try:
                # è§£æè¥ä¸šæ—¶é—´
                hours = data["business_hours"]
                
                # åˆ¤æ–­æ˜¯å¦24å°æ—¶è¥ä¸š
                if "24" in hours or "å…¨å¤©" in hours:
                    data["is_24_hours"] = True
                else:
                    data["is_24_hours"] = False
                
                # åˆ¤æ–­å½“å‰æ˜¯å¦è¥ä¸š
                data["is_open_now"] = await self._check_if_open(hours)
                
            except Exception as e:
                logger.debug(f"Failed to enhance business hours: {e}")
        
        return data
    
    async def _enhance_price_level(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºä»·æ ¼ç­‰çº§"""
        if "price" in data:
            try:
                price = float(data["price"])
                
                # æ ¹æ®ä»·æ ¼åˆ¤æ–­ç­‰çº§
                if price < 50:
                    data["price_level"] = 1  # ç»æµ
                elif price < 100:
                    data["price_level"] = 2  # é€‚ä¸­
                elif price < 200:
                    data["price_level"] = 3  # åè´µ
                else:
                    data["price_level"] = 4  # æ˜‚è´µ
                    
            except (ValueError, TypeError):
                pass
        
        return data
    
    async def _enhance_tags(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºæ ‡ç­¾"""
        tags = []
        
        # åŸºäºè¯„åˆ†çš„æ ‡ç­¾
        if "rating" in data:
            rating = data.get("rating", 0)
            if rating >= 4.5:
                tags.append("highly_rated")
            elif rating >= 4.0:
                tags.append("well_rated")
        
        # åŸºäºè¯„è®ºæ•°çš„æ ‡ç­¾
        if "review_count" in data:
            count = data.get("review_count", 0)
            if count >= 1000:
                tags.append("popular")
            elif count >= 500:
                tags.append("trending")
        
        # åŸºäºä½ç½®çš„æ ‡ç­¾
        if "district" in data:
            if any(area in data["district"] for area in ["å¸‚ä¸­å¿ƒ", "å•†ä¸šåŒº", "CBD"]):
                tags.append("city_center")
        
        # åˆå¹¶æ ‡ç­¾
        if "tags" in data:
            data["tags"].extend(tags)
        else:
            data["tags"] = tags
        
        # å»é‡
        data["tags"] = list(set(data["tags"]))
        
        return data
    
    async def _calculate_popularity_score(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—äººæ°”åˆ†æ•°"""
        score = 0.0
        
        # åŸºäºè¯„åˆ†ï¼ˆæƒé‡0.4ï¼‰
        if "rating" in data:
            rating = data.get("rating", 0)
            score += (rating / 5.0) * 0.4
        
        # åŸºäºè¯„è®ºæ•°ï¼ˆæƒé‡0.3ï¼‰
        if "review_count" in data:
            count = data.get("review_count", 0)
            # å¯¹æ•°ç¼©æ”¾
            import math
            normalized_count = math.log10(count + 1) / 4  # å‡è®¾10000ä¸ºæœ€é«˜
            score += min(normalized_count, 1.0) * 0.3
        
        # åŸºäºå›¾ç‰‡æ•°ï¼ˆæƒé‡0.2ï¼‰
        if "images" in data:
            image_count = len(data.get("images", []))
            normalized_images = min(image_count / 20, 1.0)  # 20å¼ å›¾ç‰‡ä¸ºæ»¡åˆ†
            score += normalized_images * 0.2
        
        # åŸºäºæ ‡ç­¾æ•°ï¼ˆæƒé‡0.1ï¼‰
        if "tags" in data:
            tag_count = len(data.get("tags", []))
            normalized_tags = min(tag_count / 10, 1.0)  # 10ä¸ªæ ‡ç­¾ä¸ºæ»¡åˆ†
            score += normalized_tags * 0.1
        
        data["popularity_score"] = round(score, 2)
        
        return data
    
    async def _check_if_open(self, business_hours: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¥ä¸šä¸­"""
        # ç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥è§£æè¥ä¸šæ—¶é—´å¹¶å¯¹æ¯”å½“å‰æ—¶é—´
        from datetime import datetime
        
        current_hour = datetime.now().hour
        
        # ç®€å•åˆ¤æ–­
        if "24" in business_hours or "å…¨å¤©" in business_hours:
            return True
        
        # ä¸€èˆ¬è¥ä¸šæ—¶é—´
        if 9 <= current_hour <= 22:
            return True
        
        return False

class POIEnhancer(DataEnhancer):
    """POIæ•°æ®å¢å¼ºå™¨"""
    
    def __init__(self):
        super().__init__()
        # æ·»åŠ POIç‰¹å®šçš„å¢å¼ºè§„åˆ™
        self.enhancement_rules.extend([
            self._enhance_location_info,
            self._enhance_nearby_info
        ])
    
    async def _enhance_location_info(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºä½ç½®ä¿¡æ¯"""
        if "latitude" in data and "longitude" in data:
            # æ·»åŠ geohash
            try:
                import geohash2
                data["geohash"] = geohash2.encode(
                    data["latitude"], 
                    data["longitude"], 
                    precision=7
                )
            except ImportError:
                pass
            
            # æ·»åŠ åæ ‡ç³»ä¿¡æ¯
            data["coordinate_system"] = "WGS84"
        
        return data
    
    async def _enhance_nearby_info(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼ºå‘¨è¾¹ä¿¡æ¯"""
        # è¿™é‡Œå¯ä»¥è°ƒç”¨åœ°å›¾APIè·å–å‘¨è¾¹ä¿¡æ¯
        # ç®€åŒ–å®ç°ï¼Œæ·»åŠ å ä½ä¿¡æ¯
        
        data["nearby_facilities"] = {
            "subway_stations": [],
            "bus_stops": [],
            "parking_lots": [],
            "landmarks": []
        }
        
        return data
```

**éªŒè¯æ ‡å‡†**ï¼šæ•°æ®å¢å¼ºå™¨å®ç°å®Œæˆï¼Œæ”¯æŒå¤šç»´åº¦æ•°æ®å¢å¼º

## 5.4 RESTful APIæœåŠ¡

### 5.4.1 APIæ¡†æ¶æ­å»º

#### 5.4.1.1 åˆ›å»ºAPIè·¯ç”±ç»“æ„ [Time: 3h]
```python
# src/api/v1/routers/base.py
from fastapi import APIRouter, Depends, HTTPException, Query
from typing import List, Optional, Dict, Any
from datetime import datetime
from pydantic import BaseModel, Field

# åŸºç¡€å“åº”æ¨¡å‹
class BaseResponse(BaseModel):
    """åŸºç¡€å“åº”æ¨¡å‹"""
    success: bool = True
    message: str = "Success"
    data: Any = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)

class PaginatedResponse(BaseResponse):
    """åˆ†é¡µå“åº”æ¨¡å‹"""
    total: int = 0
    page: int = 1
    page_size: int = 20
    total_pages: int = 0

# åŸºç¡€æŸ¥è¯¢å‚æ•°
class BaseQueryParams(BaseModel):
    """åŸºç¡€æŸ¥è¯¢å‚æ•°"""
    page: int = Field(default=1, ge=1, description="é¡µç ")
    page_size: int = Field(default=20, ge=1, le=100, description="æ¯é¡µæ•°é‡")
    sort_by: Optional[str] = Field(default=None, description="æ’åºå­—æ®µ")
    sort_order: Optional[str] = Field(default="desc", regex="^(asc|desc)$")

# src/api/v1/routers/poi.py
from fastapi import APIRouter, Depends, HTTPException, Query, Path
from typing import List, Optional
from sqlalchemy.orm import Session
from src.api.v1.routers.base import BaseResponse, PaginatedResponse, BaseQueryParams
from src.api.v1.schemas.poi import POICreate, POIUpdate, POIResponse, POISearchParams
from src.api.v1.services.poi_service import POIService
from src.api.v1.dependencies import get_db, get_current_user
from src.utils.logger.logger import get_logger

logger = get_logger("api.poi")

router = APIRouter(prefix="/pois", tags=["POI"])

@router.get("/search", response_model=PaginatedResponse)
async def search_pois(
    keyword: str = Query(..., description="æœç´¢å…³é”®è¯"),
    city: Optional[str] = Query(None, description="åŸå¸‚"),
    category: Optional[str] = Query(None, description="åˆ†ç±»"),
    platforms: List[str] = Query(default=[], description="å¹³å°åˆ—è¡¨"),
    query_params: BaseQueryParams = Depends(),
    db: Session = Depends(get_db),
    current_user = Depends(get_current_user)
):
    """æœç´¢POI"""
    try:
        service = POIService(db)
        
        # æ„å»ºæœç´¢å‚æ•°
        search_params = POISearchParams(
            keyword=keyword,
            city=city,
            category=category,
            platforms=platforms,
            page=query_params.page,
            page_size=query_params.page_size
        )
        
        # æ‰§è¡Œæœç´¢
        results, total = await service.search_pois(search_params)
        
        # æ„å»ºå“åº”
        return PaginatedResponse(
            data=results,
            total=total,
            page=query_params.page,
            page_size=query_params.page_size,
            total_pages=(total + query_params.page_size - 1) // query_params.page_size
        )
        
    except Exception as e:
        logger.error(f"POI search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{poi_id}", response_model=BaseResponse)
async def get_poi(
    poi_id: int = Path(..., description="POI ID"),
    db: Session = Depends(get_db),
    current_user = Depends(get_current_user)
):
    """è·å–POIè¯¦æƒ…"""
    try:
        service = POIService(db)
        poi = await service.get_poi_by_id(poi_id)
        
        if not poi:
            raise HTTPException(status_code=404, detail="POI not found")
        
        return BaseResponse(data=poi)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Get POI failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/", response_model=BaseResponse)
async def create_poi(
    poi_data: POICreate,
    db: Session = Depends(get_db),
    current_user = Depends(get_current_user)
):
    """åˆ›å»ºPOI"""
    try:
        service = POIService(db)
        poi = await service.create_poi(poi_data.dict())
        
        return BaseResponse(
            data=poi,
            message="POI created successfully"
        )
        
    except Exception as e:
        logger.error(f"Create POI failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.put("/{poi_id}", response_model=BaseResponse)
async def update_poi(
    poi_id: int = Path(..., description="POI ID"),
    poi_data: POIUpdate = None,
    db: Session = Depends(get_db),
    current_user = Depends(get_current_user)
):
    """æ›´æ–°POI"""
    try:
        service = POIService(db)
        poi = await service.update_poi(poi_id, poi_data.dict(exclude_unset=True))
        
        if not poi:
            raise HTTPException(status_code=404, detail="POI not found")
        
        return BaseResponse(
            data=poi,
            message="POI updated successfully"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Update POI failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/{poi_id}", response_model=BaseResponse)
async def delete_poi(
    poi_id: int = Path(..., description="POI ID"),
    db: Session = Depends(get_db),
    current_user = Depends(get_current_user)
):
    """åˆ é™¤POI"""
    try:
        service = POIService(db)
        success = await service.delete_poi(poi_id)
        
        if not success:
            raise HTTPException(status_code=404, detail="POI not found")
        
        return BaseResponse(message="POI deleted successfully")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Delete POI failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/nearby", response_model=PaginatedResponse)
async def get_nearby_pois(
    latitude: float = Query(..., description="çº¬åº¦"),
    longitude: float = Query(..., description="ç»åº¦"),
    radius: int = Query(default=1000, description="åŠå¾„ï¼ˆç±³ï¼‰"),
    category: Optional[str] = Query(None, description="åˆ†ç±»"),
    query_params: BaseQueryParams = Depends(),
    db: Session = Depends(get_db),
    current_user = Depends(get_current_user)
):
    """è·å–é™„è¿‘POI"""
    try:
        service = POIService(db)
        
        # æ‰§è¡Œé™„è¿‘æœç´¢
        results, total = await service.search_nearby(
            latitude=latitude,
            longitude=longitude,
            radius=radius,
            category=category,
            page=query_params.page,
            page_size=query_params.page_size
        )
        
        return PaginatedResponse(
            data=results,
            total=total,
            page=query_params.page,
            page_size=query_params.page_size,
            total_pages=(total + query_params.page_size - 1) // query_params.page_size
        )
        
    except Exception as e:
        logger.error(f"Nearby POI search failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))
```

**éªŒè¯æ ‡å‡†**ï¼šAPIè·¯ç”±ç»“æ„åˆ›å»ºå®Œæˆï¼Œæ”¯æŒå®Œæ•´çš„CRUDæ“ä½œ

#### 5.4.1.2 å®ç°APIæœåŠ¡å±‚ [Time: 3h]
```python
# src/api/v1/services/poi_service.py
from typing import List, Tuple, Optional, Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import and_, or_, func
from src.core.models.poi import POI
from src.api.v1.schemas.poi import POISearchParams
from src.processors.cleaner.base_cleaner import POICleaner
from src.processors.enhancer.data_enhancer import POIEnhancer
from src.utils.logger.logger import get_logger

logger = get_logger("api.service.poi")

class POIService:
    """POIæœåŠ¡å±‚"""
    
    def __init__(self, db: Session):
        self.db = db
        self.cleaner = POICleaner()
        self.enhancer = POIEnhancer()
    
    async def search_pois(
        self, 
        params: POISearchParams
    ) -> Tuple[List[Dict[str, Any]], int]:
        """æœç´¢POI"""
        try:
            # æ„å»ºæŸ¥è¯¢
            query = self.db.query(POI)
            
            # å…³é”®è¯æœç´¢
            if params.keyword:
                query = query.filter(
                    or_(
                        POI.name.ilike(f"%{params.keyword}%"),
                        POI.address.ilike(f"%{params.keyword}%"),
                        POI.description.ilike(f"%{params.keyword}%")
                    )
                )
            
            # åŸå¸‚è¿‡æ»¤
            if params.city:
                query = query.filter(POI.city == params.city)
            
            # åˆ†ç±»è¿‡æ»¤
            if params.category:
                query = query.filter(POI.category.ilike(f"%{params.category}%"))
            
            # å¹³å°è¿‡æ»¤
            if params.platforms:
                query = query.filter(POI.platform.in_(params.platforms))
            
            # åªè¿”å›æ¿€æ´»çš„POI
            query = query.filter(POI.is_active == True)
            
            # è·å–æ€»æ•°
            total = query.count()
            
            # åˆ†é¡µ
            offset = (params.page - 1) * params.page_size
            pois = query.offset(offset).limit(params.page_size).all()
            
            # è½¬æ¢ä¸ºå­—å…¸å¹¶å¢å¼º
            results = []
            for poi in pois:
                poi_dict = self._poi_to_dict(poi)
                enhanced = await self.enhancer.enhance(poi_dict)
                results.append(enhanced)
            
            return results, total
            
        except Exception as e:
            logger.error(f"POI search failed: {e}")
            raise
    
    async def get_poi_by_id(self, poi_id: int) -> Optional[Dict[str, Any]]:
        """æ ¹æ®IDè·å–POI"""
        try:
            poi = self.db.query(POI).filter(
                POI.id == poi_id,
                POI.is_active == True
            ).first()
            
            if not poi:
                return None
            
            # è½¬æ¢å¹¶å¢å¼º
            poi_dict = self._poi_to_dict(poi)
            enhanced = await self.enhancer.enhance(poi_dict)
            
            return enhanced
            
        except Exception as e:
            logger.error(f"Get POI by ID failed: {e}")
            raise
    
    async def create_poi(self, poi_data: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ›å»ºPOI"""
        try:
            # æ¸…æ´—æ•°æ®
            cleaned_data = await self.cleaner.clean(poi_data)
            
            # åˆ›å»ºPOIå¯¹è±¡
            poi = POI(**cleaned_data)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            self.db.add(poi)
            self.db.commit()
            self.db.refresh(poi)
            
            # è¿”å›å¢å¼ºåçš„æ•°æ®
            poi_dict = self._poi_to_dict(poi)
            enhanced = await self.enhancer.enhance(poi_dict)
            
            return enhanced
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"Create POI failed: {e}")
            raise
    
    async def update_poi(
        self, 
        poi_id: int, 
        update_data: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """æ›´æ–°POI"""
        try:
            poi = self.db.query(POI).filter(POI.id == poi_id).first()
            
            if not poi:
                return None
            
            # æ¸…æ´—æ›´æ–°æ•°æ®
            cleaned_data = await self.cleaner.clean(update_data)
            
            # æ›´æ–°å­—æ®µ
            for key, value in cleaned_data.items():
                if hasattr(poi, key):
                    setattr(poi, key, value)
            
            # ä¿å­˜
            self.db.commit()
            self.db.refresh(poi)
            
            # è¿”å›å¢å¼ºåçš„æ•°æ®
            poi_dict = self._poi_to_dict(poi)
            enhanced = await self.enhancer.enhance(poi_dict)
            
            return enhanced
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"Update POI failed: {e}")
            raise
    
    async def delete_poi(self, poi_id: int) -> bool:
        """åˆ é™¤POIï¼ˆè½¯åˆ é™¤ï¼‰"""
        try:
            poi = self.db.query(POI).filter(POI.id == poi_id).first()
            
            if not poi:
                return False
            
            # è½¯åˆ é™¤
            poi.is_active = False
            self.db.commit()
            
            return True
            
        except Exception as e:
            self.db.rollback()
            logger.error(f"Delete POI failed: {e}")
            raise
    
    async def search_nearby(
        self,
        latitude: float,
        longitude: float,
        radius: int,
        category: Optional[str] = None,
        page: int = 1,
        page_size: int = 20
    ) -> Tuple[List[Dict[str, Any]], int]:
        """æœç´¢é™„è¿‘POI"""
        try:
            # ä½¿ç”¨PostGISçš„ST_DWithinè¿›è¡Œåœ°ç†æŸ¥è¯¢
            point = func.ST_MakePoint(longitude, latitude)
            
            query = self.db.query(POI).filter(
                func.ST_DWithin(
                    POI.location,
                    point,
                    radius / 111000.0  # è½¬æ¢ä¸ºåº¦
                ),
                POI.is_active == True
            )
            
            # åˆ†ç±»è¿‡æ»¤
            if category:
                query = query.filter(POI.category.ilike(f"%{category}%"))
            
            # æŒ‰è·ç¦»æ’åº
            query = query.order_by(func.ST_Distance(POI.location, point))
            
            # è·å–æ€»æ•°
            total = query.count()
            
            # åˆ†é¡µ
            offset = (page - 1) * page_size
            pois = query.offset(offset).limit(page_size).all()
            
            # è½¬æ¢å¹¶å¢å¼º
            results = []
            for poi in pois:
                poi_dict = self._poi_to_dict(poi)
                
                # æ·»åŠ è·ç¦»ä¿¡æ¯
                distance = self.db.scalar(
                    func.ST_Distance(poi.location, point) * 111000  # è½¬æ¢ä¸ºç±³
                )
                poi_dict["distance"] = round(distance, 2)
                
                enhanced = await self.enhancer.enhance(poi_dict)
                results.append(enhanced)
            
            return results, total
            
        except Exception as e:
            logger.error(f"Nearby search failed: {e}")
            raise
    
    def _poi_to_dict(self, poi: POI) -> Dict[str, Any]:
        """POIå¯¹è±¡è½¬å­—å…¸"""
        return {
            "id": poi.id,
            "platform": poi.platform,
            "platform_poi_id": poi.platform_poi_id,
            "name": poi.name,
            "category": poi.category,
            "address": poi.address,
            "city": poi.city,
            "district": poi.district,
            "latitude": poi.latitude,
            "longitude": poi.longitude,
            "rating": poi.rating,
            "review_count": poi.review_count,
            "price": poi.price,
            "price_unit": poi.price_unit,
            "phone": poi.phone,
            "website": poi.website,
            "business_hours": poi.business_hours,
            "cover_image": poi.cover_image,
            "images": poi.images,
            "description": poi.description,
            "created_at": poi.created_at.isoformat() if poi.created_at else None,
            "updated_at": poi.updated_at.isoformat() if poi.updated_at else None
        }
```

**éªŒè¯æ ‡å‡†**ï¼šAPIæœåŠ¡å±‚å®ç°å®Œæˆï¼Œæ”¯æŒä¸šåŠ¡é€»è¾‘å¤„ç†

## 5.5 è®¤è¯æˆæƒç³»ç»Ÿ

### 5.5.1 JWTè®¤è¯å®ç°

#### 5.5.1.1 å®ç°è®¤è¯ç®¡ç†å™¨ [Time: 3h]
```python
# src/api/v1/auth/auth_manager.py
import jwt
import bcrypt
from datetime import datetime, timedelta
from typing import Optional, Dict, Any
from fastapi import HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from pydantic import BaseModel
from src.core.config.settings import settings
from src.utils.logger.logger import get_logger

logger = get_logger("api.auth")

# OAuth2é…ç½®
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="/api/v1/auth/token")

class TokenData(BaseModel):
    """Tokenæ•°æ®æ¨¡å‹"""
    access_token: str
    refresh_token: Optional[str] = None
    token_type: str = "bearer"
    expires_in: int = 3600

class UserCredentials(BaseModel):
    """ç”¨æˆ·å‡­è¯æ¨¡å‹"""
    username: str
    password: str

class AuthManager:
    """è®¤è¯ç®¡ç†å™¨"""
    
    def __init__(self):
        self.secret_key = settings.SECRET_KEY
        self.algorithm = "HS256"
        self.access_token_expire_minutes = 60
        self.refresh_token_expire_days = 7
    
    def create_access_token(
        self, 
        data: Dict[str, Any], 
        expires_delta: Optional[timedelta] = None
    ) -> str:
        """åˆ›å»ºè®¿é—®ä»¤ç‰Œ"""
        to_encode = data.copy()
        
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(minutes=self.access_token_expire_minutes)
        
        to_encode.update({
            "exp": expire,
            "iat": datetime.utcnow(),
            "type": "access"
        })
        
        encoded_jwt = jwt.encode(to_encode, self.secret_key, algorithm=self.algorithm)
        return encoded_jwt
    
    def create_refresh_token(self, data: Dict[str, Any]) -> str:
        """åˆ›å»ºåˆ·æ–°ä»¤ç‰Œ"""
        to_encode = data.copy()
        expire = datetime.utcnow() + timedelta(days=self.refresh_token_expire_days)
        
        to_encode.update({
            "exp": expire,
            "iat": datetime.utcnow(),
            "type": "refresh"
        })
        
        encoded_jwt = jwt.encode(to_encode, self.secret_key, algorithm=self.algorithm)
        return encoded_jwt
    
    def verify_token(self, token: str, token_type: str = "access") -> Dict[str, Any]:
        """éªŒè¯ä»¤ç‰Œ"""
        try:
            payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm])
            
            # éªŒè¯ä»¤ç‰Œç±»å‹
            if payload.get("type") != token_type:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid token type"
                )
            
            return payload
            
        except jwt.ExpiredSignatureError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Token has expired"
            )
        except jwt.JWTError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid token"
            )
    
    def hash_password(self, password: str) -> str:
        """å¯†ç å“ˆå¸Œ"""
        salt = bcrypt.gensalt()
        hashed = bcrypt.hashpw(password.encode('utf-8'), salt)
        return hashed.decode('utf-8')
    
    def verify_password(self, plain_password: str, hashed_password: str) -> bool:
        """éªŒè¯å¯†ç """
        return bcrypt.checkpw(
            plain_password.encode('utf-8'),
            hashed_password.encode('utf-8')
        )
    
    async def authenticate_user(
        self, 
        username: str, 
        password: str
    ) -> Optional[Dict[str, Any]]:
        """è®¤è¯ç”¨æˆ·"""
        # è¿™é‡Œåº”è¯¥æŸ¥è¯¢æ•°æ®åº“
        # ç®€åŒ–ç¤ºä¾‹
        if username == "admin" and password == "admin123":
            return {
                "user_id": 1,
                "username": "admin",
                "email": "admin@example.com",
                "role": "admin"
            }
        
        return None
    
    def create_token_response(self, user_data: Dict[str, Any]) -> TokenData:
        """åˆ›å»ºä»¤ç‰Œå“åº”"""
        # åˆ›å»ºè®¿é—®ä»¤ç‰Œ
        access_token = self.create_access_token(
            data={"sub": str(user_data["user_id"]), "username": user_data["username"]}
        )
        
        # åˆ›å»ºåˆ·æ–°ä»¤ç‰Œ
        refresh_token = self.create_refresh_token(
            data={"sub": str(user_data["user_id"])}
        )
        
        return TokenData(
            access_token=access_token,
            refresh_token=refresh_token,
            expires_in=self.access_token_expire_minutes * 60
        )

# å…¨å±€è®¤è¯ç®¡ç†å™¨
auth_manager = AuthManager()

# src/api/v1/auth/dependencies.py
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from typing import Dict, Any
from src.api.v1.auth.auth_manager import auth_manager, oauth2_scheme
from src.core.models.user import User
from src.core.database.connection import get_db

async def get_current_user(
    token: str = Depends(oauth2_scheme),
    db = Depends(get_db)
) -> User:
    """è·å–å½“å‰ç”¨æˆ·"""
    # éªŒè¯ä»¤ç‰Œ
    payload = auth_manager.verify_token(token)
    
    # è·å–ç”¨æˆ·ID
    user_id = payload.get("sub")
    if not user_id:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid token payload"
        )
    
    # æŸ¥è¯¢ç”¨æˆ·
    user = await db.query(User).filter(User.id == int(user_id)).first()
    
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="User not found"
        )
    
    if not user.is_active:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Inactive user"
        )
    
    return user

async def require_admin(current_user: User = Depends(get_current_user)) -> User:
    """éœ€è¦ç®¡ç†å‘˜æƒé™"""
    if current_user.role != "admin":
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Admin permission required"
        )
    
    return current_user
```

**éªŒè¯æ ‡å‡†**ï¼šJWTè®¤è¯ç³»ç»Ÿå®ç°å®Œæˆï¼Œæ”¯æŒä»¤ç‰Œç”Ÿæˆå’ŒéªŒè¯

---

## éªŒè¯å’Œæµ‹è¯•è¦æ±‚

### é˜¶æ®µäº”éªŒè¯æ¸…å•
- [ ] æ•°æ®æ¸…æ´—åŠŸèƒ½æµ‹è¯•é€šè¿‡
- [ ] å»é‡ç®—æ³•å‡†ç¡®ç‡ > 95%
- [ ] æ•°æ®å¢å¼ºåŠŸèƒ½æ­£å¸¸
- [ ] APIæ¥å£å“åº”æ—¶é—´ < 1ç§’
- [ ] è®¤è¯æˆæƒç³»ç»Ÿå®‰å…¨

### ä»£ç è´¨é‡è¦æ±‚
- ä»£ç è¦†ç›–ç‡ > 85%
- APIæ–‡æ¡£å®Œæ•´
- é”™è¯¯å¤„ç†å®Œå–„
- æ—¥å¿—è®°å½•è¯¦ç»†

### æ€§èƒ½è¦æ±‚
- æ‰¹é‡æ•°æ®å¤„ç† > 1000æ¡/ç§’
- APIå¹¶å‘æ”¯æŒ > 100 QPS
- å“åº”æ—¶é—´ P95 < 500ms

---

**æ³¨æ„**ï¼šæœ¬æ–‡æ¡£ä¸ºç¬¬äºŒéƒ¨åˆ†ï¼ŒåŒ…å«ç¬¬5é˜¶æ®µçš„è¯¦ç»†å®ç°ã€‚ç¬¬6-7é˜¶æ®µå°†ç»§ç»­åœ¨åç»­éƒ¨åˆ†ä¸­è¯¦ç»†è¯´æ˜ã€‚

æ¯ä¸ªä»»åŠ¡éƒ½åŒ…å«å…·ä½“çš„å®ç°ä»£ç ã€éªŒè¯æ ‡å‡†å’Œé¢„ä¼°å·¥æ—¶ï¼Œå¯ä»¥ç›´æ¥ç”¨äºæŒ‡å¯¼AIç¼–ç¨‹åŠ©æ‰‹è¿›è¡Œå¼€å‘å·¥ä½œã€‚