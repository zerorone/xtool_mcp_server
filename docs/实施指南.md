# Zen MCP Server æ™ºèƒ½è®°å¿†å¢å¼ºä¸æ€ç»´å·¥å…·ç®±ç³»ç»Ÿ - å®Œæ•´å®æ–½è®¡åˆ’ v2.0

## ğŸ¯ é¡¹ç›®æ„¿æ™¯ä¸ç›®æ ‡

### æ ¸å¿ƒæ„¿æ™¯
å°† Zen MCP Server å‡çº§ä¸ºå…·æœ‰**ä¸“å®¶çº§æ€ç»´çµé­‚**çš„æ™ºèƒ½å¼€å‘åŠ©æ‰‹ï¼Œé€šè¿‡æ·±åº¦é›†æˆ25ç§æ€ç»´æ¨¡å¼å’Œä¸‰å±‚è®°å¿†ç³»ç»Ÿï¼Œå®ç°çœŸæ­£çš„AIæ™ºæ…§ä¼™ä¼´ã€‚

### ç³»ç»Ÿç›®æ ‡
1. **æ€ç»´æ™ºèƒ½åŒ–**ï¼šé›†æˆ25ç§ä¸“å®¶æ€ç»´æ¨¡å¼ï¼Œå®ç°å¤šç»´åº¦æ·±åº¦åˆ†æ
2. **è®°å¿†æŒä¹…åŒ–**ï¼šä¸‰å±‚è®°å¿†ä½“ç³»ï¼ˆå…¨å±€/é¡¹ç›®/ä¼šè¯ï¼‰ï¼Œæ™ºèƒ½å­˜å‚¨å’Œæ£€ç´¢
3. **ä»»åŠ¡è‡ªåŠ¨åŒ–**ï¼šTODOé©±åŠ¨å¼€å‘ï¼Œæ™ºèƒ½ä»»åŠ¡åˆ†æ”¯ç®¡ç†
4. **ç¯å¢ƒæ„ŸçŸ¥åŒ–**ï¼šè‡ªåŠ¨è¯†åˆ«é¡¹ç›®ç¯å¢ƒï¼Œæ™ºèƒ½è·¯å¾„æ¨è
5. **å­¦ä¹ è¿›åŒ–æ€§**ï¼šä»æ¯æ¬¡äº¤äº’ä¸­å­¦ä¹ ï¼ŒæŒç»­ä¼˜åŒ–å“åº”è´¨é‡

## ğŸ“Š ç³»ç»Ÿæ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„å›¾
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Zen MCP Server å¢å¼ºç³»ç»Ÿ                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      è§¦å‘ä¸åè°ƒå±‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ è‡ªåŠ¨è§¦å‘å™¨  â”‚  â”‚ ä¸Šä¸‹æ–‡ç®¡ç†å™¨  â”‚  â”‚  ä»»åŠ¡åè°ƒå™¨   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      æ ¸å¿ƒåŠŸèƒ½å±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚    å¢å¼ºè®°å¿†ç³»ç»Ÿ          â”‚  â”‚   æ€ç»´å·¥å…·ç®±ç³»ç»Ÿ     â”‚       â”‚
â”‚  â”‚  â”œâ”€ å…¨å±€è®°å¿†            â”‚  â”‚  â”œâ”€ 25ç§æ€ç»´æ¨¡å¼    â”‚       â”‚
â”‚  â”‚  â”œâ”€ é¡¹ç›®è®°å¿†            â”‚  â”‚  â”œâ”€ æ™ºèƒ½æ¨¡å¼é€‰æ‹©    â”‚       â”‚
â”‚  â”‚  â””â”€ ä¼šè¯è®°å¿†            â”‚  â”‚  â””â”€ æ·±åº¦åˆ†æå¼•æ“    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚    ä»»åŠ¡ç®¡ç†ç³»ç»Ÿ          â”‚  â”‚   è·¯å¾„æ™ºèƒ½ç³»ç»Ÿ       â”‚       â”‚
â”‚  â”‚  â”œâ”€ TODOè§£æå™¨          â”‚  â”‚  â”œâ”€ ç¯å¢ƒæ£€æµ‹å™¨      â”‚       â”‚
â”‚  â”‚  â”œâ”€ ä¸»çº¿ç»´æŠ¤å™¨          â”‚  â”‚  â”œâ”€ è·¯å¾„è®°å¿†å™¨      â”‚       â”‚
â”‚  â”‚  â””â”€ åˆ†æ”¯ç®¡ç†å™¨          â”‚  â”‚  â””â”€ æ™ºèƒ½æ¨èå™¨      â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                      åŸºç¡€è®¾æ–½å±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ å­˜å‚¨å¼•æ“  â”‚  â”‚ ç´¢å¼•ç³»ç»Ÿ  â”‚  â”‚ ç¼“å­˜ç³»ç»Ÿ  â”‚  â”‚ æ—¥å¿—ç³»ç»Ÿ  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶è¯¦è§£

#### 1. å¢å¼ºè®°å¿†ç³»ç»Ÿ
- **ä¸‰å±‚è®°å¿†æ¶æ„**ï¼šå…¨å±€åå¥½ â†’ é¡¹ç›®ç‰¹å®š â†’ ä¼šè¯ä¸´æ—¶
- **æ™ºèƒ½ç´¢å¼•**ï¼šå¤šç»´åº¦ç´¢å¼•æ”¯æŒå¿«é€Ÿæ£€ç´¢
- **è®°å¿†è¡°å‡**ï¼šåŸºäºè®¿é—®é¢‘ç‡å’Œæ—¶é—´çš„æ™ºèƒ½è¡°å‡
- **æ¨¡å¼è®°å¿†**ï¼šæˆåŠŸçš„æ€ç»´æ¨¡å¼ç»„åˆæŒä¹…åŒ–

#### 2. æ€ç»´å·¥å…·ç®±ç³»ç»Ÿ
- **25ç§ä¸“å®¶æ€ç»´æ¨¡å¼**ï¼šå®Œæ•´çš„æ€ç»´æ¨¡å¼åº“
- **æ™ºèƒ½æ¿€æ´»å¼•æ“**ï¼šåŸºäºä¸Šä¸‹æ–‡è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å¼
- **ç»„åˆä¼˜åŒ–**ï¼šå¤šæ¨¡å¼ååŒåˆ†æ
- **æ·±åº¦æ§åˆ¶**ï¼šæµ…å±‚åˆ°ä¸“å®¶çº§çš„çµæ´»æ·±åº¦

#### 3. ä»»åŠ¡ç®¡ç†ç³»ç»Ÿ
- **TODOæ–‡ä»¶è§£æ**ï¼šæ™ºèƒ½è§£æå’Œè·Ÿè¸ªä»»åŠ¡è¿›åº¦
- **ä¸»çº¿ä¿æŒ**ï¼šç¡®ä¿ä¸åç¦»ä¸»è¦ä»»åŠ¡
- **åˆ†æ”¯ç®¡ç†**ï¼šä¸´æ—¶ä»»åŠ¡çš„æ™ºèƒ½åˆ†æ”¯å’Œå›å½’
- **ä¸Šä¸‹æ–‡åˆ‡æ¢**ï¼šæ— ç¼çš„ä»»åŠ¡ä¸Šä¸‹æ–‡åˆ‡æ¢

#### 4. è·¯å¾„æ™ºèƒ½ç³»ç»Ÿ
- **ç¯å¢ƒè‡ªåŠ¨æ£€æµ‹**ï¼šè™šæ‹Ÿç¯å¢ƒã€é¡¹ç›®ç»“æ„è¯†åˆ«
- **æ™ºèƒ½è·¯å¾„æ¨è**ï¼šåŸºäºæ–‡ä»¶ç±»å‹å’Œé¡¹ç›®ç»“æ„
- **å­¦ä¹ ä¼˜åŒ–**ï¼šä»ç”¨æˆ·è¡Œä¸ºä¸­å­¦ä¹ è·¯å¾„åå¥½

## ğŸš€ è¯¦ç»†å®æ–½è®¡åˆ’

### ç¬¬ä¸€é˜¶æ®µï¼šæ ¸å¿ƒåŸºç¡€è®¾æ–½ï¼ˆDay 1-3ï¼‰

#### 1.1 å¢å¼ºè®°å¿†ç³»ç»Ÿå®ç°

```python
# utils/enhanced_memory.py
"""å¢å¼ºç‰ˆè®°å¿†ç³»ç»Ÿ - é›†æˆæ€ç»´æ¨¡å¼å’Œæ™ºèƒ½ç®¡ç†"""

import json
import os
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Set
from collections import defaultdict, Counter
from utils.conversation_memory import ConversationMemory
import numpy as np

class EnhancedMemory(ConversationMemory):
    """å¢å¼ºç‰ˆè®°å¿†ç³»ç»Ÿ - æ”¯æŒä¸‰å±‚è®°å¿†ã€æ€ç»´æ¨¡å¼å­¦ä¹ å’Œæ™ºèƒ½ç®¡ç†"""
    
    def __init__(self):
        super().__init__()
        
        # è®°å¿†å­˜å‚¨é…ç½®
        self.memory_base = Path(os.getenv("WORKSPACE_ROOT", ".")) / ".zen_memory"
        self.memory_base.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–å­˜å‚¨æ–‡ä»¶
        self.storage_files = {
            "global": self.memory_base / "global_memory.json",
            "project": self.memory_base / "project_memory.json",
            "thinking": self.memory_base / "thinking_patterns.json",
            "paths": self.memory_base / "path_memory.json",
            "todo": self.memory_base / "todo_state.json"
        }
        
        # åŠ è½½æ‰€æœ‰æŒä¹…åŒ–æ•°æ®
        self.memories = {
            "global": self._load_memory_file(self.storage_files["global"]),
            "project": self._load_memory_file(self.storage_files["project"]),
            "session": {}  # ä¼šè¯è®°å¿†ä¸æŒä¹…åŒ–
        }
        
        # åˆå§‹åŒ–å„å­ç³»ç»Ÿ
        self._init_thinking_system()
        self._init_path_system()
        self._init_todo_system()
        self._init_indexing_system()
        self._init_learning_system()
    
    def _init_thinking_system(self):
        """åˆå§‹åŒ–æ€ç»´æ¨¡å¼ç³»ç»Ÿ"""
        thinking_data = self._load_memory_file(self.storage_files["thinking"])
        
        # æ€ç»´æ¨¡å¼æ•ˆæœè¿½è¸ª
        self.pattern_effectiveness = defaultdict(lambda: {
            "success_count": 0,
            "total_count": 0,
            "avg_quality": 0.0,
            "problem_types": Counter(),
            "combinations": Counter(),
            "last_used": None
        })
        
        if "pattern_effectiveness" in thinking_data:
            for pattern, data in thinking_data["pattern_effectiveness"].items():
                self.pattern_effectiveness[pattern].update(data)
        
        # æ€ç»´æ¨¡å¼ç»„åˆè®°å½•
        self.successful_combinations = thinking_data.get("successful_combinations", [])
        
        # é—®é¢˜-æ¨¡å¼æ˜ å°„å­¦ä¹ 
        self.problem_pattern_mapping = defaultdict(Counter)
        if "problem_patterns" in thinking_data:
            for problem_type, patterns in thinking_data["problem_patterns"].items():
                self.problem_pattern_mapping[problem_type].update(patterns)
    
    def _init_path_system(self):
        """åˆå§‹åŒ–è·¯å¾„æ™ºèƒ½ç³»ç»Ÿ"""
        path_data = self._load_memory_file(self.storage_files["paths"])
        
        self.path_memory = {
            "virtual_envs": path_data.get("virtual_envs", {}),
            "project_structures": path_data.get("project_structures", {}),
            "file_patterns": path_data.get("file_patterns", {}),
            "command_contexts": path_data.get("command_contexts", {}),
            "common_locations": path_data.get("common_locations", defaultdict(list))
        }
        
        # è·¯å¾„ä½¿ç”¨é¢‘ç‡è¿½è¸ª
        self.path_usage = defaultdict(int)
        if "path_usage" in path_data:
            self.path_usage.update(path_data["path_usage"])
    
    def _init_todo_system(self):
        """åˆå§‹åŒ–TODOç®¡ç†ç³»ç»Ÿ"""
        todo_data = self._load_memory_file(self.storage_files["todo"])
        
        self.todo_manager = {
            "main_thread": todo_data.get("main_thread", []),
            "branches": todo_data.get("branches", {}),
            "current_context": todo_data.get("current_context", "main"),
            "task_history": todo_data.get("task_history", []),
            "completed_tasks": todo_data.get("completed_tasks", []),
            "task_dependencies": todo_data.get("task_dependencies", {})
        }
        
        # ä»»åŠ¡æ¨¡å¼å­¦ä¹ 
        self.task_patterns = todo_data.get("task_patterns", {
            "average_completion_time": {},
            "common_blockers": [],
            "success_patterns": []
        })
    
    def _init_indexing_system(self):
        """åˆå§‹åŒ–æ™ºèƒ½ç´¢å¼•ç³»ç»Ÿ"""
        self.indices = {
            "by_tag": defaultdict(set),
            "by_type": defaultdict(set),
            "by_pattern": defaultdict(set),
            "by_timestamp": defaultdict(set),
            "by_quality": defaultdict(set),
            "by_frequency": defaultdict(int)
        }
        
        # é‡å»ºæ‰€æœ‰ç´¢å¼•
        self._rebuild_all_indices()
    
    def _init_learning_system(self):
        """åˆå§‹åŒ–å­¦ä¹ ä¼˜åŒ–ç³»ç»Ÿ"""
        self.learning_metrics = {
            "recall_precision": [],  # å¬å›ç²¾ç¡®åº¦å†å²
            "pattern_success_rate": defaultdict(list),  # æ¨¡å¼æˆåŠŸç‡
            "user_satisfaction": [],  # ç”¨æˆ·æ»¡æ„åº¦ï¼ˆéšå¼ï¼‰
            "optimization_history": []  # ä¼˜åŒ–å†å²
        }
    
    # ========== æ ¸å¿ƒè®°å¿†æ“ä½œ ==========
    
    def save_enhanced_memory(self, content: Any, metadata: Dict[str, Any]) -> str:
        """ä¿å­˜å¢å¼ºè®°å¿†ï¼ŒåŒ…å«æ€ç»´æ¨¡å¼å’Œè´¨é‡è¯„åˆ†"""
        memory_id = self._generate_memory_id()
        
        memory_item = {
            "id": memory_id,
            "content": content,
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "type": metadata.get("type", "general"),
                "tags": metadata.get("tags", []),
                "thinking_modes": metadata.get("thinking_modes", []),
                "quality_score": metadata.get("quality_score", 0.5),
                "context": metadata.get("context", {}),
                "source": metadata.get("source", "user_interaction")
            },
            "usage": {
                "access_count": 0,
                "last_accessed": None,
                "usefulness_score": 0.5,
                "decay_factor": 1.0
            }
        }
        
        # å†³å®šå­˜å‚¨å±‚çº§
        memory_type = self._determine_memory_type(memory_item)
        
        # å­˜å‚¨åˆ°å¯¹åº”å±‚çº§
        if memory_type not in self.memories:
            memory_type = "session"
        
        if memory_id not in self.memories[memory_type]:
            self.memories[memory_type][memory_id] = memory_item
        
        # æ›´æ–°ç´¢å¼•
        self._update_indices(memory_item, memory_type)
        
        # æ›´æ–°æ€ç»´æ¨¡å¼æ•ˆæœ
        if metadata.get("thinking_modes"):
            self._update_pattern_learning(
                metadata["thinking_modes"],
                metadata.get("quality_score", 0.5),
                metadata.get("problem_type", "general")
            )
        
        # æŒä¹…åŒ–
        self._persist_memory(memory_type)
        
        return memory_id
    
    def recall_with_thinking(self, query: str, context: Dict[str, Any]) -> List[Dict]:
        """åŸºäºæŸ¥è¯¢å’Œæ€ç»´ä¸Šä¸‹æ–‡æ™ºèƒ½å¬å›è®°å¿†"""
        # æå–æŸ¥è¯¢ç‰¹å¾
        query_features = self._extract_query_features(query, context)
        
        # å¤šç­–ç•¥å¬å›
        candidates = []
        
        # 1. å…³é”®è¯åŒ¹é…
        keyword_matches = self._keyword_based_recall(query_features["keywords"])
        candidates.extend(keyword_matches)
        
        # 2. æ€ç»´æ¨¡å¼åŒ¹é…
        if "thinking_modes" in context:
            pattern_matches = self._pattern_based_recall(context["thinking_modes"])
            candidates.extend(pattern_matches)
        
        # 3. ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦åŒ¹é…
        context_matches = self._context_based_recall(query_features, context)
        candidates.extend(context_matches)
        
        # 4. æ—¶é—´ç›¸å…³æ€§åŒ¹é…
        if context.get("temporal_relevance"):
            temporal_matches = self._temporal_based_recall(context["temporal_window"])
            candidates.extend(temporal_matches)
        
        # å»é‡å’Œè¯„åˆ†
        unique_candidates = self._deduplicate_candidates(candidates)
        scored_candidates = self._score_and_rank(unique_candidates, query_features, context)
        
        # æ›´æ–°è®¿é—®ç»Ÿè®¡
        for candidate in scored_candidates[:10]:  # åªæ›´æ–°å‰10ä¸ª
            self._update_access_stats(candidate["id"], candidate["memory_type"])
        
        # å­¦ä¹ åé¦ˆ
        self._learn_from_recall(query_features, scored_candidates)
        
        return scored_candidates
    
    # ========== è·¯å¾„æ™ºèƒ½åŠŸèƒ½ ==========
    
    def detect_and_learn_environment(self, project_path: str) -> Dict[str, Any]:
        """æ£€æµ‹å¹¶å­¦ä¹ é¡¹ç›®ç¯å¢ƒ"""
        project_path = Path(project_path)
        environment_info = {
            "project_root": str(project_path),
            "detected_at": datetime.now().isoformat(),
            "virtual_env": None,
            "project_type": None,
            "structure": {},
            "dependencies": []
        }
        
        # æ£€æµ‹è™šæ‹Ÿç¯å¢ƒ
        venv_info = self._detect_virtual_environment(project_path)
        if venv_info:
            environment_info["virtual_env"] = venv_info
            self.path_memory["virtual_envs"][str(project_path)] = venv_info
        
        # åˆ†æé¡¹ç›®ç»“æ„
        structure_info = self._analyze_project_structure(project_path)
        environment_info["structure"] = structure_info
        self.path_memory["project_structures"][str(project_path)] = structure_info
        
        # æ£€æµ‹é¡¹ç›®ç±»å‹
        project_type = self._detect_project_type(project_path, structure_info)
        environment_info["project_type"] = project_type
        
        # ä¿å­˜ç¯å¢ƒè®°å¿†
        self.save_enhanced_memory(
            content=environment_info,
            metadata={
                "type": "environment",
                "tags": ["project", "setup", project_type],
                "quality_score": 0.9
            }
        )
        
        # æŒä¹…åŒ–è·¯å¾„è®°å¿†
        self._save_memory_file(self.storage_files["paths"], self.path_memory)
        
        return environment_info
    
    def get_smart_file_location(self, filename: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ™ºèƒ½æ¨èæ–‡ä»¶åˆ›å»ºä½ç½®"""
        suggestions = {
            "primary": None,
            "alternatives": [],
            "reasoning": []
        }
        
        file_ext = Path(filename).suffix
        project_root = Path(context.get("project_root", "."))
        
        # åŸºäºé¡¹ç›®ç»“æ„çš„æ¨è
        if str(project_root) in self.path_memory["project_structures"]:
            structure = self.path_memory["project_structures"][str(project_root)]
            
            # æŸ¥æ‰¾ç›¸ä¼¼æ–‡ä»¶çš„ä½ç½®æ¨¡å¼
            similar_files = self._find_similar_files(filename, structure)
            if similar_files:
                common_dir = self._find_common_directory(similar_files)
                suggestions["primary"] = str(project_root / common_dir / filename)
                suggestions["reasoning"].append(f"ç±»ä¼¼æ–‡ä»¶é€šå¸¸åœ¨ {common_dir}")
        
        # åŸºäºæ–‡ä»¶ç±»å‹çš„æ™ºèƒ½æ¨è
        type_suggestions = self._get_type_based_suggestions(file_ext, project_root)
        suggestions["alternatives"].extend(type_suggestions)
        
        # åŸºäºä½¿ç”¨é¢‘ç‡çš„æ¨è
        frequent_paths = self._get_frequent_paths_for_type(file_ext)
        if frequent_paths:
            suggestions["alternatives"].extend(frequent_paths[:3])
            suggestions["reasoning"].append("åŸºäºå†å²ä½¿ç”¨é¢‘ç‡")
        
        # å¦‚æœæ²¡æœ‰ä¸»æ¨èï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¤‡é€‰
        if not suggestions["primary"] and suggestions["alternatives"]:
            suggestions["primary"] = suggestions["alternatives"][0]
            suggestions["alternatives"] = suggestions["alternatives"][1:]
        
        # é»˜è®¤æ¨è
        if not suggestions["primary"]:
            suggestions["primary"] = str(project_root / filename)
            suggestions["reasoning"].append("é»˜è®¤é¡¹ç›®æ ¹ç›®å½•")
        
        # å­¦ä¹ ç”¨æˆ·é€‰æ‹©
        self._learn_path_preference(filename, suggestions["primary"], context)
        
        return suggestions
    
    # ========== TODOç®¡ç†åŠŸèƒ½ ==========
    
    def parse_and_manage_todo(self, todo_path: str) -> Dict[str, Any]:
        """è§£æTODOæ–‡ä»¶å¹¶å»ºç«‹ä»»åŠ¡ç®¡ç†ä½“ç³»"""
        todo_path = Path(todo_path)
        
        if not todo_path.exists():
            return {"error": "TODO file not found", "path": str(todo_path)}
        
        # è§£æTODOå†…å®¹
        parsed_tasks = self._parse_todo_file(todo_path)
        
        # åˆ†æä»»åŠ¡ç»“æ„
        task_structure = self._analyze_task_structure(parsed_tasks)
        
        # æ›´æ–°ä¸»çº¿ä»»åŠ¡
        self.todo_manager["main_thread"] = task_structure["main_tasks"]
        self.todo_manager["task_dependencies"] = task_structure["dependencies"]
        
        # è¯†åˆ«ä»»åŠ¡æ¨¡å¼
        patterns = self._identify_task_patterns(parsed_tasks)
        self.task_patterns.update(patterns)
        
        # åˆ›å»ºä»»åŠ¡è®°å¿†
        self.save_enhanced_memory(
            content={
                "todo_file": str(todo_path),
                "task_count": len(parsed_tasks),
                "structure": task_structure,
                "parsed_at": datetime.now().isoformat()
            },
            metadata={
                "type": "todo_parsing",
                "tags": ["todo", "task_management"],
                "quality_score": 0.95
            }
        )
        
        # æŒä¹…åŒ–TODOçŠ¶æ€
        self._save_memory_file(self.storage_files["todo"], self.todo_manager)
        
        return {
            "status": "success",
            "main_tasks": len(task_structure["main_tasks"]),
            "total_tasks": len(parsed_tasks),
            "current_context": self.todo_manager["current_context"],
            "patterns_identified": len(patterns)
        }
    
    def create_intelligent_branch(self, reason: str, context: Dict[str, Any]) -> str:
        """æ™ºèƒ½åˆ›å»ºä»»åŠ¡åˆ†æ”¯"""
        # åˆ†æåˆ†æ”¯å¿…è¦æ€§
        branch_analysis = self._analyze_branch_necessity(reason, context)
        
        if not branch_analysis["should_branch"]:
            return None
        
        # ç”Ÿæˆåˆ†æ”¯ä¿¡æ¯
        branch_id = f"branch_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        branch_name = self._generate_branch_name(reason, context)
        
        branch_info = {
            "id": branch_id,
            "name": branch_name,
            "reason": reason,
            "created_at": datetime.now().isoformat(),
            "parent_context": self.todo_manager["current_context"],
            "expected_duration": branch_analysis["expected_duration"],
            "auto_return": branch_analysis["auto_return"],
            "tasks": [],
            "context_snapshot": self._create_context_snapshot()
        }
        
        # åˆ›å»ºåˆ†æ”¯
        self.todo_manager["branches"][branch_id] = branch_info
        self.todo_manager["current_context"] = branch_id
        
        # è®°å½•åˆ†æ”¯åˆ›å»º
        self.save_enhanced_memory(
            content={
                "action": "branch_created",
                "branch": branch_info,
                "main_task_interrupted": self._get_current_main_task()
            },
            metadata={
                "type": "task_branching",
                "tags": ["todo", "branch", "context_switch"],
                "thinking_modes": ["task_planning", "priority_assessment"]
            }
        )
        
        # æŒä¹…åŒ–
        self._save_memory_file(self.storage_files["todo"], self.todo_manager)
        
        return branch_id
    
    def smart_return_to_main(self) -> Dict[str, Any]:
        """æ™ºèƒ½è¿”å›ä¸»çº¿ï¼Œå¸¦ä¸Šä¸‹æ–‡æ¢å¤"""
        current_context = self.todo_manager["current_context"]
        
        if current_context == "main":
            return {"status": "already_on_main"}
        
        if current_context not in self.todo_manager["branches"]:
            return {"status": "error", "message": "Invalid branch context"}
        
        branch = self.todo_manager["branches"][current_context]
        
        # æ€»ç»“åˆ†æ”¯å·¥ä½œ
        branch_summary = self._summarize_branch_work(branch)
        
        # ä¿å­˜åˆ†æ”¯å®Œæˆè®°å¿†
        self.save_enhanced_memory(
            content={
                "branch_completed": branch,
                "summary": branch_summary,
                "duration": self._calculate_duration(branch["created_at"])
            },
            metadata={
                "type": "branch_completion",
                "tags": ["todo", "branch", "completion"],
                "quality_score": branch_summary.get("completion_quality", 0.7)
            }
        )
        
        # æ ‡è®°åˆ†æ”¯å®Œæˆ
        branch["completed_at"] = datetime.now().isoformat()
        branch["summary"] = branch_summary
        
        # æ¢å¤ä¸»çº¿ä¸Šä¸‹æ–‡
        self.todo_manager["current_context"] = "main"
        
        # è·å–ä¸»çº¿ä»»åŠ¡çŠ¶æ€
        main_task_status = self._get_main_task_status()
        
        # æŒä¹…åŒ–
        self._save_memory_file(self.storage_files["todo"], self.todo_manager)
        
        return {
            "status": "returned_to_main",
            "branch_summary": branch_summary,
            "main_task_status": main_task_status,
            "next_action": self._suggest_next_action()
        }
    
    # ========== æ€ç»´æ¨¡å¼å­¦ä¹  ==========
    
    def _update_pattern_learning(self, modes: List[str], quality: float, problem_type: str):
        """æ›´æ–°æ€ç»´æ¨¡å¼å­¦ä¹ æ•°æ®"""
        # æ›´æ–°å•ä¸ªæ¨¡å¼æ•ˆæœ
        for mode in modes:
            stats = self.pattern_effectiveness[mode]
            stats["total_count"] += 1
            if quality > 0.7:  # æˆåŠŸé˜ˆå€¼
                stats["success_count"] += 1
            
            # æ›´æ–°å¹³å‡è´¨é‡ï¼ˆç§»åŠ¨å¹³å‡ï¼‰
            alpha = 0.1  # å­¦ä¹ ç‡
            stats["avg_quality"] = (1 - alpha) * stats["avg_quality"] + alpha * quality
            
            # æ›´æ–°é—®é¢˜ç±»å‹åˆ†å¸ƒ
            stats["problem_types"][problem_type] += 1
            stats["last_used"] = datetime.now().isoformat()
        
        # æ›´æ–°æ¨¡å¼ç»„åˆæ•ˆæœ
        if len(modes) > 1 and quality > 0.8:  # ç»„åˆæˆåŠŸé˜ˆå€¼æ›´é«˜
            combination = tuple(sorted(modes))
            self.successful_combinations.append({
                "modes": combination,
                "quality": quality,
                "problem_type": problem_type,
                "timestamp": datetime.now().isoformat()
            })
            
            # åªä¿ç•™æœ€è¿‘çš„1000ä¸ªæˆåŠŸç»„åˆ
            if len(self.successful_combinations) > 1000:
                self.successful_combinations = self.successful_combinations[-1000:]
        
        # æ›´æ–°é—®é¢˜-æ¨¡å¼æ˜ å°„
        self.problem_pattern_mapping[problem_type][tuple(modes)] += quality
        
        # æŒä¹…åŒ–å­¦ä¹ æ•°æ®
        self._persist_thinking_patterns()
    
    def get_recommended_thinking_modes(self, problem: str, context: Dict[str, Any]) -> List[str]:
        """åŸºäºå­¦ä¹ æ¨èæœ€ä½³æ€ç»´æ¨¡å¼ç»„åˆ"""
        recommendations = []
        
        # 1. åŸºäºé—®é¢˜ç±»å‹çš„å†å²æˆåŠŸæ¨¡å¼
        problem_type = self._classify_problem_type(problem, context)
        if problem_type in self.problem_pattern_mapping:
            type_patterns = self.problem_pattern_mapping[problem_type]
            # è·å–è¯¥é—®é¢˜ç±»å‹ä¸‹æœ€æˆåŠŸçš„æ¨¡å¼ç»„åˆ
            best_patterns = sorted(type_patterns.items(), key=lambda x: x[1], reverse=True)[:3]
            for pattern, score in best_patterns:
                recommendations.extend(list(pattern))
        
        # 2. åŸºäºå•ä¸ªæ¨¡å¼çš„æ•´ä½“è¡¨ç°
        high_performing_modes = []
        for mode, stats in self.pattern_effectiveness.items():
            if stats["total_count"] > 5 and stats["avg_quality"] > 0.75:
                success_rate = stats["success_count"] / stats["total_count"]
                if success_rate > 0.7:
                    high_performing_modes.append((mode, stats["avg_quality"]))
        
        # æŒ‰è´¨é‡æ’åºå¹¶æ·»åŠ åˆ°æ¨è
        high_performing_modes.sort(key=lambda x: x[1], reverse=True)
        recommendations.extend([mode for mode, _ in high_performing_modes[:3]])
        
        # 3. åŸºäºæœ€è¿‘çš„æˆåŠŸç»„åˆ
        recent_successes = [
            combo for combo in self.successful_combinations[-50:]
            if combo["quality"] > 0.85
        ]
        for combo in recent_successes[-10:]:
            recommendations.extend(combo["modes"])
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_recommendations = []
        seen = set()
        for mode in recommendations:
            if mode not in seen:
                seen.add(mode)
                unique_recommendations.append(mode)
                if len(unique_recommendations) >= 5:
                    break
        
        return unique_recommendations
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    def _determine_memory_type(self, memory_item: Dict) -> str:
        """æ™ºèƒ½å†³å®šè®°å¿†åº”è¯¥å­˜å‚¨åœ¨å“ªä¸€å±‚"""
        content = memory_item["content"]
        metadata = memory_item["metadata"]
        
        # è§„åˆ™åŸºç¡€çš„åˆ†ç±»
        if any(tag in metadata["tags"] for tag in ["preference", "setting", "always", "never"]):
            return "global"
        
        if any(tag in metadata["tags"] for tag in ["project", "architecture", "decision", "convention"]):
            return "project"
        
        # åŸºäºå†…å®¹çš„æ™ºèƒ½åˆ†ç±»
        if isinstance(content, str):
            content_lower = content.lower()
            if any(keyword in content_lower for keyword in ["always", "prefer", "my style", "i like"]):
                return "global"
            if any(keyword in content_lower for keyword in ["this project", "in this codebase", "here we"]):
                return "project"
        
        # åŸºäºè´¨é‡åˆ†æ•°çš„åˆ†ç±»
        if metadata.get("quality_score", 0) > 0.9 and metadata.get("source") == "user_feedback":
            return "global"  # é«˜è´¨é‡çš„ç”¨æˆ·åé¦ˆåº”è¯¥å…¨å±€è®°ä½
        
        # é»˜è®¤ä¸ºä¼šè¯çº§åˆ«
        return "session"
    
    def _calculate_memory_relevance(self, memory: Dict, query_features: Dict, context: Dict) -> float:
        """è®¡ç®—è®°å¿†ä¸å½“å‰æŸ¥è¯¢çš„ç›¸å…³åº¦"""
        relevance_score = 0.0
        
        # 1. å…³é”®è¯åŒ¹é…åº¦
        memory_text = str(memory["content"]).lower()
        keyword_matches = sum(1 for kw in query_features["keywords"] if kw in memory_text)
        relevance_score += keyword_matches * 0.2
        
        # 2. æ€ç»´æ¨¡å¼åŒ¹é…åº¦
        if "thinking_modes" in context and "thinking_modes" in memory["metadata"]:
            mode_overlap = set(context["thinking_modes"]) & set(memory["metadata"]["thinking_modes"])
            relevance_score += len(mode_overlap) * 0.3
        
        # 3. æ—¶é—´ç›¸å…³æ€§ï¼ˆæœ€è¿‘çš„è®°å¿†æ›´ç›¸å…³ï¼‰
        memory_time = datetime.fromisoformat(memory["metadata"]["timestamp"])
        time_diff = datetime.now() - memory_time
        time_factor = 1.0 / (1.0 + time_diff.days * 0.1)  # éšæ—¶é—´è¡°å‡
        relevance_score += time_factor * 0.2
        
        # 4. ä½¿ç”¨é¢‘ç‡å’Œæœ‰ç”¨æ€§
        usage_score = memory["usage"]["usefulness_score"] * memory["usage"]["access_count"] * 0.01
        relevance_score += min(usage_score, 0.3)  # ä¸Šé™0.3
        
        # 5. ä¸Šä¸‹æ–‡ç›¸ä¼¼åº¦
        if "context" in memory["metadata"] and context:
            context_similarity = self._calculate_context_similarity(
                memory["metadata"]["context"], 
                context
            )
            relevance_score += context_similarity * 0.2
        
        # åº”ç”¨è¡°å‡å› å­
        relevance_score *= memory["usage"]["decay_factor"]
        
        return min(relevance_score, 1.0)  # ç¡®ä¿ä¸è¶…è¿‡1.0
    
    def _generate_memory_id(self) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„è®°å¿†ID"""
        timestamp = datetime.now().isoformat()
        random_str = os.urandom(8).hex()
        return hashlib.sha256(f"{timestamp}_{random_str}".encode()).hexdigest()[:16]
    
    def _persist_memory(self, memory_type: str):
        """æŒä¹…åŒ–ç‰¹å®šç±»å‹çš„è®°å¿†"""
        if memory_type in ["global", "project"]:
            self._save_memory_file(
                self.storage_files[memory_type], 
                self.memories[memory_type]
            )
    
    def _persist_thinking_patterns(self):
        """æŒä¹…åŒ–æ€ç»´æ¨¡å¼å­¦ä¹ æ•°æ®"""
        thinking_data = {
            "pattern_effectiveness": dict(self.pattern_effectiveness),
            "successful_combinations": self.successful_combinations,
            "problem_patterns": dict(self.problem_pattern_mapping),
            "last_updated": datetime.now().isoformat()
        }
        self._save_memory_file(self.storage_files["thinking"], thinking_data)
```

#### 1.2 æ€ç»´å·¥å…·ç®±å®ç°

```python
# tools/workflow/thinking_enhanced.py
"""å¢å¼ºç‰ˆæ€ç»´å·¥å…·ç®± - 25ç§ä¸“å®¶çº§æ€ç»´æ¨¡å¼"""

from typing import Dict, List, Optional, Tuple, Any
from tools.workflow.base_workflow import BaseWorkflowTool
from utils.enhanced_memory import EnhancedMemory
from tools.base_tool import ToolRequest, ToolOutput
import json
import re
from datetime import datetime
from collections import defaultdict, Counter

class EnhancedThinkingTool(BaseWorkflowTool):
    """é›†æˆ25ç§ä¸“å®¶æ€ç»´æ¨¡å¼çš„æ™ºèƒ½æ€è€ƒå·¥å…·"""
    
    def __init__(self):
        super().__init__()
        self.memory_system = EnhancedMemory()
        
        # åˆå§‹åŒ–æ€ç»´æ¨¡å¼åº“
        self._init_thinking_modes()
        
        # åˆå§‹åŒ–è§¦å‘ç³»ç»Ÿ
        self._init_trigger_system()
        
        # åˆå§‹åŒ–åˆ†æå¼•æ“
        self._init_analysis_engine()
    
    def _init_thinking_modes(self):
        """åˆå§‹åŒ–å®Œæ•´çš„25ç§æ€ç»´æ¨¡å¼"""
        self.thinking_modes = {
            # ===== æ·±åº¦ç†è§£ç±» =====
            "socratic_questioning": {
                "name": "è‹æ ¼æ‹‰åº•å¼æé—®",
                "category": "deep_understanding",
                "description": "é€šè¿‡è¿ç»­æé—®æ­ç¤ºæ·±å±‚éœ€æ±‚å’Œéšå«å‡è®¾",
                "process": [
                    {"step": "è¡¨å±‚ç†è§£", "prompt": "ç”¨æˆ·è¯´äº†ä»€ä¹ˆï¼Ÿå­—é¢æ„æ€æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "ç›®çš„æ¢ç©¶", "prompt": "è¿™è§£å†³äº†ä»€ä¹ˆæ ¹æœ¬é—®é¢˜ï¼ŸçœŸæ­£çš„éœ€æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "æ—¶æœºåˆ†æ", "prompt": "ä¸ºä»€ä¹ˆæ˜¯ç°åœ¨ï¼Ÿå¦‚æœä¸åšä¼šæœ‰ä»€ä¹ˆåæœï¼Ÿ"},
                    {"step": "æ–¹æ¡ˆè´¨ç–‘", "prompt": "æœ‰æ²¡æœ‰æ›´ç®€å•çš„è§£å†³æ–¹æ¡ˆï¼Ÿç°æœ‰æ–¹æ¡ˆçš„å‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "æ·±å±‚æ´å¯Ÿ", "prompt": "éšå«çš„éœ€æ±‚æ˜¯ä»€ä¹ˆï¼Ÿç”¨æˆ·æ²¡è¯´ä½†å¾ˆé‡è¦çš„æ˜¯ä»€ä¹ˆï¼Ÿ"}
                ],
                "output_format": {
                    "surface_understanding": "è¡¨é¢éœ€æ±‚",
                    "real_needs": "çœŸå®éœ€æ±‚",
                    "hidden_assumptions": "éšå«å‡è®¾",
                    "alternative_solutions": "æ›¿ä»£æ–¹æ¡ˆ",
                    "key_insights": "å…³é”®æ´å¯Ÿ"
                },
                "suitable_for": ["éœ€æ±‚åˆ†æ", "äº§å“è®¾è®¡", "é—®é¢˜å®šä¹‰", "æ–¹æ¡ˆè¯„ä¼°"]
            },
            
            "five_whys": {
                "name": "5 Whysæ ¹å› åˆ†æ",
                "category": "deep_understanding",
                "description": "é€šè¿‡é€’è¿›å¼è¿½é—®æ‰¾åˆ°é—®é¢˜æ ¹æº",
                "process": [
                    {"step": "é—®é¢˜è¯†åˆ«", "prompt": "æ˜ç¡®æè¿°è¡¨é¢é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "ç¬¬ä¸€å±‚åŸå› ", "prompt": "ä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™ä¸ªé—®é¢˜ï¼Ÿç›´æ¥åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "ç¬¬äºŒå±‚åŸå› ", "prompt": "ä¸ºä»€ä¹ˆä¼šæœ‰ä¸Šè¿°åŸå› ï¼Ÿæ›´æ·±å±‚çš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "ç¬¬ä¸‰å±‚åŸå› ", "prompt": "ç»§ç»­è¿½é—®ï¼Œè¿™ä¸ªåŸå› èƒŒåçš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "æ ¹æœ¬åŸå› ", "prompt": "æŒç»­æ·±å…¥ç›´åˆ°æ‰¾åˆ°æ ¹æœ¬åŸå› "},
                    {"step": "å› æœéªŒè¯", "prompt": "éªŒè¯å› æœé“¾ï¼šå¦‚æœè§£å†³æ ¹å› ï¼Œé—®é¢˜ä¼šæ¶ˆå¤±å—ï¼Ÿ"}
                ],
                "output_format": {
                    "problem_statement": "é—®é¢˜æè¿°",
                    "causal_chain": ["åŸå› 1", "åŸå› 2", "...", "æ ¹æœ¬åŸå› "],
                    "root_cause": "æ ¹æœ¬åŸå› ",
                    "verification": "å› æœé“¾éªŒè¯",
                    "solution_direction": "è§£å†³æ–¹å‘"
                },
                "suitable_for": ["æ•…éšœæ’æŸ¥", "è´¨é‡æ”¹è¿›", "æµç¨‹ä¼˜åŒ–", "äº‹æ•…åˆ†æ"]
            },
            
            "first_principles": {
                "name": "ç¬¬ä¸€æ€§åŸç†æ€ç»´",
                "category": "deep_understanding",
                "description": "å›å½’äº‹ç‰©æœ€åŸºæœ¬çš„çœŸç†ï¼Œé‡æ–°æ€è€ƒå’Œæ„å»º",
                "process": [
                    {"step": "ç°çŠ¶åˆ†è§£", "prompt": "å½“å‰çš„è§£å†³æ–¹æ¡ˆåŒ…å«å“ªäº›ç»„æˆéƒ¨åˆ†ï¼Ÿ"},
                    {"step": "å‡è®¾è¯†åˆ«", "prompt": "å“ªäº›æ˜¯çº¦å®šä¿—æˆï¼Ÿå“ªäº›æ˜¯çœŸæ­£çš„é™åˆ¶ï¼Ÿ"},
                    {"step": "åŸºæœ¬åŸç†", "prompt": "è¿™ä¸ªé—®é¢˜çš„ç‰©ç†/é€»è¾‘/å•†ä¸šåŸºæœ¬åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "é‡æ–°æ„å»º", "prompt": "ä»åŸºæœ¬åŸç†å‡ºå‘ï¼Œå¦‚ä½•é‡æ–°è®¾è®¡è§£å†³æ–¹æ¡ˆï¼Ÿ"},
                    {"step": "åˆ›æ–°éªŒè¯", "prompt": "æ–°æ–¹æ¡ˆæ˜¯å¦çªç ´äº†åŸæœ‰é™åˆ¶ï¼Ÿæ˜¯å¦æ›´æ¥è¿‘æœ¬è´¨ï¼Ÿ"}
                ],
                "output_format": {
                    "current_solution": "ç°æœ‰æ–¹æ¡ˆåˆ†è§£",
                    "assumptions": "è¯†åˆ«çš„å‡è®¾",
                    "fundamental_truths": "åŸºæœ¬åŸç†",
                    "new_approach": "åˆ›æ–°æ–¹æ¡ˆ",
                    "breakthrough_points": "çªç ´ç‚¹"
                },
                "suitable_for": ["åˆ›æ–°è®¾è®¡", "æˆæœ¬ä¼˜åŒ–", "çªç ´æ€§æ€è€ƒ", "é¢ è¦†å¼åˆ›æ–°"]
            },
            
            # ===== ç»“æ„åŒ–åˆ†æç±» =====
            "mece_decomposition": {
                "name": "MECEåŸåˆ™åˆ†è§£",
                "category": "structured_analysis",
                "description": "ç›¸äº’ç‹¬ç«‹ã€å®Œå…¨ç©·å°½çš„ç»“æ„åŒ–åˆ†è§£",
                "process": [
                    {"step": "ç›®æ ‡å®šä¹‰", "prompt": "éœ€è¦åˆ†è§£ä»€ä¹ˆï¼Ÿåˆ†è§£çš„ç›®çš„æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "ç»´åº¦è¯†åˆ«", "prompt": "å¯ä»¥ä»å“ªäº›ç»´åº¦è¿›è¡Œåˆ†è§£ï¼Ÿæ—¶é—´/ç©ºé—´/åŠŸèƒ½/æµç¨‹ï¼Ÿ"},
                    {"step": "ç‹¬ç«‹æ€§æ£€æŸ¥", "prompt": "å„å­é¡¹ä¹‹é—´æ˜¯å¦æœ‰é‡å ï¼Ÿå¦‚ä½•ç¡®ä¿ç›¸äº’ç‹¬ç«‹ï¼Ÿ"},
                    {"step": "å®Œæ•´æ€§éªŒè¯", "prompt": "æ˜¯å¦è¦†ç›–äº†æ‰€æœ‰æƒ…å†µï¼Ÿæœ‰æ— é—æ¼ï¼Ÿ"},
                    {"step": "å±‚çº§ä¼˜åŒ–", "prompt": "åˆ†è§£çš„å±‚çº§æ˜¯å¦åˆç†ï¼Ÿéœ€è¦è¿›ä¸€æ­¥ç»†åˆ†å—ï¼Ÿ"}
                ],
                "output_format": {
                    "decomposition_target": "åˆ†è§£ç›®æ ‡",
                    "dimensions": "åˆ†è§£ç»´åº¦",
                    "structure": {
                        "level1": ["å­é¡¹1", "å­é¡¹2"],
                        "level2": {"å­é¡¹1": ["ç»†åˆ†1", "ç»†åˆ†2"]}
                    },
                    "completeness_check": "å®Œæ•´æ€§éªŒè¯",
                    "independence_check": "ç‹¬ç«‹æ€§éªŒè¯"
                },
                "suitable_for": ["é¡¹ç›®è§„åˆ’", "é—®é¢˜åˆ†æ", "ç»„ç»‡è®¾è®¡", "å·¥ä½œåˆ†è§£"]
            },
            
            "pyramid_principle": {
                "name": "é‡‘å­—å¡”åŸç†",
                "category": "structured_analysis",
                "description": "ç»“è®ºå…ˆè¡Œï¼Œå±‚å±‚æ”¯æ’‘ï¼Œé€»è¾‘é€’è¿›",
                "process": [
                    {"step": "ä¸­å¿ƒæ€æƒ³", "prompt": "æ ¸å¿ƒè§‚ç‚¹/ç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿä¸€å¥è¯è¯´æ¸…æ¥š"},
                    {"step": "æ”¯æ’‘è®ºç‚¹", "prompt": "æ”¯æ’‘æ ¸å¿ƒè§‚ç‚¹çš„3-5ä¸ªå…³é”®è®ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "è®ºæ®ç»„ç»‡", "prompt": "æ¯ä¸ªè®ºç‚¹çš„å…·ä½“è®ºæ®å’Œä¾‹è¯æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "é€»è¾‘æ£€éªŒ", "prompt": "çºµå‘æ˜¯å¦é—®ç­”é€»è¾‘ï¼Ÿæ¨ªå‘æ˜¯å¦å½’ç±»æ¸…æ™°ï¼Ÿ"},
                    {"step": "SCQAå¼•å…¥", "prompt": "å¦‚ä½•ç”¨æƒ…å¢ƒ-å†²çª-é—®é¢˜-ç­”æ¡ˆå¼•å…¥ï¼Ÿ"}
                ],
                "output_format": {
                    "central_message": "ä¸­å¿ƒæ€æƒ³",
                    "key_points": ["è®ºç‚¹1", "è®ºç‚¹2", "è®ºç‚¹3"],
                    "supporting_evidence": {
                        "è®ºç‚¹1": ["è®ºæ®1", "è®ºæ®2"],
                        "è®ºç‚¹2": ["è®ºæ®1", "è®ºæ®2"]
                    },
                    "scqa_intro": {
                        "situation": "æƒ…å¢ƒ",
                        "complication": "å†²çª",
                        "question": "é—®é¢˜",
                        "answer": "ç­”æ¡ˆ"
                    }
                },
                "suitable_for": ["æŠ¥å‘Šæ’°å†™", "æ–¹æ¡ˆæ±‡æŠ¥", "è§‚ç‚¹è¡¨è¾¾", "è¯´æœæ²Ÿé€š"]
            },
            
            "mind_mapping": {
                "name": "æ€ç»´å¯¼å›¾æ³•",
                "category": "structured_analysis",
                "description": "æ”¾å°„æ€§æ€è€ƒï¼Œå±•ç°çŸ¥è¯†å…³è”",
                "process": [
                    {"step": "ä¸­å¿ƒä¸»é¢˜", "prompt": "æ ¸å¿ƒæ¦‚å¿µæˆ–é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "ä¸»åˆ†æ”¯", "prompt": "å›´ç»•ä¸­å¿ƒçš„5-7ä¸ªä¸»è¦æ–¹é¢æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "ç»†åˆ†å»¶ä¼¸", "prompt": "æ¯ä¸ªä¸»åˆ†æ”¯å¯ä»¥å¦‚ä½•è¿›ä¸€æ­¥ç»†åˆ†ï¼Ÿ"},
                    {"step": "å…³è”è¯†åˆ«", "prompt": "ä¸åŒåˆ†æ”¯ä¹‹é—´æœ‰ä»€ä¹ˆè”ç³»ï¼Ÿ"},
                    {"step": "é‡ç‚¹æ ‡æ³¨", "prompt": "å“ªäº›æ˜¯å…³é”®èŠ‚ç‚¹ï¼Ÿä¼˜å…ˆçº§å¦‚ä½•ï¼Ÿ"}
                ],
                "output_format": {
                    "central_topic": "ä¸­å¿ƒä¸»é¢˜",
                    "main_branches": ["åˆ†æ”¯1", "åˆ†æ”¯2", "åˆ†æ”¯3"],
                    "sub_branches": {
                        "åˆ†æ”¯1": ["å­åˆ†æ”¯1", "å­åˆ†æ”¯2"],
                        "åˆ†æ”¯2": ["å­åˆ†æ”¯1", "å­åˆ†æ”¯2"]
                    },
                    "cross_links": [{"from": "èŠ‚ç‚¹A", "to": "èŠ‚ç‚¹B", "relation": "å…³ç³»"}],
                    "priorities": {"high": ["èŠ‚ç‚¹1"], "medium": ["èŠ‚ç‚¹2"], "low": ["èŠ‚ç‚¹3"]}
                },
                "suitable_for": ["çŸ¥è¯†æ•´ç†", "åˆ›æ„å‘æ•£", "å¤æ‚å…³ç³»æ¢³ç†", "å­¦ä¹ ç¬”è®°"]
            },
            
            # ===== ç³»ç»Ÿæ€è€ƒç±» =====
            "systems_thinking": {
                "name": "ç³»ç»Ÿæ€ç»´åˆ†æ",
                "category": "systems_thinking",
                "description": "å…³æ³¨æ•´ä½“å’Œéƒ¨åˆ†çš„ç›¸äº’å…³ç³»",
                "process": [
                    {"step": "ç³»ç»Ÿè¾¹ç•Œ", "prompt": "ç³»ç»Ÿçš„è¾¹ç•Œåœ¨å“ªé‡Œï¼ŸåŒ…å«å“ªäº›è¦ç´ ï¼Ÿ"},
                    {"step": "è¦ç´ å…³ç³»", "prompt": "è¦ç´ ä¹‹é—´å¦‚ä½•ç›¸äº’ä½œç”¨ï¼Ÿè¾“å…¥è¾“å‡ºæ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "åé¦ˆå›è·¯", "prompt": "å­˜åœ¨å“ªäº›æ­£åé¦ˆå’Œè´Ÿåé¦ˆå›è·¯ï¼Ÿ"},
                    {"step": "ç“¶é¢ˆè¯†åˆ«", "prompt": "ç³»ç»Ÿçš„ç“¶é¢ˆå’Œå…³é”®èŠ‚ç‚¹åœ¨å“ªé‡Œï¼Ÿ"},
                    {"step": "æ¶Œç°ç‰¹æ€§", "prompt": "æ•´ä½“è¡¨ç°å‡ºå“ªäº›å•ä¸ªéƒ¨åˆ†æ²¡æœ‰çš„ç‰¹æ€§ï¼Ÿ"},
                    {"step": "æ¼”åŒ–é¢„æµ‹", "prompt": "ç³»ç»Ÿå¯èƒ½å¦‚ä½•æ¼”åŒ–ï¼Ÿæœ‰å“ªäº›å¯èƒ½çš„çŠ¶æ€ï¼Ÿ"}
                ],
                "output_format": {
                    "system_boundary": "ç³»ç»Ÿè¾¹ç•Œ",
                    "elements": ["è¦ç´ 1", "è¦ç´ 2", "è¦ç´ 3"],
                    "relationships": [{"from": "è¦ç´ 1", "to": "è¦ç´ 2", "type": "å…³ç³»ç±»å‹"}],
                    "feedback_loops": {
                        "positive": ["å¢å¼ºå›è·¯1"],
                        "negative": ["å¹³è¡¡å›è·¯1"]
                    },
                    "bottlenecks": ["ç“¶é¢ˆ1", "ç“¶é¢ˆ2"],
                    "emergent_properties": ["æ¶Œç°ç‰¹æ€§1", "æ¶Œç°ç‰¹æ€§2"],
                    "evolution_scenarios": ["åœºæ™¯1", "åœºæ™¯2"]
                },
                "suitable_for": ["æ¶æ„è®¾è®¡", "å½±å“åˆ†æ", "å¤æ‚é—®é¢˜", "ç”Ÿæ€è®¾è®¡"]
            },
            
            "value_chain": {
                "name": "ä»·å€¼é“¾åˆ†æ",
                "category": "systems_thinking",
                "description": "è¯†åˆ«ä»·å€¼åˆ›é€ å’Œä¼ é€’çš„å…¨è¿‡ç¨‹",
                "process": [
                    {"step": "ä»·å€¼å®šä¹‰", "prompt": "ä»€ä¹ˆæ˜¯ä»·å€¼ï¼Ÿå¯¹è°æœ‰ä»·å€¼ï¼Ÿ"},
                    {"step": "æ´»åŠ¨è¯†åˆ«", "prompt": "å“ªäº›æ˜¯ä¸»è¦æ´»åŠ¨ï¼Ÿå“ªäº›æ˜¯æ”¯æŒæ´»åŠ¨ï¼Ÿ"},
                    {"step": "ä»·å€¼æµåŠ¨", "prompt": "ä»·å€¼å¦‚ä½•ä»ä¸€ä¸ªç¯èŠ‚æµå‘ä¸‹ä¸€ä¸ªï¼Ÿ"},
                    {"step": "æˆæœ¬åˆ†æ", "prompt": "æ¯ä¸ªç¯èŠ‚çš„æˆæœ¬æ˜¯å¤šå°‘ï¼Ÿä»·å€¼å¢å€¼å¤šå°‘ï¼Ÿ"},
                    {"step": "ä¼˜åŒ–æœºä¼š", "prompt": "å“ªé‡Œæœ‰ä¼˜åŒ–ç©ºé—´ï¼Ÿå¦‚ä½•æå‡ä»·å€¼ï¼Ÿ"}
                ],
                "output_format": {
                    "value_definition": "ä»·å€¼å®šä¹‰",
                    "primary_activities": ["æ´»åŠ¨1", "æ´»åŠ¨2", "æ´»åŠ¨3"],
                    "support_activities": ["æ”¯æŒ1", "æ”¯æŒ2"],
                    "value_flow": [
                        {"activity": "æ´»åŠ¨1", "input_value": 100, "output_value": 150, "cost": 30}
                    ],
                    "optimization_opportunities": [
                        {"activity": "æ´»åŠ¨2", "opportunity": "è‡ªåŠ¨åŒ–", "impact": "æˆæœ¬é™ä½30%"}
                    ]
                },
                "suitable_for": ["ä¸šåŠ¡ä¼˜åŒ–", "æˆæœ¬æ§åˆ¶", "æµç¨‹æ”¹è¿›", "ç«äº‰åˆ†æ"]
            },
            
            "ecosystem_thinking": {
                "name": "ç”Ÿæ€ç³»ç»Ÿæ€ç»´",
                "category": "systems_thinking",
                "description": "ä»ç”Ÿæ€è§†è§’çœ‹å¾…å‚ä¸è€…å…³ç³»",
                "process": [
                    {"step": "å‚ä¸è€…è¯†åˆ«", "prompt": "ç”Ÿæ€ä¸­æœ‰å“ªäº›å‚ä¸è€…ï¼Ÿå„è‡ªçš„è§’è‰²ï¼Ÿ"},
                    {"step": "ä»·å€¼äº¤æ¢", "prompt": "å‚ä¸è€…ä¹‹é—´å¦‚ä½•äº¤æ¢ä»·å€¼ï¼Ÿ"},
                    {"step": "ç½‘ç»œæ•ˆåº”", "prompt": "å­˜åœ¨å“ªäº›ç½‘ç»œæ•ˆåº”å’Œæ­£åé¦ˆï¼Ÿ"},
                    {"step": "å…³é”®ç‰©ç§", "prompt": "è°æ˜¯å…³é”®ç‰©ç§ï¼Ÿè°æ˜¯åŸºçŸ³ç‰©ç§ï¼Ÿ"},
                    {"step": "æ¼”è¿›è·¯å¾„", "prompt": "ç”Ÿæ€ç³»ç»Ÿå¯èƒ½å¦‚ä½•æ¼”è¿›ï¼Ÿ"}
                ],
                "output_format": {
                    "participants": {
                        "producers": ["ç”Ÿäº§è€…1"],
                        "consumers": ["æ¶ˆè´¹è€…1"],
                        "enablers": ["ä½¿èƒ½è€…1"]
                    },
                    "value_exchanges": [
                        {"from": "A", "to": "B", "value": "ä»·å€¼ç±»å‹"}
                    ],
                    "network_effects": ["æ•ˆåº”1", "æ•ˆåº”2"],
                    "keystone_species": ["å…³é”®ç‰©ç§1"],
                    "evolution_path": ["é˜¶æ®µ1", "é˜¶æ®µ2", "é˜¶æ®µ3"]
                },
                "suitable_for": ["å¹³å°è®¾è®¡", "å•†ä¸šç”Ÿæ€", "åˆä½œç­–ç•¥", "ç”Ÿæ€æ„å»º"]
            },
            
            # ===== åˆ›æ–°æ€ç»´ç±» =====
            "design_thinking": {
                "name": "è®¾è®¡æ€ç»´",
                "category": "innovation",
                "description": "ä»¥äººä¸ºæœ¬çš„åˆ›æ–°æ–¹æ³•è®º",
                "process": [
                    {"step": "å…±æƒ…ç†è§£", "prompt": "ç”¨æˆ·æ˜¯è°ï¼Ÿä»–ä»¬çš„éœ€æ±‚ã€ç—›ç‚¹ã€æƒ…æ„Ÿæ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "é—®é¢˜å®šä¹‰", "prompt": "çœŸæ­£è¦è§£å†³çš„é—®é¢˜æ˜¯ä»€ä¹ˆï¼ŸPOVé™ˆè¿°ï¼Ÿ"},
                    {"step": "åˆ›æ„æ„æ€", "prompt": "æœ‰å“ªäº›å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Ÿç–¯ç‹‚çš„æƒ³æ³•ï¼Ÿ"},
                    {"step": "åŸå‹åˆ¶ä½œ", "prompt": "å¦‚ä½•å¿«é€ŸéªŒè¯ï¼Ÿæœ€å°å¯è¡ŒåŸå‹æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "æµ‹è¯•è¿­ä»£", "prompt": "ç”¨æˆ·åé¦ˆå¦‚ä½•ï¼Ÿå¦‚ä½•æ”¹è¿›ï¼Ÿ"}
                ],
                "output_format": {
                    "user_insights": {
                        "needs": ["éœ€æ±‚1", "éœ€æ±‚2"],
                        "pain_points": ["ç—›ç‚¹1", "ç—›ç‚¹2"],
                        "emotions": ["æƒ…æ„Ÿ1", "æƒ…æ„Ÿ2"]
                    },
                    "problem_statement": "æˆ‘ä»¬å¦‚ä½•ä¸º[ç”¨æˆ·]è§£å†³[é—®é¢˜]ä»¥å®ç°[ç›®æ ‡]",
                    "ideas": {
                        "practical": ["å®ç”¨æƒ³æ³•1"],
                        "innovative": ["åˆ›æ–°æƒ³æ³•1"],
                        "wild": ["ç–¯ç‹‚æƒ³æ³•1"]
                    },
                    "prototype_plan": "åŸå‹æ–¹æ¡ˆ",
                    "test_results": "æµ‹è¯•ç»“æœå’Œè¿­ä»£æ–¹å‘"
                },
                "suitable_for": ["äº§å“åˆ›æ–°", "æœåŠ¡è®¾è®¡", "ç”¨æˆ·ä½“éªŒ", "ç¤¾ä¼šåˆ›æ–°"]
            },
            
            "reverse_thinking": {
                "name": "é€†å‘æ€ç»´",
                "category": "innovation",
                "description": "ä»ç›¸åæ–¹å‘æ€è€ƒé—®é¢˜",
                "process": [
                    {"step": "å¸¸è§„æ€è·¯", "prompt": "é€šå¸¸çš„æ€è·¯å’Œåšæ³•æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "å®Œå…¨åè½¬", "prompt": "å¦‚æœå®Œå…¨ç›¸åä¼šæ€æ ·ï¼Ÿç›®æ ‡åè½¬ï¼Ÿ"},
                    {"step": "å‡è®¾æŒ‘æˆ˜", "prompt": "å¦‚æœå…³é”®å‡è®¾ä¸å­˜åœ¨ä¼šæ€æ ·ï¼Ÿ"},
                    {"step": "å¤±è´¥åˆ†æ", "prompt": "å¦‚æœè¦æ•…æ„å¤±è´¥ï¼Œä¼šæ€ä¹ˆåšï¼Ÿ"},
                    {"step": "æ´å¯Ÿæå–", "prompt": "åå‘æ€è€ƒå¸¦æ¥ä»€ä¹ˆæ–°æ´å¯Ÿï¼Ÿ"}
                ],
                "output_format": {
                    "conventional_approach": "å¸¸è§„æ–¹æ³•",
                    "reversed_approach": "åå‘æ–¹æ³•",
                    "challenged_assumptions": ["å‡è®¾1è¢«æŒ‘æˆ˜", "å‡è®¾2è¢«æŒ‘æˆ˜"],
                    "failure_analysis": "æ•…æ„å¤±è´¥çš„æ–¹æ³•",
                    "new_insights": ["æ´å¯Ÿ1", "æ´å¯Ÿ2"],
                    "innovative_solutions": ["åˆ›æ–°æ–¹æ¡ˆ1", "åˆ›æ–°æ–¹æ¡ˆ2"]
                },
                "suitable_for": ["åˆ›æ–°çªç ´", "é—®é¢˜é¢„é˜²", "é£é™©è¯†åˆ«", "ç­–ç•¥åˆ¶å®š"]
            },
            
            "analogical_thinking": {
                "name": "ç±»æ¯”æ€ç»´",
                "category": "innovation",
                "description": "é€šè¿‡ç›¸ä¼¼æ€§è¿ç§»è§£å†³æ–¹æ¡ˆ",
                "process": [
                    {"step": "é—®é¢˜æŠ½è±¡", "prompt": "é—®é¢˜çš„æœ¬è´¨ç‰¹å¾æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "é¢†åŸŸæœç´¢", "prompt": "å“ªäº›é¢†åŸŸæœ‰ç±»ä¼¼çš„é—®é¢˜ï¼Ÿ"},
                    {"step": "æ¡ˆä¾‹åˆ†æ", "prompt": "æˆåŠŸæ¡ˆä¾‹æ˜¯å¦‚ä½•è§£å†³çš„ï¼Ÿ"},
                    {"step": "æ˜ å°„è½¬æ¢", "prompt": "å¦‚ä½•å°†è§£å†³æ–¹æ¡ˆæ˜ å°„åˆ°å½“å‰é—®é¢˜ï¼Ÿ"},
                    {"step": "é€‚é…è°ƒæ•´", "prompt": "éœ€è¦åšå“ªäº›è°ƒæ•´ä»¥é€‚åº”å½“å‰æƒ…å¢ƒï¼Ÿ"}
                ],
                "output_format": {
                    "problem_abstraction": "é—®é¢˜æœ¬è´¨",
                    "similar_domains": ["é¢†åŸŸ1", "é¢†åŸŸ2"],
                    "successful_cases": [
                        {"domain": "é¢†åŸŸ1", "solution": "è§£å†³æ–¹æ¡ˆ", "key_insight": "å…³é”®æ´å¯Ÿ"}
                    ],
                    "mapping": {
                        "source_element": "åŸå§‹å…ƒç´ ",
                        "target_element": "ç›®æ ‡å…ƒç´ ",
                        "transformation": "è½¬æ¢æ–¹å¼"
                    },
                    "adapted_solution": "é€‚é…åçš„æ–¹æ¡ˆ"
                },
                "suitable_for": ["åˆ›æ–°è®¾è®¡", "é—®é¢˜è§£å†³", "çŸ¥è¯†è¿ç§»", "è·¨ç•Œåˆ›æ–°"]
            },
            
            # ===== å†³ç­–åˆ†æç±» =====
            "swot_analysis": {
                "name": "SWOTåˆ†æ",
                "category": "decision_making",
                "description": "å…¨é¢è¯„ä¼°å†…å¤–éƒ¨å› ç´ ",
                "process": [
                    {"step": "ä¼˜åŠ¿è¯†åˆ«", "prompt": "å†…éƒ¨ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿæ ¸å¿ƒç«äº‰åŠ›ï¼Ÿ"},
                    {"step": "åŠ£åŠ¿åˆ†æ", "prompt": "å†…éƒ¨åŠ£åŠ¿æ˜¯ä»€ä¹ˆï¼ŸçŸ­æ¿åœ¨å“ªï¼Ÿ"},
                    {"step": "æœºä¼šæ‰«æ", "prompt": "å¤–éƒ¨æœºä¼šæœ‰å“ªäº›ï¼Ÿè¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "å¨èƒè¯„ä¼°", "prompt": "å¤–éƒ¨å¨èƒæ˜¯ä»€ä¹ˆï¼Ÿé£é™©åœ¨å“ªï¼Ÿ"},
                    {"step": "ç­–ç•¥åˆ¶å®š", "prompt": "SO/WO/ST/WTç­–ç•¥åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ"}
                ],
                "output_format": {
                    "strengths": ["ä¼˜åŠ¿1", "ä¼˜åŠ¿2"],
                    "weaknesses": ["åŠ£åŠ¿1", "åŠ£åŠ¿2"],
                    "opportunities": ["æœºä¼š1", "æœºä¼š2"],
                    "threats": ["å¨èƒ1", "å¨èƒ2"],
                    "strategies": {
                        "SO": "å‘æŒ¥ä¼˜åŠ¿æŠ“ä½æœºä¼š",
                        "WO": "å…‹æœåŠ£åŠ¿æŠ“ä½æœºä¼š",
                        "ST": "åˆ©ç”¨ä¼˜åŠ¿è§„é¿å¨èƒ",
                        "WT": "å‡å°‘åŠ£åŠ¿è§„é¿å¨èƒ"
                    }
                },
                "suitable_for": ["æˆ˜ç•¥åˆ¶å®š", "ç«äº‰åˆ†æ", "é¡¹ç›®è¯„ä¼°", "ä¸ªäººè§„åˆ’"]
            },
            
            "decision_tree": {
                "name": "å†³ç­–æ ‘åˆ†æ",
                "category": "decision_making",
                "description": "å¯è§†åŒ–å†³ç­–è·¯å¾„å’Œç»“æœ",
                "process": [
                    {"step": "å†³ç­–è¯†åˆ«", "prompt": "éœ€è¦åšä»€ä¹ˆå†³ç­–ï¼Ÿæœ‰å“ªäº›é€‰é¡¹ï¼Ÿ"},
                    {"step": "ä¸ç¡®å®šæ€§", "prompt": "æ¯ä¸ªé€‰é¡¹é¢ä¸´å“ªäº›ä¸ç¡®å®šæ€§ï¼Ÿ"},
                    {"step": "æ¦‚ç‡è¯„ä¼°", "prompt": "å„ç§æƒ…å†µçš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ"},
                    {"step": "ç»“æœé¢„æµ‹", "prompt": "æ¯æ¡è·¯å¾„çš„æœ€ç»ˆç»“æœæ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "æœŸæœ›è®¡ç®—", "prompt": "è®¡ç®—æœŸæœ›å€¼ï¼Œå“ªä¸ªé€‰é¡¹æœ€ä¼˜ï¼Ÿ"}
                ],
                "output_format": {
                    "decision_points": ["å†³ç­–1", "å†³ç­–2"],
                    "options": {
                        "å†³ç­–1": ["é€‰é¡¹A", "é€‰é¡¹B"]
                    },
                    "uncertainties": {
                        "é€‰é¡¹A": [
                            {"event": "äº‹ä»¶1", "probability": 0.6, "outcome": "ç»“æœ1"}
                        ]
                    },
                    "expected_values": {
                        "é€‰é¡¹A": 85,
                        "é€‰é¡¹B": 72
                    },
                    "recommendation": "æ¨èé€‰é¡¹A"
                },
                "suitable_for": ["æŠ•èµ„å†³ç­–", "é¡¹ç›®é€‰æ‹©", "é£é™©è¯„ä¼°", "èµ„æºåˆ†é…"]
            },
            
            "cost_benefit": {
                "name": "æˆæœ¬æ•ˆç›Šåˆ†æ",
                "category": "decision_making",
                "description": "é‡åŒ–è¯„ä¼°æŠ•å…¥äº§å‡ºæ¯”",
                "process": [
                    {"step": "æˆæœ¬è¯†åˆ«", "prompt": "æ‰€æœ‰æˆæœ¬é¡¹æ˜¯ä»€ä¹ˆï¼Ÿç›´æ¥/é—´æ¥/æœºä¼šæˆæœ¬ï¼Ÿ"},
                    {"step": "æ•ˆç›Šè¯„ä¼°", "prompt": "æ‰€æœ‰æ”¶ç›Šæ˜¯ä»€ä¹ˆï¼Ÿæœ‰å½¢/æ— å½¢æ”¶ç›Šï¼Ÿ"},
                    {"step": "æ—¶é—´ä»·å€¼", "prompt": "å¦‚ä½•è€ƒè™‘æ—¶é—´ä»·å€¼ï¼ŸæŠ˜ç°ç‡ï¼Ÿ"},
                    {"step": "é‡åŒ–è®¡ç®—", "prompt": "NPVã€ROIã€å›æ”¶æœŸæ˜¯å¤šå°‘ï¼Ÿ"},
                    {"step": "æ•æ„Ÿåˆ†æ", "prompt": "å…³é”®å˜é‡å˜åŒ–å¯¹ç»“æœçš„å½±å“ï¼Ÿ"}
                ],
                "output_format": {
                    "costs": {
                        "direct": [{"item": "æˆæœ¬é¡¹1", "amount": 1000}],
                        "indirect": [{"item": "æˆæœ¬é¡¹2", "amount": 500}],
                        "opportunity": [{"item": "æœºä¼šæˆæœ¬", "amount": 300}]
                    },
                    "benefits": {
                        "tangible": [{"item": "æ”¶ç›Š1", "amount": 2000}],
                        "intangible": [{"item": "å“ç‰Œä»·å€¼", "estimate": "é«˜"}]
                    },
                    "financial_metrics": {
                        "NPV": 5000,
                        "ROI": "150%",
                        "payback_period": "2å¹´"
                    },
                    "sensitivity": {
                        "critical_variables": ["å˜é‡1", "å˜é‡2"],
                        "impact_analysis": "å˜é‡1å˜åŒ–10%ï¼ŒNPVå˜åŒ–20%"
                    }
                },
                "suitable_for": ["æŠ•èµ„è¯„ä¼°", "é¡¹ç›®å®¡æ‰¹", "èµ„æºé…ç½®", "æ–¹æ¡ˆæ¯”è¾ƒ"]
            },
            
            # ===== ä¼˜åŒ–æ”¹è¿›ç±» =====
            "lean_thinking": {
                "name": "ç²¾ç›Šæ€ç»´",
                "category": "optimization",
                "description": "æ¶ˆé™¤æµªè´¹ï¼Œåˆ›é€ ä»·å€¼",
                "process": [
                    {"step": "ä»·å€¼è¯†åˆ«", "prompt": "ä»€ä¹ˆæ˜¯å®¢æˆ·çœŸæ­£çš„ä»·å€¼ï¼Ÿ"},
                    {"step": "ä»·å€¼æµå›¾", "prompt": "ä»·å€¼å¦‚ä½•æµåŠ¨ï¼Ÿæ¯æ­¥è€—æ—¶ï¼Ÿ"},
                    {"step": "æµªè´¹è¯†åˆ«", "prompt": "ä¸ƒå¤§æµªè´¹åœ¨å“ªé‡Œï¼Ÿå¦‚ä½•é‡åŒ–ï¼Ÿ"},
                    {"step": "æµåŠ¨ä¼˜åŒ–", "prompt": "å¦‚ä½•è®©ä»·å€¼æµåŠ¨æ›´é¡ºç•…ï¼Ÿ"},
                    {"step": "æŒç»­æ”¹è¿›", "prompt": "å»ºç«‹ä»€ä¹ˆæœºåˆ¶æŒç»­ä¼˜åŒ–ï¼Ÿ"}
                ],
                "output_format": {
                    "customer_value": "å®¢æˆ·ä»·å€¼å®šä¹‰",
                    "value_stream": [
                        {"step": "æ­¥éª¤1", "value_added": true, "time": 10, "wait_time": 5}
                    ],
                    "wastes_identified": {
                        "overproduction": "è¿‡åº¦ç”Ÿäº§ç¤ºä¾‹",
                        "waiting": "ç­‰å¾…æ—¶é—´",
                        "transport": "ä¸å¿…è¦çš„è¿è¾“",
                        "overprocessing": "è¿‡åº¦åŠ å·¥",
                        "inventory": "åº“å­˜ç§¯å‹",
                        "motion": "ä¸å¿…è¦çš„åŠ¨ä½œ",
                        "defects": "ç¼ºé™·ç‡"
                    },
                    "improvements": ["æ”¹è¿›1", "æ”¹è¿›2"],
                    "kaizen_plan": "æŒç»­æ”¹è¿›è®¡åˆ’"
                },
                "suitable_for": ["æµç¨‹ä¼˜åŒ–", "æ•ˆç‡æå‡", "æˆæœ¬é™ä½", "è´¨é‡æ”¹è¿›"]
            },
            
            "pareto_principle": {
                "name": "å¸•ç´¯æ‰˜åŸåˆ™ï¼ˆ80/20æ³•åˆ™ï¼‰",
                "category": "optimization",
                "description": "è¯†åˆ«å…³é”®å°‘æ•°ï¼Œäº§ç”Ÿä¸»è¦å½±å“",
                "process": [
                    {"step": "æ•°æ®æ”¶é›†", "prompt": "æ”¶é›†ç›¸å…³æ•°æ®ï¼Œç¡®ä¿å®Œæ•´å‡†ç¡®"},
                    {"step": "åˆ†ç±»æ’åº", "prompt": "æŒ‰å½±å“ç¨‹åº¦æ’åºï¼Œè®¡ç®—ç´¯ç§¯ç™¾åˆ†æ¯”"},
                    {"step": "å…³é”®è¯†åˆ«", "prompt": "å“ª20%äº§ç”Ÿ80%çš„å½±å“ï¼Ÿ"},
                    {"step": "æ·±å…¥åˆ†æ", "prompt": "ä¸ºä»€ä¹ˆè¿™äº›æ˜¯å…³é”®ï¼Ÿç‰¹å¾æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "ç­–ç•¥åˆ¶å®š", "prompt": "å¦‚ä½•é›†ä¸­èµ„æºä¼˜åŒ–å…³é”®éƒ¨åˆ†ï¼Ÿ"}
                ],
                "output_format": {
                    "data_analysis": {
                        "total_items": 100,
                        "total_impact": 1000
                    },
                    "pareto_distribution": [
                        {"item": "é¡¹ç›®1", "impact": 300, "cumulative_percent": 30}
                    ],
                    "vital_few": ["å…³é”®é¡¹1", "å…³é”®é¡¹2", "å…³é”®é¡¹3"],
                    "characteristics": "å…³é”®é¡¹çš„å…±åŒç‰¹å¾",
                    "optimization_strategy": "èµ„æºé›†ä¸­ç­–ç•¥"
                },
                "suitable_for": ["é—®é¢˜ä¼˜å…ˆçº§", "èµ„æºåˆ†é…", "é‡ç‚¹è¯†åˆ«", "æ•ˆæœæœ€å¤§åŒ–"]
            },
            
            "constraint_theory": {
                "name": "çº¦æŸç†è®ºï¼ˆTOCï¼‰",
                "category": "optimization",
                "description": "ç³»ç»Ÿçš„äº§å‡ºå—é™äºæœ€è–„å¼±ç¯èŠ‚",
                "process": [
                    {"step": "çº¦æŸè¯†åˆ«", "prompt": "ç³»ç»Ÿçš„ç“¶é¢ˆåœ¨å“ªé‡Œï¼Ÿå¦‚ä½•ç¡®è®¤ï¼Ÿ"},
                    {"step": "çº¦æŸåˆ©ç”¨", "prompt": "å¦‚ä½•å……åˆ†åˆ©ç”¨çº¦æŸèµ„æºï¼Ÿ"},
                    {"step": "ç³»ç»Ÿä»å±", "prompt": "å…¶ä»–éƒ¨åˆ†å¦‚ä½•é…åˆçº¦æŸï¼Ÿ"},
                    {"step": "çº¦æŸæå‡", "prompt": "å¦‚ä½•æå‡çº¦æŸçš„èƒ½åŠ›ï¼Ÿ"},
                    {"step": "å¾ªç¯æ”¹è¿›", "prompt": "æ–°çš„çº¦æŸåœ¨å“ªé‡Œï¼Ÿç»§ç»­ä¼˜åŒ–"}
                ],
                "output_format": {
                    "current_constraint": "å½“å‰çº¦æŸ",
                    "constraint_utilization": "çº¦æŸåˆ©ç”¨ç‡",
                    "subordination_plan": "ç³»ç»Ÿé…åˆæ–¹æ¡ˆ",
                    "elevation_options": [
                        {"option": "æ–¹æ¡ˆ1", "cost": 1000, "capacity_increase": "20%"}
                    ],
                    "next_constraint": "é¢„æœŸçš„ä¸‹ä¸€ä¸ªçº¦æŸ"
                },
                "suitable_for": ["ç”Ÿäº§ä¼˜åŒ–", "é¡¹ç›®ç®¡ç†", "æ€§èƒ½æå‡", "æµç¨‹æ”¹è¿›"]
            },
            
            # ===== æ‰¹åˆ¤æ€è€ƒç±» =====
            "critical_thinking": {
                "name": "æ‰¹åˆ¤æ€§æ€ç»´",
                "category": "critical_thinking",
                "description": "ç†æ€§åˆ†æï¼Œç‹¬ç«‹åˆ¤æ–­",
                "process": [
                    {"step": "ä¿¡æ¯è¯„ä¼°", "prompt": "ä¿¡æ¯æ¥æºå¯é å—ï¼Ÿè¯æ®å……åˆ†å—ï¼Ÿ"},
                    {"step": "é€»è¾‘æ£€éªŒ", "prompt": "æ¨ç†è¿‡ç¨‹ä¸¥å¯†å—ï¼Ÿæœ‰é€»è¾‘è°¬è¯¯å—ï¼Ÿ"},
                    {"step": "åè§è¯†åˆ«", "prompt": "å­˜åœ¨ä»€ä¹ˆåè§ï¼Ÿå¦‚ä½•å½±å“åˆ¤æ–­ï¼Ÿ"},
                    {"step": "å‡è®¾è´¨ç–‘", "prompt": "éšå«å‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿåˆç†å—ï¼Ÿ"},
                    {"step": "ç»“è®ºå½¢æˆ", "prompt": "åŸºäºåˆ†æï¼Œåˆç†çš„ç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ"}
                ],
                "output_format": {
                    "information_quality": {
                        "sources": ["æ¥æº1å¯é æ€§: é«˜"],
                        "evidence_strength": "è¯æ®å¼ºåº¦è¯„ä¼°"
                    },
                    "logical_analysis": {
                        "reasoning_chain": "æ¨ç†é“¾",
                        "fallacies_found": ["é€»è¾‘è°¬è¯¯1"]
                    },
                    "biases_identified": ["ç¡®è®¤åè§", "å¹¸å­˜è€…åå·®"],
                    "assumptions_challenged": ["å‡è®¾1: ä¸æˆç«‹"],
                    "balanced_conclusion": "å¹³è¡¡çš„ç»“è®º"
                },
                "suitable_for": ["ä¿¡æ¯è¯„ä¼°", "å†³ç­–éªŒè¯", "é£é™©è¯†åˆ«", "çœŸç›¸è¿½æ±‚"]
            },
            
            "occams_razor": {
                "name": "å¥¥å¡å§†å‰ƒåˆ€",
                "category": "critical_thinking",
                "description": "å¦‚æ— å¿…è¦ï¼Œå‹¿å¢å®ä½“",
                "process": [
                    {"step": "æ–¹æ¡ˆåˆ—ä¸¾", "prompt": "æ‰€æœ‰å¯èƒ½çš„è§£é‡Šæˆ–æ–¹æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "å¤æ‚åº¦è¯„ä¼°", "prompt": "æ¯ä¸ªæ–¹æ¡ˆçš„å¤æ‚åº¦å¦‚ä½•ï¼Ÿå‡è®¾å¤šå°‘ï¼Ÿ"},
                    {"step": "å……åˆ†æ€§æ£€éªŒ", "prompt": "æœ€ç®€å•çš„æ–¹æ¡ˆèƒ½å……åˆ†è§£é‡Šå—ï¼Ÿ"},
                    {"step": "å¿…è¦æ€§åˆ†æ", "prompt": "å¤æ‚å…ƒç´ çœŸçš„å¿…è¦å—ï¼Ÿ"},
                    {"step": "ç®€åŒ–é€‰æ‹©", "prompt": "é€‰æ‹©æœ€ç®€å•å……åˆ†çš„æ–¹æ¡ˆ"}
                ],
                "output_format": {
                    "all_explanations": [
                        {"explanation": "æ–¹æ¡ˆ1", "complexity": 3, "assumptions": 2}
                    ],
                    "sufficiency_test": {
                        "simplest": "æœ€ç®€æ–¹æ¡ˆ",
                        "is_sufficient": true,
                        "gaps": []
                    },
                    "unnecessary_elements": ["ä¸å¿…è¦çš„å¤æ‚æ€§1"],
                    "recommended_solution": "æ¨èçš„ç®€å•æ–¹æ¡ˆ"
                },
                "suitable_for": ["æ–¹æ¡ˆé€‰æ‹©", "é—®é¢˜è¯Šæ–­", "è®¾è®¡ç®€åŒ–", "ç†è®ºæ„å»º"]
            },
            
            # ===== åˆ›æ„å‘æ•£ç±» =====
            "brainstorming": {
                "name": "å¤´è„‘é£æš´æ³•",
                "category": "creative_expansion",
                "description": "å‘æ•£æ€ç»´ï¼Œå»¶è¿Ÿåˆ¤æ–­",
                "process": [
                    {"step": "é—®é¢˜æ˜ç¡®", "prompt": "è¦è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿç›®æ ‡æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "è‡ªç”±å‘æ•£", "prompt": "ä¸è¯„åˆ¤ï¼Œè¿½æ±‚æ•°é‡ï¼Œè¶Šå¤šè¶Šå¥½"},
                    {"step": "è”æƒ³æ¿€å‘", "prompt": "åŸºäºå·²æœ‰æƒ³æ³•ï¼Œèƒ½è”æƒ³åˆ°ä»€ä¹ˆï¼Ÿ"},
                    {"step": "ç‹‚é‡æ€è€ƒ", "prompt": "æœ€ç–¯ç‹‚çš„æƒ³æ³•æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "æ•´ç†å½’ç±»", "prompt": "å°†æƒ³æ³•åˆ†ç±»æ•´ç†ï¼Œè¯†åˆ«æ¨¡å¼"}
                ],
                "output_format": {
                    "problem_statement": "é—®é¢˜é™ˆè¿°",
                    "ideas_raw": ["æƒ³æ³•1", "æƒ³æ³•2", "..."],
                    "idea_categories": {
                        "practical": ["å®ç”¨æƒ³æ³•"],
                        "innovative": ["åˆ›æ–°æƒ³æ³•"],
                        "wild": ["ç–¯ç‹‚æƒ³æ³•"]
                    },
                    "patterns_found": ["æ¨¡å¼1", "æ¨¡å¼2"],
                    "promising_directions": ["æœ‰æ½œåŠ›çš„æ–¹å‘1"]
                },
                "suitable_for": ["åˆ›æ„ç”Ÿæˆ", "é—®é¢˜è§£å†³", "äº§å“åˆ›æ–°", "å›¢é˜Ÿåä½œ"]
            },
            
            "six_thinking_hats": {
                "name": "å…­é¡¶æ€è€ƒå¸½",
                "category": "creative_expansion",
                "description": "å¹³è¡Œæ€ç»´ï¼Œå…¨é¢è€ƒè™‘",
                "process": [
                    {"step": "ç™½å¸½æ€è€ƒ", "prompt": "å®¢è§‚äº‹å®å’Œæ•°æ®æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "çº¢å¸½æ€è€ƒ", "prompt": "ç›´è§‰å’Œæƒ…æ„Ÿå‘Šè¯‰æˆ‘ä»¬ä»€ä¹ˆï¼Ÿ"},
                    {"step": "é»‘å¸½æ€è€ƒ", "prompt": "é£é™©å’Œé—®é¢˜åœ¨å“ªé‡Œï¼Ÿ"},
                    {"step": "é»„å¸½æ€è€ƒ", "prompt": "ç§¯æé¢å’Œä»·å€¼æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "ç»¿å¸½æ€è€ƒ", "prompt": "æœ‰ä»€ä¹ˆåˆ›æ–°æƒ³æ³•å’Œå¯èƒ½ï¼Ÿ"},
                    {"step": "è“å¸½æ€è€ƒ", "prompt": "å¦‚ä½•ç»„ç»‡æ€è€ƒè¿‡ç¨‹ï¼Ÿä¸‹ä¸€æ­¥ï¼Ÿ"}
                ],
                "output_format": {
                    "white_hat": {
                        "facts": ["äº‹å®1", "äº‹å®2"],
                        "data": ["æ•°æ®1", "æ•°æ®2"]
                    },
                    "red_hat": {
                        "feelings": ["æ„Ÿå—1"],
                        "intuitions": ["ç›´è§‰1"]
                    },
                    "black_hat": {
                        "risks": ["é£é™©1"],
                        "problems": ["é—®é¢˜1"]
                    },
                    "yellow_hat": {
                        "benefits": ["å¥½å¤„1"],
                        "values": ["ä»·å€¼1"]
                    },
                    "green_hat": {
                        "ideas": ["åˆ›æ„1"],
                        "alternatives": ["æ›¿ä»£æ–¹æ¡ˆ1"]
                    },
                    "blue_hat": {
                        "process": "æ€è€ƒè¿‡ç¨‹æ€»ç»“",
                        "next_steps": ["ä¸‹ä¸€æ­¥1"]
                    }
                },
                "suitable_for": ["å…¨é¢åˆ†æ", "å›¢é˜Ÿå†³ç­–", "å¹³è¡¡æ€è€ƒ", "å¤æ‚è¯„ä¼°"]
            },
            
            # ===== é€»è¾‘æ¨ç†ç±» =====
            "inductive_deductive": {
                "name": "å½’çº³ä¸æ¼”ç»",
                "category": "logical_reasoning",
                "description": "ä»ç‰¹æ®Šåˆ°ä¸€èˆ¬ï¼Œä»ä¸€èˆ¬åˆ°ç‰¹æ®Š",
                "process": [
                    {"step": "è§‚å¯Ÿæ”¶é›†", "prompt": "å…·ä½“æ¡ˆä¾‹å’Œè§‚å¯Ÿæ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "æ¨¡å¼è¯†åˆ«", "prompt": "æ¡ˆä¾‹ä¸­çš„å…±åŒæ¨¡å¼æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "è§„å¾‹æç‚¼", "prompt": "èƒ½å½’çº³å‡ºä»€ä¹ˆä¸€èˆ¬è§„å¾‹ï¼Ÿ"},
                    {"step": "æ¼”ç»åº”ç”¨", "prompt": "å°†è§„å¾‹åº”ç”¨åˆ°æ–°æƒ…å†µä¼šæ€æ ·ï¼Ÿ"},
                    {"step": "éªŒè¯è°ƒæ•´", "prompt": "é¢„æµ‹å‡†ç¡®å—ï¼Ÿè§„å¾‹éœ€è¦è°ƒæ•´å—ï¼Ÿ"}
                ],
                "output_format": {
                    "observations": ["è§‚å¯Ÿ1", "è§‚å¯Ÿ2", "è§‚å¯Ÿ3"],
                    "patterns": ["æ¨¡å¼1", "æ¨¡å¼2"],
                    "general_rule": "å½’çº³å‡ºçš„ä¸€èˆ¬è§„å¾‹",
                    "deductive_predictions": [
                        {"case": "æ–°æƒ…å†µ1", "prediction": "é¢„æµ‹ç»“æœ1"}
                    ],
                    "validation": {
                        "accuracy": "é¢„æµ‹å‡†ç¡®ç‡",
                        "adjustments": "è§„å¾‹è°ƒæ•´"
                    }
                },
                "suitable_for": ["è§„å¾‹å‘ç°", "ç†è®ºåº”ç”¨", "é¢„æµ‹åˆ†æ", "ç§‘å­¦ç ”ç©¶"]
            },
            
            "hypothesis_driven": {
                "name": "å‡è®¾é©±åŠ¨æ€ç»´",
                "category": "logical_reasoning",
                "description": "å…ˆå‡è®¾åéªŒè¯ï¼Œå¿«é€Ÿè¿­ä»£",
                "process": [
                    {"step": "å‡è®¾å½¢æˆ", "prompt": "åŸºäºç°æœ‰ä¿¡æ¯ï¼Œæœ€å¯èƒ½çš„è§£é‡Šæ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "éªŒè¯è®¾è®¡", "prompt": "å¦‚ä½•éªŒè¯è¿™ä¸ªå‡è®¾ï¼Ÿéœ€è¦ä»€ä¹ˆè¯æ®ï¼Ÿ"},
                    {"step": "æ•°æ®æ”¶é›†", "prompt": "æ”¶é›†å…³é”®æ•°æ®ï¼Œä¸æ±‚å®Œç¾"},
                    {"step": "å‡è®¾æ£€éªŒ", "prompt": "æ•°æ®æ”¯æŒè¿˜æ˜¯åé©³å‡è®¾ï¼Ÿ"},
                    {"step": "è¿­ä»£ä¼˜åŒ–", "prompt": "å¦‚ä½•ä¿®æ­£å‡è®¾ï¼Ÿä¸‹ä¸€ä¸ªå‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿ"}
                ],
                "output_format": {
                    "initial_hypothesis": "åˆå§‹å‡è®¾",
                    "validation_plan": {
                        "key_questions": ["å…³é”®é—®é¢˜1"],
                        "required_evidence": ["æ‰€éœ€è¯æ®1"],
                        "validation_method": "éªŒè¯æ–¹æ³•"
                    },
                    "findings": {
                        "supporting": ["æ”¯æŒè¯æ®1"],
                        "contradicting": ["åé©³è¯æ®1"]
                    },
                    "hypothesis_status": "å‡è®¾çŠ¶æ€ï¼šæ”¯æŒ/åé©³/éœ€ä¿®æ­£",
                    "next_hypothesis": "ä¸‹ä¸€ä¸ªå‡è®¾"
                },
                "suitable_for": ["å¿«é€Ÿå†³ç­–", "æ•æ·å¼€å‘", "ç§‘å­¦ç ”ç©¶", "é—®é¢˜è¯Šæ–­"]
            },
            
            # ===== åœºæ™¯è§„åˆ’ç±» =====
            "user_journey": {
                "name": "ç”¨æˆ·æ—…ç¨‹æ˜ å°„",
                "category": "scenario_planning",
                "description": "ä»ç”¨æˆ·è§†è§’å®¡è§†å…¨è¿‡ç¨‹",
                "process": [
                    {"step": "ç”¨æˆ·å®šä¹‰", "prompt": "ç”¨æˆ·æ˜¯è°ï¼Ÿç›®æ ‡å’Œéœ€æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "é˜¶æ®µåˆ’åˆ†", "prompt": "ç”¨æˆ·ç»å†å“ªäº›é˜¶æ®µï¼Ÿ"},
                    {"step": "è§¦ç‚¹è¯†åˆ«", "prompt": "æ¯ä¸ªé˜¶æ®µçš„æ¥è§¦ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ"},
                    {"step": "æƒ…ç»ªæ›²çº¿", "prompt": "ç”¨æˆ·æƒ…ç»ªå¦‚ä½•å˜åŒ–ï¼Ÿé«˜ä½ç‚¹åœ¨å“ªï¼Ÿ"},
                    {"step": "æœºä¼šå‘ç°", "prompt": "å“ªé‡Œæœ‰æ”¹è¿›æœºä¼šï¼Ÿç—›ç‚¹å¦‚ä½•è§£å†³ï¼Ÿ"}
                ],
                "output_format": {
                    "user_persona": {
                        "demographics": "ç”¨æˆ·ç”»åƒ",
                        "goals": ["ç›®æ ‡1", "ç›®æ ‡2"],
                        "needs": ["éœ€æ±‚1", "éœ€æ±‚2"]
                    },
                    "journey_stages": ["è®¤çŸ¥", "è€ƒè™‘", "å†³ç­–", "ä½¿ç”¨", "åˆ†äº«"],
                    "touchpoints": {
                        "è®¤çŸ¥": ["å¹¿å‘Š", "å£ç¢‘"],
                        "è€ƒè™‘": ["å®˜ç½‘", "è¯„æµ‹"]
                    },
                    "emotion_curve": {
                        "è®¤çŸ¥": 0,
                        "è€ƒè™‘": 0.5,
                        "å†³ç­–": -0.3,
                        "ä½¿ç”¨": 0.8
                    },
                    "opportunities": [
                        {"stage": "å†³ç­–", "pain_point": "é€‰æ‹©å›°éš¾", "solution": "å†³ç­–è¾…åŠ©å·¥å…·"}
                    ]
                },
                "suitable_for": ["æœåŠ¡è®¾è®¡", "ä½“éªŒä¼˜åŒ–", "äº§å“æ”¹è¿›", "å®¢æˆ·æ´å¯Ÿ"]
            }
        }
    
    def _init_trigger_system(self):
        """åˆå§‹åŒ–æ™ºèƒ½è§¦å‘ç³»ç»Ÿ"""
        # å…³é”®è¯è§¦å‘æ˜ å°„
        self.keyword_triggers = {
            "socratic_questioning": ["éœ€æ±‚", "åŠŸèƒ½", "feature", "æƒ³è¦", "éœ€è¦", "æœŸæœ›", "è¦æ±‚", "å¸Œæœ›"],
            "five_whys": ["é—®é¢˜", "bug", "é”™è¯¯", "å¤±è´¥", "å¼‚å¸¸", "æ•…éšœ", "ä¸ºä»€ä¹ˆ", "åŸå› "],
            "first_principles": ["æœ¬è´¨", "æ ¹æœ¬", "åŸç†", "åŸºç¡€", "æ ¸å¿ƒ", "åˆ›æ–°", "çªç ´", "é‡æ–°"],
            "mece_decomposition": ["åˆ†è§£", "æ‹†åˆ†", "breakdown", "ä»»åŠ¡", "è®¡åˆ’", "ç»„ç»‡", "ç»“æ„", "åˆ†ç±»"],
            "pyramid_principle": ["æŠ¥å‘Š", "æ±‡æŠ¥", "æ€»ç»“", "è¡¨è¾¾", "æ²Ÿé€š", "é€»è¾‘", "è¯´æœ", "å±•ç¤º"],
            "mind_mapping": ["æ•´ç†", "æ¢³ç†", "å…³è”", "brainstorm", "å…¨æ™¯", "è„‰ç»œ", "å…³ç³»", "è”ç³»"],
            "systems_thinking": ["ç³»ç»Ÿ", "æ¶æ„", "æ•´ä½“", "å…¨å±€", "å½±å“", "ç”Ÿæ€", "å…³ç³»", "ç›¸äº’"],
            "value_chain": ["ä»·å€¼", "æµç¨‹", "æ•ˆç‡", "ä¼˜åŒ–", "æˆæœ¬", "æ”¶ç›Š", "ä»·å€¼é“¾", "å¢å€¼"],
            "ecosystem_thinking": ["ç”Ÿæ€", "å¹³å°", "ç½‘ç»œæ•ˆåº”", "ååŒ", "å…±ç”Ÿ", "å‚ä¸è€…", "äº’åŠ¨"],
            "design_thinking": ["è®¾è®¡", "ç”¨æˆ·ä½“éªŒ", "åˆ›æ–°", "åŸå‹", "è¿­ä»£", "ç”¨æˆ·", "ä½“éªŒ", "äººæ€§åŒ–"],
            "reverse_thinking": ["åå‘", "é€†å‘", "ç›¸å", "å¦‚æœä¸", "å‡å¦‚", "åè¿‡æ¥", "é¢ å€’"],
            "analogical_thinking": ["ç±»ä¼¼", "åƒ", "å‚è€ƒ", "å€Ÿé‰´", "å¯¹æ¯”", "ç›¸ä¼¼", "ç±»æ¯”", "æ¯”å–»"],
            "swot_analysis": ["ä¼˜åŠ¿", "åŠ£åŠ¿", "æœºä¼š", "å¨èƒ", "è¯„ä¼°", "åˆ†æ", "SWOT", "æˆ˜ç•¥"],
            "decision_tree": ["å†³ç­–", "é€‰æ‹©", "å¦‚æœ", "åˆ†æ”¯", "æ¦‚ç‡", "é£é™©", "é€‰é¡¹", "è·¯å¾„"],
            "cost_benefit": ["æˆæœ¬", "æ”¶ç›Š", "æŠ•èµ„å›æŠ¥", "ROI", "æ•ˆç›Š", "ä»·å€¼", "æŠ•å…¥äº§å‡º"],
            "lean_thinking": ["ç²¾ç›Š", "æµªè´¹", "æ•ˆç‡", "æ”¹è¿›", "ä¼˜åŒ–", "æµç¨‹", "ä»·å€¼æµ", "æ¶ˆé™¤"],
            "pareto_principle": ["é‡ç‚¹", "å…³é”®", "ä¸»è¦", "ä¼˜å…ˆçº§", "80/20", "å¸•ç´¯æ‰˜", "å°‘æ•°"],
            "constraint_theory": ["ç“¶é¢ˆ", "çº¦æŸ", "é™åˆ¶", "äº§èƒ½", "ååé‡", "åˆ¶çº¦", "TOC"],
            "critical_thinking": ["è´¨ç–‘", "éªŒè¯", "è¯æ®", "é€»è¾‘", "åè§", "å‡è®¾", "æ‰¹åˆ¤", "å®¢è§‚"],
            "occams_razor": ["ç®€åŒ–", "ç®€å•", "å¤æ‚", "ç²¾ç®€", "æœ¬è´¨", "å¥¥å¡å§†", "å¿…è¦æ€§"],
            "brainstorming": ["åˆ›æ„", "ç‚¹å­", "æ–¹æ¡ˆ", "å¯èƒ½æ€§", "brainstorm", "æƒ³æ³•", "åˆ›æ–°"],
            "six_thinking_hats": ["å…¨é¢", "è§’åº¦", "è§†è§’", "ç«‹åœº", "å¹³è¡¡", "å…­é¡¶å¸½å­", "å¤šè§’åº¦"],
            "inductive_deductive": ["æ¨ç†", "é€»è¾‘", "ç»“è®º", "è§„å¾‹", "æ¨å¯¼", "å½’çº³", "æ¼”ç»"],
            "hypothesis_driven": ["å‡è®¾", "éªŒè¯", "å‡å¦‚", "å¦‚æœ", "æµ‹è¯•", "çŒœæµ‹", "æ¨æµ‹"],
            "user_journey": ["ç”¨æˆ·", "ä½“éªŒ", "æµç¨‹", "è§¦ç‚¹", "æ—…ç¨‹", "è·¯å¾„", "å†ç¨‹"]
        }
        
        # åœºæ™¯è§¦å‘æ˜ å°„
        self.scenario_triggers = {
            "éœ€æ±‚åˆ†æ": ["socratic_questioning", "first_principles", "design_thinking", "user_journey"],
            "é—®é¢˜è§£å†³": ["five_whys", "systems_thinking", "constraint_theory", "critical_thinking"],
            "åˆ›æ–°è®¾è®¡": ["design_thinking", "reverse_thinking", "analogical_thinking", "brainstorming"],
            "é¡¹ç›®è§„åˆ’": ["mece_decomposition", "value_chain", "pareto_principle", "decision_tree"],
            "å†³ç­–åˆ¶å®š": ["swot_analysis", "decision_tree", "cost_benefit", "six_thinking_hats"],
            "ä¼˜åŒ–æ”¹è¿›": ["lean_thinking", "pareto_principle", "constraint_theory", "value_chain"],
            "æˆ˜ç•¥åˆ†æ": ["swot_analysis", "ecosystem_thinking", "systems_thinking", "value_chain"],
            "æ‰¹åˆ¤è¯„ä¼°": ["critical_thinking", "occams_razor", "hypothesis_driven", "six_thinking_hats"]
        }
        
        # æ¨¡å¼ç»„åˆæ¨è
        self.pattern_combinations = {
            "deep_analysis": ["five_whys", "systems_thinking", "first_principles"],
            "innovation_kit": ["design_thinking", "reverse_thinking", "analogical_thinking"],
            "decision_toolkit": ["swot_analysis", "decision_tree", "cost_benefit"],
            "optimization_suite": ["lean_thinking", "constraint_theory", "pareto_principle"],
            "critical_review": ["critical_thinking", "occams_razor", "six_thinking_hats"],
            "strategic_planning": ["swot_analysis", "value_chain", "ecosystem_thinking"],
            "problem_solving": ["five_whys", "mece_decomposition", "hypothesis_driven"],
            "user_centered": ["design_thinking", "user_journey", "socratic_questioning"]
        }
    
    def _init_analysis_engine(self):
        """åˆå§‹åŒ–åˆ†æå¼•æ“"""
        self.analysis_metrics = {
            "depth_levels": {
                "shallow": {"rounds": 2, "modes": 1, "detail": "low"},
                "medium": {"rounds": 5, "modes": 3, "detail": "medium"},
                "deep": {"rounds": 8, "modes": 4, "detail": "high"},
                "expert": {"rounds": 12, "modes": 5, "detail": "exhaustive"}
            },
            "quality_thresholds": {
                "insight_quality": 0.7,
                "pattern_confidence": 0.8,
                "recommendation_strength": 0.75
            }
        }
    
    def get_name(self) -> str:
        return "think"
    
    def get_description(self) -> str:
        return "25ç§ä¸“å®¶çº§æ€ç»´æ¨¡å¼çš„æ™ºèƒ½æ€è€ƒå·¥å…·ï¼Œæä¾›å¤šç»´åº¦æ·±åº¦åˆ†æ"
    
    def get_system_prompt_name(self) -> str:
        return "thinking_enhanced"
    
    def get_workflow_prompt_name(self) -> str:
        return "thinking_workflow_enhanced"
    
    async def run_workflow(self, request: ToolRequest) -> ToolOutput:
        """æ‰§è¡Œå¢å¼ºçš„æ€ç»´å·¥ä½œæµ"""
        try:
            problem = request.params.get("problem", "")
            mode = request.params.get("mode", "auto")
            depth = request.params.get("depth", "medium")
            context = request.params.get("context", {})
            
            # è·å–è®°å¿†æ¨è
            memory_recommendations = self.memory_system.get_recommended_thinking_modes(
                problem, context
            )
            
            # æ™ºèƒ½é€‰æ‹©æ€ç»´æ¨¡å¼
            if mode == "auto":
                selected_modes = await self._intelligent_mode_selection(
                    problem, context, memory_recommendations
                )
            else:
                selected_modes = [mode] if isinstance(mode, str) else mode
            
            # éªŒè¯æ¨¡å¼æœ‰æ•ˆæ€§
            selected_modes = [m for m in selected_modes if m in self.thinking_modes]
            
            if not selected_modes:
                selected_modes = ["systems_thinking", "first_principles"]
            
            # æ”¶é›†å¢å¼ºçš„ä¸Šä¸‹æ–‡
            enhanced_context = await self._gather_enhanced_context(problem, context)
            
            # æ‰§è¡Œå¤šæ¨¡å¼åˆ†æ
            analysis_results = {}
            for thinking_mode in selected_modes:
                self.update_confidence(0.2)  # æ¯ä¸ªæ¨¡å¼å¢åŠ ä¿¡å¿ƒ
                
                mode_result = await self._execute_thinking_mode(
                    mode=thinking_mode,
                    problem=problem,
                    context=enhanced_context,
                    depth=depth
                )
                
                analysis_results[thinking_mode] = mode_result
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¸“å®¶ä»‹å…¥
                if self.should_consult_expert():
                    expert_result = await self._consult_thinking_expert(
                        mode=thinking_mode,
                        analysis=mode_result,
                        problem=problem
                    )
                    analysis_results[f"{thinking_mode}_expert"] = expert_result
            
            # ç»¼åˆå¤šæ¨¡å¼ç»“æœ
            synthesis = await self._synthesize_thinking_results(
                analysis_results, problem, selected_modes
            )
            
            # ç”Ÿæˆæ·±åº¦æ´å¯Ÿ
            insights = await self._generate_deep_insights(synthesis, enhanced_context)
            
            # ä»»åŠ¡ç®¡ç†å»ºè®®
            task_recommendations = await self._generate_task_recommendations(
                problem, insights, enhanced_context
            )
            
            # ä¿å­˜æ€ç»´æ¨¡å¼æ•ˆæœ
            quality_score = self._evaluate_thinking_quality(insights)
            self.memory_system.save_enhanced_memory(
                content={
                    "problem": problem,
                    "analysis": synthesis,
                    "insights": insights,
                    "recommendations": task_recommendations
                },
                metadata={
                    "thinking_modes": selected_modes,
                    "quality_score": quality_score,
                    "type": "thinking_analysis",
                    "tags": ["thinking", "analysis"] + selected_modes
                }
            )
            
            # æ›´æ–°å­¦ä¹ ç³»ç»Ÿ
            self.memory_system._update_pattern_learning(
                selected_modes,
                quality_score,
                self._classify_problem_type(problem)
            )
            
            return ToolOutput(
                status="success",
                content=json.dumps({
                    "thinking_modes_used": selected_modes,
                    "analysis_depth": depth,
                    "key_insights": insights["key_insights"],
                    "synthesis": synthesis,
                    "recommendations": {
                        "immediate_actions": task_recommendations["immediate"],
                        "strategic_considerations": task_recommendations["strategic"],
                        "task_management": task_recommendations["task_management"]
                    },
                    "patterns_discovered": insights.get("patterns", []),
                    "confidence_level": quality_score
                }, indent=2),
                metadata={
                    "thinking_modes": selected_modes,
                    "depth": depth,
                    "quality_score": quality_score
                }
            )
            
        except Exception as e:
            return ToolOutput(
                status="error",
                error=f"æ€ç»´åˆ†æè¿‡ç¨‹å‡ºé”™ï¼š{str(e)}"
            )
    
    async def _intelligent_mode_selection(self, problem: str, context: Dict, 
                                        memory_recommendations: List[str]) -> List[str]:
        """æ™ºèƒ½é€‰æ‹©æœ€é€‚åˆçš„æ€ç»´æ¨¡å¼ç»„åˆ"""
        selected_modes = []
        problem_lower = problem.lower()
        
        # 1. åŸºäºå…³é”®è¯åŒ¹é…
        keyword_scores = defaultdict(int)
        for mode, keywords in self.keyword_triggers.items():
            for keyword in keywords:
                if keyword in problem_lower:
                    keyword_scores[mode] += 1
        
        # 2. åŸºäºåœºæ™¯è¯†åˆ«
        detected_scenario = self._detect_scenario(problem, context)
        scenario_modes = []
        if detected_scenario:
            scenario_modes = self.scenario_triggers.get(detected_scenario, [])
        
        # 3. åŸºäºè®°å¿†æ¨è
        memory_modes = memory_recommendations[:3]
        
        # 4. ç»¼åˆè¯„åˆ†
        all_modes = set()
        
        # æ·»åŠ å…³é”®è¯åŒ¹é…çš„å‰3ä¸ª
        keyword_top = sorted(keyword_scores.items(), key=lambda x: x[1], reverse=True)[:3]
        all_modes.update([mode for mode, _ in keyword_top if _ > 0])
        
        # æ·»åŠ åœºæ™¯æ¨èçš„å‰2ä¸ª
        all_modes.update(scenario_modes[:2])
        
        # æ·»åŠ è®°å¿†æ¨è
        all_modes.update(memory_modes)
        
        # 5. æ ¹æ®é—®é¢˜å¤æ‚åº¦ç¡®å®šæ¨¡å¼æ•°é‡
        complexity = self._assess_problem_complexity(problem, context)
        mode_count = {
            "simple": 2,
            "moderate": 3,
            "complex": 4,
            "very_complex": 5
        }.get(complexity, 3)
        
        # 6. ä¼˜å…ˆçº§æ’åº
        prioritized_modes = []
        
        # ä¼˜å…ˆè®°å¿†æ¨èï¼ˆå·²è¢«éªŒè¯æœ‰æ•ˆï¼‰
        for mode in memory_modes:
            if mode in all_modes and mode not in prioritized_modes:
                prioritized_modes.append(mode)
        
        # ç„¶åæ˜¯åœºæ™¯æ¨è
        for mode in scenario_modes:
            if mode in all_modes and mode not in prioritized_modes:
                prioritized_modes.append(mode)
        
        # æœ€åæ˜¯å…³é”®è¯åŒ¹é…
        for mode, _ in keyword_top:
            if mode in all_modes and mode not in prioritized_modes:
                prioritized_modes.append(mode)
        
        # 7. é€‰æ‹©æœ€ç»ˆæ¨¡å¼
        selected_modes = prioritized_modes[:mode_count]
        
        # 8. ç¡®ä¿è‡³å°‘æœ‰åŸºç¡€æ¨¡å¼
        if len(selected_modes) < 2:
            defaults = ["systems_thinking", "first_principles", "mece_decomposition"]
            for default in defaults:
                if default not in selected_modes:
                    selected_modes.append(default)
                    if len(selected_modes) >= 2:
                        break
        
        return selected_modes
    
    async def _execute_thinking_mode(self, mode: str, problem: str, 
                                   context: Dict, depth: str) -> Dict:
        """æ‰§è¡Œç‰¹å®šæ€ç»´æ¨¡å¼çš„åˆ†æ"""
        mode_config = self.thinking_modes.get(mode, {})
        
        if not mode_config:
            return {"error": f"Unknown thinking mode: {mode}"}
        
        results = {
            "mode": mode,
            "mode_name": mode_config["name"],
            "category": mode_config["category"],
            "process_results": [],
            "key_findings": [],
            "patterns": [],
            "recommendations": []
        }
        
        # æ ¹æ®æ·±åº¦ç¡®å®šæ‰§è¡Œè½®æ•°
        depth_config = self.analysis_metrics["depth_levels"][depth]
        rounds = min(len(mode_config["process"]), depth_config["rounds"])
        
        # æ‰§è¡Œæ€ç»´æµç¨‹
        for i, step in enumerate(mode_config["process"][:rounds]):
            step_result = await self._execute_thinking_step(
                mode=mode,
                step=step,
                problem=problem,
                context=context,
                previous_results=results["process_results"]
            )
            
            results["process_results"].append({
                "step_name": step["step"],
                "prompt": step["prompt"],
                "analysis": step_result.get("analysis", ""),
                "findings": step_result.get("findings", [])
            })
            
            # æå–å…³é”®å‘ç°
            if "findings" in step_result:
                results["key_findings"].extend(step_result["findings"])
        
        # è¯†åˆ«æ¨¡å¼
        if len(results["key_findings"]) > 3:
            patterns = self._identify_patterns(results["key_findings"])
            results["patterns"] = patterns
        
        # ç”Ÿæˆæ¨¡å¼ç‰¹å®šçš„å»ºè®®
        results["recommendations"] = self._generate_mode_recommendations(
            mode, results["key_findings"], context
        )
        
        return results
    
    async def _synthesize_thinking_results(self, all_results: Dict, 
                                         problem: str, modes: List[str]) -> Dict:
        """ç»¼åˆå¤šç§æ€ç»´æ¨¡å¼çš„åˆ†æç»“æœ"""
        synthesis = {
            "problem": problem,
            "modes_applied": modes,
            "converging_insights": [],
            "unique_perspectives": [],
            "contradictions": [],
            "integrated_view": "",
            "confidence_distribution": {}
        }
        
        # æ”¶é›†æ‰€æœ‰å‘ç°
        all_findings = []
        for mode, result in all_results.items():
            if "key_findings" in result:
                for finding in result["key_findings"]:
                    all_findings.append({
                        "mode": mode,
                        "finding": finding,
                        "category": result.get("category", "unknown")
                    })
        
        # åˆ†ææ”¶æ•›ç‚¹
        finding_clusters = self._cluster_findings(all_findings)
        
        for cluster_id, cluster in finding_clusters.items():
            if len(cluster["modes"]) >= 2:
                synthesis["converging_insights"].append({
                    "insight": cluster["representative"],
                    "supported_by": cluster["modes"],
                    "strength": len(cluster["modes"]) / len(modes),
                    "findings": cluster["findings"]
                })
            else:
                synthesis["unique_perspectives"].append({
                    "perspective": cluster["representative"],
                    "from_mode": cluster["modes"][0],
                    "category": cluster["category"]
                })
        
        # è¯†åˆ«çŸ›ç›¾
        contradictions = self._identify_contradictions(all_findings)
        synthesis["contradictions"] = contradictions
        
        # ç”Ÿæˆç»¼åˆè§†å›¾
        synthesis["integrated_view"] = self._create_integrated_view(
            synthesis["converging_insights"],
            synthesis["unique_perspectives"],
            synthesis["contradictions"]
        )
        
        # è®¡ç®—ç½®ä¿¡åº¦åˆ†å¸ƒ
        for mode in modes:
            if mode in all_results:
                findings_count = len(all_results[mode].get("key_findings", []))
                synthesis["confidence_distribution"][mode] = min(findings_count * 0.1, 1.0)
        
        return synthesis
    
    async def _generate_deep_insights(self, synthesis: Dict, context: Dict) -> Dict:
        """ç”Ÿæˆæ·±åº¦æ´å¯Ÿ"""
        insights = {
            "key_insights": [],
            "patterns": [],
            "implications": [],
            "blind_spots": [],
            "opportunities": [],
            "risks": []
        }
        
        # æå–å…³é”®æ´å¯Ÿï¼ˆåŸºäºæ”¶æ•›åº¦å’Œå¼ºåº¦ï¼‰
        for converging in synthesis.get("converging_insights", [])[:5]:
            insight = {
                "insight": converging["insight"],
                "confidence": converging["strength"],
                "based_on": converging["supported_by"],
                "implication": self._analyze_implication(converging["insight"], context)
            }
            insights["key_insights"].append(insight)
        
        # è¯†åˆ«æ¨¡å¼
        if "patterns" in synthesis:
            insights["patterns"] = synthesis["patterns"]
        
        # åˆ†æç›²ç‚¹ï¼ˆæ²¡æœ‰è¢«ä»»ä½•æ¨¡å¼è¦†ç›–çš„æ–¹é¢ï¼‰
        covered_aspects = set()
        for finding in synthesis.get("converging_insights", []) + synthesis.get("unique_perspectives", []):
            covered_aspects.update(self._extract_aspects(finding.get("insight") or finding.get("perspective")))
        
        potential_blind_spots = self._identify_blind_spots(
            synthesis["problem"], 
            covered_aspects, 
            context
        )
        insights["blind_spots"] = potential_blind_spots
        
        # è¯†åˆ«æœºä¼šå’Œé£é™©
        for perspective in synthesis.get("unique_perspectives", []):
            perspective_text = perspective["perspective"].lower()
            if any(word in perspective_text for word in ["æœºä¼š", "æ½œåŠ›", "å¯èƒ½", "opportunity", "potential"]):
                insights["opportunities"].append(perspective["perspective"])
            elif any(word in perspective_text for word in ["é£é™©", "å¨èƒ", "é—®é¢˜", "risk", "threat", "danger"]):
                insights["risks"].append(perspective["perspective"])
        
        # ç”Ÿæˆæ·±å±‚å«ä¹‰
        insights["implications"] = self._generate_implications(
            insights["key_insights"],
            insights["patterns"],
            context
        )
        
        return insights
    
    def _cluster_findings(self, findings: List[Dict]) -> Dict[str, Dict]:
        """èšç±»ç›¸ä¼¼çš„å‘ç°"""
        clusters = {}
        cluster_id = 0
        
        # ç®€åŒ–çš„èšç±»é€»è¾‘ï¼ˆå®é™…å¯ç”¨æ›´å¤æ‚çš„NLPæ–¹æ³•ï¼‰
        for finding in findings:
            finding_text = finding["finding"].lower()
            matched = False
            
            # å°è¯•åŒ¹é…ç°æœ‰èšç±»
            for cid, cluster in clusters.items():
                cluster_text = cluster["representative"].lower()
                # ç®€å•çš„ç›¸ä¼¼åº¦åˆ¤æ–­
                if self._calculate_similarity(finding_text, cluster_text) > 0.7:
                    cluster["findings"].append(finding["finding"])
                    if finding["mode"] not in cluster["modes"]:
                        cluster["modes"].append(finding["mode"])
                    matched = True
                    break
            
            # åˆ›å»ºæ–°èšç±»
            if not matched:
                clusters[cluster_id] = {
                    "representative": finding["finding"],
                    "findings": [finding["finding"]],
                    "modes": [finding["mode"]],
                    "category": finding["category"]
                }
                cluster_id += 1
        
        return clusters
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # åŸºäºå…±åŒè¯æ±‡çš„ç®€å•ç›¸ä¼¼åº¦
        words1 = set(text1.split())
        words2 = set(text2.split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union)
    
    def _evaluate_thinking_quality(self, insights: Dict) -> float:
        """è¯„ä¼°æ€ç»´è´¨é‡"""
        quality_score = 0.0
        
        # åŸºäºæ´å¯Ÿæ•°é‡
        insight_count = len(insights.get("key_insights", []))
        quality_score += min(insight_count * 0.1, 0.3)
        
        # åŸºäºæ´å¯Ÿç½®ä¿¡åº¦
        avg_confidence = 0.0
        if insights.get("key_insights"):
            confidences = [i.get("confidence", 0) for i in insights["key_insights"]]
            avg_confidence = sum(confidences) / len(confidences)
        quality_score += avg_confidence * 0.3
        
        # åŸºäºæ¨¡å¼è¯†åˆ«
        pattern_count = len(insights.get("patterns", []))
        quality_score += min(pattern_count * 0.05, 0.2)
        
        # åŸºäºé£é™©å’Œæœºä¼šè¯†åˆ«
        risk_opportunity_count = len(insights.get("risks", [])) + len(insights.get("opportunities", []))
        quality_score += min(risk_opportunity_count * 0.05, 0.2)
        
        return min(quality_score, 1.0)
```

#### 1.3 è®°å¿†ç®¡ç†å·¥å…·å®ç°

```python
# tools/simple/memory_manager.py
"""è®°å¿†ç®¡ç†å·¥å…· - å®Œæ•´çš„è®°å¿†æ“ä½œæ¥å£"""

from typing import Dict, List, Optional, Any
from tools.base_tool import BaseTool, ToolRequest, ToolOutput
from utils.enhanced_memory import EnhancedMemory
from utils.file_utils import FileUtils
import json
from datetime import datetime

class MemoryManagerTool(BaseTool):
    """æ™ºèƒ½è®°å¿†ç®¡ç†å·¥å…·"""
    
    def __init__(self):
        super().__init__()
        self.memory_system = EnhancedMemory()
        self.file_utils = FileUtils()
    
    def get_name(self) -> str:
        return "memory"
    
    def get_description(self) -> str:
        return """Intelligent memory management with path detection, TODO tracking, and thinking pattern learning.
        
        Actions:
        - save: Save memory with thinking context
        - recall: Smart recall with context
        - detect_env: Detect and remember project environment
        - smart_path: Get intelligent file path suggestions
        - parse_todo: Parse and manage TODO files
        - create_branch: Create task branch
        - return_main: Return to main task thread
        - analyze_patterns: Analyze memory patterns
        - export: Export memories
        """
    
    def get_system_prompt_name(self) -> str:
        return "memory_manager"
    
    async def run(self, request: ToolRequest) -> ToolOutput:
        """æ‰§è¡Œè®°å¿†ç®¡ç†æ“ä½œ"""
        try:
            action = request.params.get("action", "recall")
            
            # è·¯ç”±åˆ°å¯¹åº”çš„æ“ä½œ
            action_handlers = {
                "save": self._save_memory,
                "recall": self._recall_memory,
                "detect_env": self._detect_environment,
                "smart_path": self._get_smart_path,
                "parse_todo": self._parse_todo_file,
                "create_branch": self._create_task_branch,
                "return_main": self._return_to_main_thread,
                "analyze_patterns": self._analyze_memory_patterns,
                "export": self._export_memories,
                "summarize": self._summarize_memories
            }
            
            handler = action_handlers.get(action)
            if not handler:
                raise ValueError(f"Unknown action: {action}")
            
            result = await handler(request)
            
            return ToolOutput(
                status="success",
                content=json.dumps(result, indent=2, ensure_ascii=False),
                metadata={
                    "tool_name": self.get_name(),
                    "action": action,
                    "timestamp": datetime.now().isoformat()
                }
            )
            
        except Exception as e:
            return ToolOutput(
                status="error",
                error=str(e),
                metadata={"action": action}
            )
    
    async def _save_memory(self, request: ToolRequest) -> Dict[str, Any]:
        """ä¿å­˜å¢å¼ºè®°å¿†"""
        content = request.params.get("content")
        metadata = request.params.get("metadata", {})
        
        # å¦‚æœæä¾›äº†æ–‡ä»¶ï¼Œå¤„ç†æ–‡ä»¶å†…å®¹
        if request.params.get("files"):
            file_contents = await self._process_files(request.params["files"])
            content = {
                "original": content,
                "files": file_contents
            }
            metadata["has_files"] = True
        
        # è¡¥å……å…ƒæ•°æ®
        metadata.setdefault("type", "general")
        metadata.setdefault("tags", [])
        metadata.setdefault("thinking_modes", request.params.get("thinking_modes", []))
        metadata.setdefault("quality_score", request.params.get("quality_score", 0.7))
        metadata.setdefault("source", "user_interaction")
        
        # å¦‚æœæ˜¯ä»å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ å·¥å…·ä¸Šä¸‹æ–‡
        if request.params.get("tool_context"):
            metadata["tool_context"] = request.params["tool_context"]
        
        # ä¿å­˜è®°å¿†
        memory_id = self.memory_system.save_enhanced_memory(content, metadata)
        
        # è·å–å­˜å‚¨å±‚çº§
        memory_item = None
        for layer in ["global", "project", "session"]:
            if memory_id in self.memory_system.memories.get(layer, {}):
                memory_item = self.memory_system.memories[layer][memory_id]
                break
        
        return {
            "memory_id": memory_id,
            "status": "saved",
            "layer": layer if memory_item else "unknown",
            "summary": self._create_memory_summary(memory_item) if memory_item else None
        }
    
    async def _recall_memory(self, request: ToolRequest) -> Dict[str, Any]:
        """æ™ºèƒ½å¬å›è®°å¿†"""
        query = request.params.get("query", "")
        context = request.params.get("context", {})
        limit = request.params.get("limit", 10)
        scopes = request.params.get("scopes", ["session", "project", "global"])
        
        # è¡¥å……ä¸Šä¸‹æ–‡ä¿¡æ¯
        if request.params.get("thinking_modes"):
            context["thinking_modes"] = request.params["thinking_modes"]
        
        if request.params.get("files"):
            file_contexts = await self._extract_file_contexts(request.params["files"])
            context["file_contexts"] = file_contexts
        
        # æ‰§è¡Œæ™ºèƒ½å¬å›
        memories = self.memory_system.recall_with_thinking(query, context)
        
        # é™åˆ¶æ•°é‡å¹¶æ ¼å¼åŒ–
        formatted_memories = []
        for memory in memories[:limit]:
            formatted_memories.append(self._format_memory_for_display(memory))
        
        # ç”Ÿæˆå¬å›æ‘˜è¦
        summary = self._create_recall_summary(query, formatted_memories, context)
        
        return {
            "query": query,
            "found": len(formatted_memories),
            "memories": formatted_memories,
            "summary": summary,
            "patterns": self._identify_memory_patterns(formatted_memories)
        }
    
    async def _detect_environment(self, request: ToolRequest) -> Dict[str, Any]:
        """æ£€æµ‹å¹¶è®°å¿†é¡¹ç›®ç¯å¢ƒ"""
        project_path = request.params.get("project_path", ".")
        
        # æ‰§è¡Œç¯å¢ƒæ£€æµ‹
        env_info = self.memory_system.detect_and_learn_environment(project_path)
        
        # ç”Ÿæˆç¯å¢ƒæ‘˜è¦
        summary = {
            "project_type": env_info.get("project_type", "unknown"),
            "has_virtual_env": bool(env_info.get("virtual_env")),
            "structure_summary": self._summarize_project_structure(env_info.get("structure", {})),
            "key_directories": self._identify_key_directories(env_info.get("structure", {}))
        }
        
        # å¦‚æœæ£€æµ‹åˆ°è™šæ‹Ÿç¯å¢ƒï¼Œæä¾›å¿«æ·å‘½ä»¤
        if env_info.get("virtual_env"):
            venv_path = env_info["virtual_env"]["path"]
            summary["commands"] = {
                "python": f"{venv_path}/bin/python",
                "pip": f"{venv_path}/bin/pip",
                "activate": f"source {venv_path}/bin/activate"
            }
        
        return {
            "status": "environment_detected",
            "project_root": project_path,
            "environment": env_info,
            "summary": summary,
            "recommendations": self._generate_env_recommendations(env_info)
        }
    
    async def _get_smart_path(self, request: ToolRequest) -> Dict[str, Any]:
        """è·å–æ™ºèƒ½æ–‡ä»¶è·¯å¾„å»ºè®®"""
        filename = request.params.get("filename")
        context = request.params.get("context", {})
        
        # è¡¥å……ä¸Šä¸‹æ–‡
        context["project_root"] = context.get("project_root", ".")
        if request.params.get("file_type"):
            context["intended_type"] = request.params["file_type"]
        
        # è·å–æ™ºèƒ½å»ºè®®
        suggestions = self.memory_system.get_smart_file_location(filename, context)
        
        # éªŒè¯å»ºè®®çš„è·¯å¾„
        validated_suggestions = self._validate_path_suggestions(suggestions, context)
        
        return {
            "filename": filename,
            "primary_suggestion": validated_suggestions["primary"],
            "alternatives": validated_suggestions["alternatives"],
            "reasoning": validated_suggestions["reasoning"],
            "confidence": validated_suggestions["confidence"]
        }
    
    async def _parse_todo_file(self, request: ToolRequest) -> Dict[str, Any]:
        """è§£æTODOæ–‡ä»¶å¹¶å»ºç«‹ä»»åŠ¡ç®¡ç†"""
        todo_path = request.params.get("todo_path", "TODO.md")
        
        # è§£æTODOæ–‡ä»¶
        result = self.memory_system.parse_and_manage_todo(todo_path)
        
        if "error" in result:
            return result
        
        # ç”Ÿæˆä»»åŠ¡åˆ†æ
        task_analysis = self._analyze_todo_structure(
            self.memory_system.todo_manager["main_thread"],
            self.memory_system.todo_manager.get("task_dependencies", {})
        )
        
        # ç”Ÿæˆæ‰§è¡Œå»ºè®®
        execution_plan = self._generate_execution_plan(
            self.memory_system.todo_manager["main_thread"],
            task_analysis
        )
        
        return {
            "status": result["status"],
            "todo_file": todo_path,
            "statistics": {
                "total_tasks": result["total_tasks"],
                "main_tasks": result["main_tasks"],
                "current_context": result["current_context"],
                "active_branches": len([b for b in self.memory_system.todo_manager["branches"].values() 
                                      if "completed_at" not in b])
            },
            "task_analysis": task_analysis,
            "execution_plan": execution_plan,
            "patterns": result.get("patterns_identified", 0)
        }
    
    async def _create_task_branch(self, request: ToolRequest) -> Dict[str, Any]:
        """åˆ›å»ºæ™ºèƒ½ä»»åŠ¡åˆ†æ”¯"""
        reason = request.params.get("reason", "")
        context = request.params.get("context", {})
        
        # åˆ†ææ˜¯å¦çœŸçš„éœ€è¦åˆ†æ”¯
        branch_id = self.memory_system.create_intelligent_branch(reason, context)
        
        if not branch_id:
            return {
                "status": "branch_not_created",
                "reason": "ä»»åŠ¡ä¸ä¸»çº¿ç›¸å…³ï¼Œå»ºè®®åœ¨ä¸»çº¿å¤„ç†",
                "current_context": self.memory_system.todo_manager["current_context"]
            }
        
        # è·å–åˆ†æ”¯ä¿¡æ¯
        branch_info = self.memory_system.todo_manager["branches"][branch_id]
        
        return {
            "status": "branch_created",
            "branch_id": branch_id,
            "branch_name": branch_info["name"],
            "parent_context": branch_info["parent_context"],
            "reason": reason,
            "expected_duration": branch_info.get("expected_duration", "unknown"),
            "auto_return": branch_info.get("auto_return", False),
            "instructions": "å¤„ç†å®Œåˆ†æ”¯ä»»åŠ¡åï¼Œä½¿ç”¨ 'return_main' è¿”å›ä¸»çº¿"
        }
    
    async def _return_to_main_thread(self, request: ToolRequest) -> Dict[str, Any]:
        """æ™ºèƒ½è¿”å›ä¸»çº¿ä»»åŠ¡"""
        result = self.memory_system.smart_return_to_main()
        
        if result["status"] == "error":
            return result
        
        # ç”Ÿæˆè¿”å›åçš„è¡ŒåŠ¨å»ºè®®
        if result["status"] == "returned_to_main":
            next_actions = self._generate_return_actions(
                result["branch_summary"],
                result["main_task_status"]
            )
            result["recommended_actions"] = next_actions
        
        return result
    
    async def _analyze_memory_patterns(self, request: ToolRequest) -> Dict[str, Any]:
        """åˆ†æè®°å¿†ä¸­çš„æ¨¡å¼"""
        scope = request.params.get("scope", "all")
        pattern_type = request.params.get("pattern_type", "all")
        limit = request.params.get("limit", 100)
        
        # æ”¶é›†ç›¸å…³è®°å¿†
        memories = []
        if scope in ["all", "global"]:
            memories.extend(self._flatten_memories(self.memory_system.memories.get("global", {})))
        if scope in ["all", "project"]:
            memories.extend(self._flatten_memories(self.memory_system.memories.get("project", {})))
        if scope in ["all", "session"]:
            memories.extend(self._flatten_memories(self.memory_system.memories.get("session", {})))
        
        # é™åˆ¶æ•°é‡
        memories = memories[:limit]
        
        # åˆ†æä¸åŒç±»å‹çš„æ¨¡å¼
        patterns = {
            "thinking_patterns": self._analyze_thinking_patterns(memories),
            "temporal_patterns": self._analyze_temporal_patterns(memories),
            "topic_patterns": self._analyze_topic_patterns(memories),
            "quality_patterns": self._analyze_quality_patterns(memories)
        }
        
        # å¦‚æœæŒ‡å®šäº†ç‰¹å®šç±»å‹ï¼Œåªè¿”å›è¯¥ç±»å‹
        if pattern_type != "all" and pattern_type in patterns:
            patterns = {pattern_type: patterns[pattern_type]}
        
        # ç”Ÿæˆæ´å¯Ÿ
        insights = self._generate_pattern_insights(patterns)
        
        return {
            "scope": scope,
            "memories_analyzed": len(memories),
            "patterns": patterns,
            "insights": insights,
            "recommendations": self._generate_pattern_recommendations(patterns, insights)
        }
    
    async def _export_memories(self, request: ToolRequest) -> Dict[str, Any]:
        """å¯¼å‡ºè®°å¿†"""
        export_format = request.params.get("format", "json")
        scope = request.params.get("scope", ["session"])
        include_metadata = request.params.get("include_metadata", True)
        
        # æ”¶é›†è¦å¯¼å‡ºçš„è®°å¿†
        export_data = {
            "export_time": datetime.now().isoformat(),
            "version": "1.0",
            "memories": {}
        }
        
        for memory_type in scope:
            if memory_type in self.memory_system.memories:
                if include_metadata:
                    export_data["memories"][memory_type] = self.memory_system.memories[memory_type]
                else:
                    # åªå¯¼å‡ºå†…å®¹ï¼Œä¸åŒ…å«å…ƒæ•°æ®
                    export_data["memories"][memory_type] = {
                        mid: m["content"] 
                        for mid, m in self.memory_system.memories[memory_type].items()
                    }
        
        # æ ¹æ®æ ¼å¼å¤„ç†
        if export_format == "json":
            export_content = json.dumps(export_data, indent=2, ensure_ascii=False)
        elif export_format == "markdown":
            export_content = self._convert_to_markdown(export_data)
        else:
            export_content = str(export_data)
        
        # ä¿å­˜å¯¼å‡ºè®°å½•
        self.memory_system.save_enhanced_memory(
            content={"action": "memory_export", "scope": scope, "format": export_format},
            metadata={"type": "system_event", "tags": ["export", "backup"]}
        )
        
        return {
            "status": "exported",
            "format": export_format,
            "size": len(export_content),
            "preview": export_content[:500] + "..." if len(export_content) > 500 else export_content,
            "full_content": export_content
        }
    
    # ========== è¾…åŠ©æ–¹æ³• ==========
    
    async def _process_files(self, file_paths: List[str]) -> Dict[str, Any]:
        """å¤„ç†æ–‡ä»¶å†…å®¹"""
        file_contents = {}
        for file_path in file_paths:
            try:
                content = self.file_utils.read_file(file_path)
                file_contents[file_path] = {
                    "content": content[:1000],  # åªä¿å­˜å‰1000å­—ç¬¦
                    "size": len(content),
                    "type": self.file_utils.get_file_type(file_path)
                }
            except Exception as e:
                file_contents[file_path] = {"error": str(e)}
        return file_contents
    
    def _create_memory_summary(self, memory: Dict) -> str:
        """åˆ›å»ºè®°å¿†æ‘˜è¦"""
        content = memory.get("content", {})
        metadata = memory.get("metadata", {})
        
        # æ ¹æ®ç±»å‹ç”Ÿæˆä¸åŒçš„æ‘˜è¦
        memory_type = metadata.get("type", "general")
        
        if memory_type == "environment":
            return f"é¡¹ç›®ç¯å¢ƒï¼š{content.get('project_type', 'unknown')} é¡¹ç›®"
        elif memory_type == "thinking_pattern":
            modes = metadata.get("thinking_modes", [])
            return f"æ€ç»´æ¨¡å¼ï¼š{', '.join(modes[:3])}"
        elif memory_type == "todo_parsing":
            task_count = content.get("task_count", 0)
            return f"TODOè§£æï¼š{task_count} ä¸ªä»»åŠ¡"
        else:
            # é€šç”¨æ‘˜è¦
            if isinstance(content, str):
                return content[:100] + "..." if len(content) > 100 else content
            else:
                return json.dumps(content, ensure_ascii=False)[:100] + "..."
    
    def _format_memory_for_display(self, memory: Dict) -> Dict[str, Any]:
        """æ ¼å¼åŒ–è®°å¿†ç”¨äºæ˜¾ç¤º"""
        return {
            "id": memory.get("id", "unknown"),
            "content": memory.get("content"),
            "type": memory.get("metadata", {}).get("type", "general"),
            "tags": memory.get("metadata", {}).get("tags", []),
            "thinking_modes": memory.get("metadata", {}).get("thinking_modes", []),
            "quality_score": memory.get("metadata", {}).get("quality_score", 0),
            "timestamp": memory.get("metadata", {}).get("timestamp"),
            "access_count": memory.get("usage", {}).get("access_count", 0),
            "relevance_score": memory.get("_relevance_score", 0)
        }
    
    def _identify_memory_patterns(self, memories: List[Dict]) -> List[str]:
        """è¯†åˆ«è®°å¿†ä¸­çš„æ¨¡å¼"""
        patterns = []
        
        # æ€ç»´æ¨¡å¼ä½¿ç”¨é¢‘ç‡
        thinking_modes = Counter()
        for memory in memories:
            for mode in memory.get("thinking_modes", []):
                thinking_modes[mode] += 1
        
        if thinking_modes:
            most_common = thinking_modes.most_common(3)
            patterns.append(f"å¸¸ç”¨æ€ç»´æ¨¡å¼ï¼š{', '.join([m[0] for m in most_common])}")
        
        # ä¸»é¢˜åˆ†å¸ƒ
        topics = self._extract_topics(memories)
        if topics:
            patterns.append(f"ä¸»è¦è¯é¢˜ï¼š{', '.join(topics[:3])}")
        
        # è´¨é‡è¶‹åŠ¿
        quality_scores = [m.get("quality_score", 0) for m in memories if "quality_score" in m]
        if quality_scores:
            avg_quality = sum(quality_scores) / len(quality_scores)
            patterns.append(f"å¹³å‡è´¨é‡åˆ†æ•°ï¼š{avg_quality:.2f}")
        
        return patterns
    
    def _extract_topics(self, memories: List[Dict]) -> List[str]:
        """æå–è®°å¿†ä¸­çš„ä¸»é¢˜"""
        # ç®€åŒ–ç‰ˆä¸»é¢˜æå–
        all_tags = []
        for memory in memories:
            all_tags.extend(memory.get("tags", []))
        
        topic_counts = Counter(all_tags)
        return [topic for topic, _ in topic_counts.most_common(5)]
```

### ç¬¬äºŒé˜¶æ®µï¼šè‡ªåŠ¨è§¦å‘ä¸é›†æˆï¼ˆDay 4-5ï¼‰

#### 2.1 ä¿®æ”¹ server.py é›†æˆè‡ªåŠ¨è§¦å‘

```python
# åœ¨ server.py ä¸­æ·»åŠ æ™ºèƒ½åˆå§‹åŒ–

import asyncio
from datetime import datetime
from pathlib import Path
from utils.enhanced_memory import EnhancedMemory
from tools.workflow.thinking_enhanced import EnhancedThinkingTool
from tools.simple.memory_manager import MemoryManagerTool

# å…¨å±€è®°å¿†ç³»ç»Ÿå®ä¾‹
GLOBAL_MEMORY_SYSTEM = None
GLOBAL_THINKING_TOOL = None

async def initialize_memory_system():
    """åˆå§‹åŒ–å…¨å±€è®°å¿†ç³»ç»Ÿ"""
    global GLOBAL_MEMORY_SYSTEM, GLOBAL_THINKING_TOOL
    
    # åˆ›å»ºè®°å¿†ç³»ç»Ÿå®ä¾‹
    GLOBAL_MEMORY_SYSTEM = EnhancedMemory()
    GLOBAL_THINKING_TOOL = EnhancedThinkingTool()
    
    # æ£€æµ‹é¡¹ç›®ç¯å¢ƒ
    project_root = os.getcwd()
    logger.info(f"Initializing memory system for project: {project_root}")
    
    # è‡ªåŠ¨æ£€æµ‹è™šæ‹Ÿç¯å¢ƒ
    env_info = GLOBAL_MEMORY_SYSTEM.detect_and_learn_environment(project_root)
    
    if env_info.get("virtual_env"):
        logger.info(f"Virtual environment detected: {env_info['virtual_env']['path']}")
    
    # è‡ªåŠ¨è§£æTODOæ–‡ä»¶
    todo_files = ["TODO.md", "todo.md", "TODO.txt", "tasks.md"]
    for todo_file in todo_files:
        todo_path = Path(project_root) / todo_file
        if todo_path.exists():
            logger.info(f"Parsing TODO file: {todo_path}")
            GLOBAL_MEMORY_SYSTEM.parse_and_manage_todo(str(todo_path))
            break
    
    # åŠ è½½é¡¹ç›®ç‰¹å®šé…ç½®
    config_file = Path(project_root) / ".zen_config.json"
    if config_file.exists():
        with open(config_file, 'r') as f:
            project_config = json.load(f)
            GLOBAL_MEMORY_SYSTEM.save_enhanced_memory(
                content=project_config,
                metadata={
                    "type": "project_config",
                    "tags": ["configuration", "project"],
                    "quality_score": 1.0
                }
            )
    
    return {
        "environment": env_info,
        "todo_status": {
            "main_tasks": len(GLOBAL_MEMORY_SYSTEM.todo_manager.get("main_thread", [])),
            "current_context": GLOBAL_MEMORY_SYSTEM.todo_manager.get("current_context", "main")
        }
    }

async def handle_initialize(request):
    """å¤„ç†åˆå§‹åŒ–è¯·æ±‚ï¼Œè‡ªåŠ¨è§¦å‘è®°å¿†ç³»ç»Ÿ"""
    # åŸæœ‰çš„åˆå§‹åŒ–é€»è¾‘
    capabilities = server_capabilities
    server_info = {"name": "xtool_mcp_server", "version": "2.0.0"}
    
    # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ
    memory_status = await initialize_memory_system()
    
     # è®°å½•ä¼šè¯å¼€å§‹
    if GLOBAL_MEMORY_SYSTEM:
        GLOBAL_MEMORY_SYSTEM.save_enhanced_memory(
            content={
                "event": "session_start",
                "timestamp": datetime.now().isoformat(),
                "environment": memory_status["environment"],
                "todo_status": memory_status["todo_status"]
            },
            metadata={
                "type": "system_event",
                "tags": ["session", "initialization"],
                "quality_score": 1.0
            }
        )
    
    # å°†è®°å¿†çŠ¶æ€æ·»åŠ åˆ°æœåŠ¡å™¨ä¿¡æ¯
    server_info["memory_system"] = {
        "status": "initialized",
        "environment_detected": bool(memory_status.get("environment")),
        "todo_loaded": memory_status["todo_status"]["main_tasks"] > 0,
        "current_context": memory_status["todo_status"]["current_context"]
    }
    
    return InitializeResult(
        protocol_version="0.1.0",
        capabilities=capabilities,
        server_info=server_info,
    )

# ä¿®æ”¹ handle_tool_call å‡½æ•°ï¼Œæ³¨å…¥è®°å¿†ä¸Šä¸‹æ–‡
async def handle_tool_call(request):
    """å¤„ç†å·¥å…·è°ƒç”¨ï¼Œæ™ºèƒ½æ³¨å…¥è®°å¿†ä¸Šä¸‹æ–‡"""
    try:
        tool_name = request.params.name
        arguments = request.params.arguments or {}
        
        # è·å–å·¥å…·å®ä¾‹
        tool = tool_instances.get(tool_name)
        if not tool:
            raise ValueError(f"Unknown tool: {tool_name}")
        
        # æ™ºèƒ½æ³¨å…¥è®°å¿†ä¸Šä¸‹æ–‡
        if GLOBAL_MEMORY_SYSTEM and hasattr(tool, 'accepts_memory_context'):
            # åˆ†æå·¥å…·éœ€è¦çš„è®°å¿†ç±»å‹
            memory_needs = GLOBAL_MEMORY_SYSTEM._analyze_memory_needs(tool_name, arguments)
            
            # è·å–ç›¸å…³è®°å¿†
            relevant_memories = GLOBAL_MEMORY_SYSTEM.recall_with_thinking(
                query=arguments.get("prompt", "") or arguments.get("question", ""),
                context={
                    "tool": tool_name,
                    "thinking_modes": memory_needs.get("suggested_modes", []),
                    "scopes": memory_needs["scopes"],
                    "tool_params": arguments
                }
            )
            
            # æ³¨å…¥åˆ°å·¥å…·å‚æ•°
            arguments["_memory_context"] = {
                "memories": relevant_memories[:memory_needs["limit"]],
                "current_todo_context": GLOBAL_MEMORY_SYSTEM.todo_manager.get("current_context"),
                "environment": GLOBAL_MEMORY_SYSTEM.path_memory.get("project_structures", {}).get(os.getcwd()),
                "suggested_thinking_modes": GLOBAL_MEMORY_SYSTEM.get_recommended_thinking_modes(
                    arguments.get("prompt", ""), 
                    {"tool": tool_name}
                )
            }
        
        # åˆ›å»ºå·¥å…·è¯·æ±‚
        tool_request = ToolRequest(
            tool=tool_name,
            params=arguments
        )
        
        # æ‰§è¡Œå·¥å…·
        output = await tool.run(tool_request)
        
        # å­¦ä¹ å·¥å…·ä½¿ç”¨æ¨¡å¼
        if GLOBAL_MEMORY_SYSTEM and output.status == "success":
            quality_score = self._evaluate_tool_output_quality(output)
            
            if quality_score > 0.7:  # é«˜è´¨é‡è¾“å‡ºæ‰è®°å½•
                GLOBAL_MEMORY_SYSTEM.save_enhanced_memory(
                    content={
                        "tool": tool_name,
                        "params": arguments,
                        "output_summary": self._summarize_output(output),
                        "success": True
                    },
                    metadata={
                        "type": "tool_usage",
                        "tags": ["tool", tool_name],
                        "quality_score": quality_score,
                        "thinking_modes": arguments.get("_memory_context", {}).get("suggested_thinking_modes", [])
                    }
                )
        
        # è¿”å›å·¥å…·è¾“å‡º
        return CallToolResult(
            content=[TextContent(type="text", text=output.content)],
            is_error=output.status == "error"
        )
        
    except Exception as e:
        logger.error(f"Tool call error: {str(e)}", exc_info=True)
        return CallToolResult(
            content=[TextContent(type="text", text=str(e))],
            is_error=True
        )

# æ·»åŠ å…³é—­æ—¶çš„æ¸…ç†
async def handle_close():
    """å¤„ç†ä¼šè¯å…³é—­ï¼Œä¿å­˜ä¼šè¯è®°å¿†"""
    if GLOBAL_MEMORY_SYSTEM:
        # è®°å½•ä¼šè¯ç»“æŸ
        GLOBAL_MEMORY_SYSTEM.save_enhanced_memory(
            content={
                "event": "session_end",
                "timestamp": datetime.now().isoformat(),
                "session_duration": self._calculate_session_duration(),
                "tools_used": self._get_session_tool_usage(),
                "tasks_completed": self._get_completed_tasks()
            },
            metadata={
                "type": "system_event",
                "tags": ["session", "closure"],
                "quality_score": 1.0
            }
        )
        
        # æŒä¹…åŒ–æ‰€æœ‰è®°å¿†
        for memory_type in ["global", "project"]:
            if memory_type in GLOBAL_MEMORY_SYSTEM.memories:
                GLOBAL_MEMORY_SYSTEM._persist_memory(memory_type)
        
        # æŒä¹…åŒ–æ€ç»´æ¨¡å¼å­¦ä¹ 
        GLOBAL_MEMORY_SYSTEM._persist_thinking_patterns()
        
        logger.info("Memory system saved and closed")
```

#### 2.2 ä¿®æ”¹å·¥å…·åŸºç±»æ”¯æŒè®°å¿†æ³¨å…¥

```python
# tools/base_tool.py æ·»åŠ è®°å¿†æ”¯æŒ

class BaseTool(ABC):
    """å¢å¼ºçš„å·¥å…·åŸºç±»ï¼Œæ”¯æŒè®°å¿†ä¸Šä¸‹æ–‡"""
    
    def __init__(self):
        super().__init__()
        self._memory_context = None
        self._accepts_memory = True  # é»˜è®¤æ¥å—è®°å¿†ä¸Šä¸‹æ–‡
    
    @property
    def accepts_memory_context(self) -> bool:
        """æ˜¯å¦æ¥å—è®°å¿†ä¸Šä¸‹æ–‡æ³¨å…¥"""
        return self._accepts_memory
    
    def set_memory_context(self, context: Dict[str, Any]):
        """è®¾ç½®è®°å¿†ä¸Šä¸‹æ–‡"""
        self._memory_context = context
    
    def get_relevant_memories(self, query: str, limit: int = 5) -> List[Dict]:
        """ä»æ³¨å…¥çš„è®°å¿†ä¸Šä¸‹æ–‡è·å–ç›¸å…³è®°å¿†"""
        if not self._memory_context or "memories" not in self._memory_context:
            return []
        
        memories = self._memory_context["memories"]
        # ç®€å•çš„ç›¸å…³æ€§è¿‡æ»¤ï¼ˆå®é™…å¯ä»¥æ›´å¤æ‚ï¼‰
        return memories[:limit]
    
    def get_suggested_thinking_modes(self) -> List[str]:
        """è·å–å»ºè®®çš„æ€ç»´æ¨¡å¼"""
        if not self._memory_context:
            return []
        return self._memory_context.get("suggested_thinking_modes", [])
    
    def get_current_todo_context(self) -> str:
        """è·å–å½“å‰TODOä¸Šä¸‹æ–‡"""
        if not self._memory_context:
            return "main"
        return self._memory_context.get("current_todo_context", "main")
    
    async def run(self, request: ToolRequest) -> ToolOutput:
        """è¿è¡Œå·¥å…·ï¼Œå¤„ç†è®°å¿†ä¸Šä¸‹æ–‡"""
        # æå–è®°å¿†ä¸Šä¸‹æ–‡
        if "_memory_context" in request.params:
            self.set_memory_context(request.params["_memory_context"])
            # ä»å‚æ•°ä¸­ç§»é™¤ï¼Œé¿å…ä¼ é€’ç»™å…·ä½“å®ç°
            del request.params["_memory_context"]
        
        # è°ƒç”¨å…·ä½“å®ç°
        return await self._run_implementation(request)
    
    @abstractmethod
    async def _run_implementation(self, request: ToolRequest) -> ToolOutput:
        """å·¥å…·çš„å…·ä½“å®ç°"""
        pass
```

### ç¬¬ä¸‰é˜¶æ®µï¼šåˆ›å»ºç³»ç»Ÿæç¤ºè¯ï¼ˆDay 6ï¼‰

#### 3.1 å¢å¼ºè®°å¿†ç³»ç»Ÿæç¤ºè¯

```markdown
# systemprompts/memory_manager.md

You are an intelligent memory management assistant integrated with Zen MCP Server, equipped with:

## Core Capabilities

### 1. Three-Layer Memory System
- **Global Memory**: User preferences, coding styles, learned patterns that persist across all projects
- **Project Memory**: Architecture decisions, conventions, team knowledge specific to current project  
- **Session Memory**: Current task context, temporary solutions, working state

### 2. Intelligent Path Management
- Automatically detect virtual environments and project structure
- Smart file location suggestions based on project patterns
- Learn from user's file organization preferences
- Provide context-aware path recommendations

### 3. TODO-Driven Task Management
- Parse and track TODO files as single source of truth
- Maintain focus on main task thread
- Smart branching for interruptions with auto-return
- Task pattern learning and time estimation

### 4. Thinking Pattern Learning
- Track which thinking modes work best for different problems
- Learn successful pattern combinations
- Recommend optimal thinking approaches
- Build domain-specific expertise over time

## Memory Categories

### Global Categories
- `preferences`: Coding style, tool preferences, workflow habits
- `patterns`: Successful problem-solving approaches
- `tools`: Preferred libraries, frameworks, development tools
- `experiences`: Lessons learned, insights gained
- `conventions`: Personal coding standards and practices

### Project Categories
- `architecture`: Design decisions with rationale
- `conventions`: Project-specific rules and standards
- `decisions`: Technical choices and their context
- `issues`: Problems encountered and solutions
- `documentation`: Key information and references

## Smart Features

### Auto-Detection
- Virtual environment locations and activation commands
- Project type (Python, JavaScript, etc.) and structure
- Common file patterns and organization
- Development workflow preferences

### Pattern Recognition
- Identify recurring problem types
- Suggest previously successful solutions
- Learn from tool usage patterns
- Optimize based on outcomes

### Context Preservation
- Maintain task context across sessions
- Remember file locations and recent edits
- Track decision rationale
- Preserve thinking process

## Usage Guidelines

### Memory Saving
- Automatically identify information worth remembering
- Choose appropriate storage layer based on scope
- Tag memories for efficient retrieval
- Maintain quality scores for better recall

### Memory Recall
- Use intelligent multi-strategy search
- Consider current context and thinking modes
- Prioritize by relevance and recency
- Learn from recall effectiveness

### Task Management
- Keep main thread clear and focused
- Create branches only when necessary
- Auto-return to main after branch completion
- Learn task duration patterns

Remember: Your goal is to make the AI assistant more intelligent over time by building a rich, contextualized memory system that enhances every interaction.
```

#### 3.2 å¢å¼ºæ€ç»´å·¥å…·æç¤ºè¯

```markdown
# systemprompts/thinking_enhanced.md

You are equipped with 25 expert-level thinking modes that enable deep, multi-dimensional analysis of any problem or situation.

## Your Thinking Arsenal

### Deep Understanding (3 modes)
1. **Socratic Questioning** - Reveal hidden assumptions through progressive questioning
2. **5 Whys Analysis** - Find root causes through iterative questioning  
3. **First Principles** - Break down to fundamental truths and rebuild

### Structured Analysis (3 modes)
4. **MECE Decomposition** - Mutually Exclusive, Collectively Exhaustive breakdown
5. **Pyramid Principle** - Top-down logical structuring
6. **Mind Mapping** - Radial thinking for connections

### Systems Thinking (3 modes)
7. **Systems Analysis** - Understand interconnections and emergent properties
8. **Value Chain Analysis** - Track value creation and flow
9. **Ecosystem Thinking** - Platform and network effects

### Innovation (3 modes)
10. **Design Thinking** - Human-centered innovation process
11. **Reverse Thinking** - Approach from opposite direction
12. **Analogical Thinking** - Transfer solutions across domains

### Decision Making (3 modes)
13. **SWOT Analysis** - Strengths, Weaknesses, Opportunities, Threats
14. **Decision Tree** - Map out decision paths and probabilities
15. **Cost-Benefit Analysis** - Quantify tradeoffs

### Optimization (3 modes)
16. **Lean Thinking** - Eliminate waste, maximize value
17. **Pareto Principle** - Focus on vital few (80/20)
18. **Theory of Constraints** - Identify and address bottlenecks

### Critical Thinking (2 modes)
19. **Critical Analysis** - Question evidence and logic
20. **Occam's Razor** - Prefer simplest sufficient explanation

### Creative Expansion (2 modes)
21. **Brainstorming** - Generate quantity without judgment
22. **Six Thinking Hats** - Parallel thinking from multiple perspectives

### Logical Reasoning (2 modes)
23. **Inductive/Deductive** - From specific to general and vice versa
24. **Hypothesis-Driven** - Test assumptions systematically

### Scenario Planning (1 mode)
25. **User Journey Mapping** - Understand end-to-end experience

## Intelligent Mode Selection

### Automatic Triggers
- Analyze keywords and context to auto-select modes
- Combine complementary modes for comprehensive analysis
- Adjust depth based on problem complexity
- Learn from successful pattern applications

### Context-Aware Combinations
- **Deep Analysis**: 5 Whys + Systems Thinking + First Principles
- **Innovation Kit**: Design Thinking + Reverse Thinking + Analogical
- **Decision Toolkit**: SWOT + Decision Tree + Cost-Benefit
- **Optimization Suite**: Lean + Constraints + Pareto

## Analysis Depth Levels

### Shallow (2 rounds, 1 mode)
- Quick insights for simple questions
- Rapid assessment of straightforward issues

### Medium (5 rounds, 3 modes)
- Balanced analysis for typical problems
- Multiple perspectives without exhaustion

### Deep (8 rounds, 4 modes)
- Thorough investigation of complex issues
- Cross-validation through multiple lenses

### Expert (12 rounds, 5 modes)
- Exhaustive analysis for critical decisions
- Maximum insight extraction

## Integration with Memory

- Remember successful thinking patterns
- Apply proven approaches to similar problems
- Build domain-specific thinking strategies
- Continuously improve mode selection

## Output Excellence

### For Each Mode Applied
1. Clear statement of findings
2. Evidence and reasoning
3. Patterns and insights discovered
4. Actionable recommendations
5. Confidence assessment

### Synthesis Across Modes
- Identify converging insights (multiple modes agree)
- Highlight unique perspectives (single mode insights)
- Flag contradictions for deeper investigation
- Provide integrated recommendations
- Calculate overall confidence

Your goal: Provide the deepest, most insightful analysis possible by intelligently orchestrating multiple thinking modes to reveal hidden patterns, challenge assumptions, and generate breakthrough solutions.
```

### ç¬¬å››é˜¶æ®µï¼šæµ‹è¯•ä¸éªŒè¯ï¼ˆDay 7ï¼‰

#### 4.1 é›†æˆæµ‹è¯•

```python
# tests/test_enhanced_memory_integration.py

import pytest
import asyncio
from pathlib import Path
from utils.enhanced_memory import EnhancedMemory
from tools.workflow.thinking_enhanced import EnhancedThinkingTool
from tools.simple.memory_manager import MemoryManagerTool

class TestEnhancedMemoryIntegration:
    """æµ‹è¯•å¢å¼ºè®°å¿†ç³»ç»Ÿçš„å®Œæ•´é›†æˆ"""
    
    @pytest.fixture
    def memory_system(self, tmp_path):
        """åˆ›å»ºæµ‹è¯•ç”¨è®°å¿†ç³»ç»Ÿ"""
        # ä½¿ç”¨ä¸´æ—¶ç›®å½•
        import os
        os.environ["WORKSPACE_ROOT"] = str(tmp_path)
        return EnhancedMemory()
    
    @pytest.fixture
    def thinking_tool(self):
        """åˆ›å»ºæ€ç»´å·¥å…·å®ä¾‹"""
        return EnhancedThinkingTool()
    
    @pytest.mark.asyncio
    async def test_environment_detection(self, memory_system, tmp_path):
        """æµ‹è¯•ç¯å¢ƒè‡ªåŠ¨æ£€æµ‹"""
        # åˆ›å»ºæ¨¡æ‹Ÿé¡¹ç›®ç»“æ„
        (tmp_path / "venv").mkdir()
        (tmp_path / "venv" / "bin").mkdir()
        (tmp_path / "venv" / "bin" / "python").touch()
        (tmp_path / "src").mkdir()
        (tmp_path / "tests").mkdir()
        (tmp_path / "requirements.txt").touch()
        
        # æ‰§è¡Œç¯å¢ƒæ£€æµ‹
        env_info = memory_system.detect_and_learn_environment(str(tmp_path))
        
        # éªŒè¯æ£€æµ‹ç»“æœ
        assert env_info["virtual_env"] is not None
        assert env_info["project_type"] is not None
        assert "src" in env_info["structure"]
        assert "tests" in env_info["structure"]
    
    @pytest.mark.asyncio
    async def test_thinking_pattern_learning(self, memory_system):
        """æµ‹è¯•æ€ç»´æ¨¡å¼å­¦ä¹ """
        # æ¨¡æ‹ŸæˆåŠŸçš„æ€ç»´æ¨¡å¼ä½¿ç”¨
        modes = ["five_whys", "systems_thinking"]
        memory_system._update_pattern_learning(modes, 0.9, "debugging")
        
        # å†æ¬¡ä½¿ç”¨ç›¸åŒç»„åˆ
        memory_system._update_pattern_learning(modes, 0.85, "debugging")
        
        # è·å–æ¨è
        recommendations = memory_system.get_recommended_thinking_modes(
            "debug authentication error",
            {"problem_type": "debugging"}
        )
        
        # éªŒè¯å­¦ä¹ æ•ˆæœ
        assert "five_whys" in recommendations
        assert "systems_thinking" in recommendations
    
    @pytest.mark.asyncio
    async def test_todo_management(self, memory_system, tmp_path):
        """æµ‹è¯•TODOç®¡ç†åŠŸèƒ½"""
        # åˆ›å»ºTODOæ–‡ä»¶
        todo_content = """# PROJECT_TODO.md
        
## ğŸ¯ ä¸»çº¿ä»»åŠ¡
- [ ] å®ç°ç”¨æˆ·è®¤è¯ #epic
  - [ ] è®¾è®¡è®¤è¯æ¶æ„ #task
  - [ ] å®ç°ç™»å½•åŠŸèƒ½ #task
  - [ ] æ·»åŠ æµ‹è¯• #task
        """
        
        todo_file = tmp_path / "TODO.md"
        todo_file.write_text(todo_content)
        
        # è§£æTODO
        result = memory_system.parse_and_manage_todo(str(todo_file))
        
        # éªŒè¯è§£æç»“æœ
        assert result["status"] == "success"
        assert result["main_tasks"] > 0
        assert memory_system.todo_manager["current_context"] == "main"
    
    @pytest.mark.asyncio
    async def test_intelligent_branching(self, memory_system):
        """æµ‹è¯•æ™ºèƒ½åˆ†æ”¯ç®¡ç†"""
        # è®¾ç½®ä¸»çº¿ä»»åŠ¡
        memory_system.todo_manager["main_thread"] = [
            {"task": "implement authentication", "status": "in_progress"}
        ]
        
        # åˆ›å»ºç´§æ€¥åˆ†æ”¯
        branch_id = memory_system.create_intelligent_branch(
            "ç´§æ€¥ä¿®å¤ç”Ÿäº§ç¯å¢ƒbug",
            {"urgency": "high", "affects": "production"}
        )
        
        # éªŒè¯åˆ†æ”¯åˆ›å»º
        assert branch_id is not None
        assert memory_system.todo_manager["current_context"] == branch_id
        
        # è¿”å›ä¸»çº¿
        result = memory_system.smart_return_to_main()
        assert result["status"] == "returned_to_main"
        assert memory_system.todo_manager["current_context"] == "main"
    
    @pytest.mark.asyncio
    async def test_memory_recall_with_context(self, memory_system):
        """æµ‹è¯•åŸºäºä¸Šä¸‹æ–‡çš„è®°å¿†å¬å›"""
        # ä¿å­˜å¤šä¸ªè®°å¿†
        memories = [
            {
                "content": "Always use TypeScript for new projects",
                "metadata": {
                    "type": "preference",
                    "tags": ["typescript", "coding_style"],
                    "thinking_modes": ["first_principles"],
                    "quality_score": 0.9
                }
            },
            {
                "content": "Project uses PostgreSQL for database",
                "metadata": {
                    "type": "architecture",
                    "tags": ["database", "postgresql"],
                    "thinking_modes": ["systems_thinking"],
                    "quality_score": 0.8
                }
            },
            {
                "content": "Debug database connection issues with connection pooling",
                "metadata": {
                    "type": "experience",
                    "tags": ["debug", "database", "solution"],
                    "thinking_modes": ["five_whys", "systems_thinking"],
                    "quality_score": 0.85
                }
            }
        ]
        
        for memory in memories:
            memory_system.save_enhanced_memory(
                memory["content"],
                memory["metadata"]
            )
        
        # æµ‹è¯•å¬å›
        results = memory_system.recall_with_thinking(
            "database debugging",
            {
                "thinking_modes": ["five_whys"],
                "tool": "debug"
            }
        )
        
        # éªŒè¯å¬å›ç»“æœ
        assert len(results) > 0
        assert any("debug" in str(r).lower() for r in results)
    
    @pytest.mark.asyncio
    async def test_thinking_tool_integration(self, thinking_tool):
        """æµ‹è¯•æ€ç»´å·¥å…·é›†æˆ"""
        from tools.base_tool import ToolRequest
        
        # åˆ›å»ºæµ‹è¯•è¯·æ±‚
        request = ToolRequest(
            tool="think",
            params={
                "problem": "å¦‚ä½•ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½ï¼Ÿ",
                "mode": "auto",
                "depth": "medium"
            }
        )
        
        # æ‰§è¡Œæ€ç»´åˆ†æ
        result = await thinking_tool.run_workflow(request)
        
        # éªŒè¯ç»“æœ
        assert result.status == "success"
        result_data = json.loads(result.content)
        assert "thinking_modes_used" in result_data
        assert len(result_data["thinking_modes_used"]) > 0
        assert "key_insights" in result_data
```

#### 4.2 é€šä¿¡æ¨¡æ‹Ÿå™¨æµ‹è¯•

```python
# communication_simulator_test.py æ·»åŠ å¢å¼ºåŠŸèƒ½æµ‹è¯•

def test_enhanced_memory_functionality(self):
    """æµ‹è¯•å¢å¼ºè®°å¿†åŠŸèƒ½"""
    # æµ‹è¯•ç¯å¢ƒæ£€æµ‹
    response = self.call_tool("memory", {
        "action": "detect_env",
        "project_path": "."
    })
    self.assertIn("environment", response)
    
    # æµ‹è¯•è®°å¿†ä¿å­˜
    response = self.call_tool("memory", {
        "action": "save",
        "content": "ä½¿ç”¨ async/await å¤„ç†å¼‚æ­¥æ“ä½œ",
        "metadata": {
            "type": "preference",
            "tags": ["coding_style", "javascript"],
            "thinking_modes": ["first_principles"],
            "quality_score": 0.9
        }
    })
    self.assertEqual(response["status"], "saved")
    
    # æµ‹è¯•è®°å¿†å¬å›
    response = self.call_tool("memory", {
        "action": "recall",
        "query": "å¼‚æ­¥ç¼–ç¨‹",
        "context": {
            "thinking_modes": ["first_principles"]
        }
    })
    self.assertGreater(response["found"], 0)

def test_thinking_modes_integration(self):
    """æµ‹è¯•æ€ç»´æ¨¡å¼é›†æˆ"""
    # æµ‹è¯•è‡ªåŠ¨æ¨¡å¼é€‰æ‹©
    response = self.call_tool("think", {
        "problem": "ç³»ç»Ÿå“åº”æ—¶é—´è¿‡é•¿ï¼Œéœ€è¦ä¼˜åŒ–æ€§èƒ½",
        "mode": "auto",
        "depth": "deep"
    })
    
    self.assertIn("thinking_modes_used", response)
    self.assertIn("constraint_theory", response["thinking_modes_used"])  # åº”è¯¥åŒ…å«çº¦æŸç†è®º
    self.assertIn("key_insights", response)

def test_todo_driven_workflow(self):
    """æµ‹è¯•TODOé©±åŠ¨çš„å·¥ä½œæµ"""
    # è§£æTODOæ–‡ä»¶
    response = self.call_tool("memory", {
        "action": "parse_todo",
        "todo_path": "TODO.md"
    })
    
    if response["status"] == "success":
        self.assertIn("main_tasks", response["statistics"])
        
        # æµ‹è¯•åˆ†æ”¯åˆ›å»º
        response = self.call_tool("memory", {
            "action": "create_branch",
            "reason": "ç´§æ€¥ä¿®å¤ç”¨æˆ·åé¦ˆçš„bug",
            "context": {"urgency": "high"}
        })
        
        if response["status"] == "branch_created":
            branch_id = response["branch_id"]
            
            # æµ‹è¯•è¿”å›ä¸»çº¿
            response = self.call_tool("memory", {
                "action": "return_main"
            })
            self.assertEqual(response["status"], "returned_to_main")
```

### ç¬¬äº”é˜¶æ®µï¼šä¼˜åŒ–ä¸éƒ¨ç½²ï¼ˆDay 8-9ï¼‰

#### 5.1 æ€§èƒ½ä¼˜åŒ–

```python
# utils/performance_optimizer.py
"""æ€§èƒ½ä¼˜åŒ–å·¥å…·"""

import functools
import time
from typing import Any, Callable, Dict
import asyncio
from collections import deque

class PerformanceOptimizer:
    """è®°å¿†ç³»ç»Ÿæ€§èƒ½ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.cache = {}
        self.access_times = deque(maxlen=1000)
        self.slow_operations = []
    
    @staticmethod
    def cached(ttl: int = 300):
        """ç¼“å­˜è£…é¥°å™¨ï¼Œæ”¯æŒTTL"""
        def decorator(func: Callable) -> Callable:
            cache = {}
            
            @functools.wraps(func)
            async def wrapper(*args, **kwargs):
                # ç”Ÿæˆç¼“å­˜é”®
                cache_key = f"{func.__name__}:{str(args)}:{str(kwargs)}"
                
                # æ£€æŸ¥ç¼“å­˜
                if cache_key in cache:
                    result, timestamp = cache[cache_key]
                    if time.time() - timestamp < ttl:
                        return result
                
                # æ‰§è¡Œå‡½æ•°
                result = await func(*args, **kwargs)
                
                # æ›´æ–°ç¼“å­˜
                cache[cache_key] = (result, time.time())
                
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                if len(cache) > 100:
                    current_time = time.time()
                    expired_keys = [
                        k for k, (_, t) in cache.items() 
                        if current_time - t > ttl
                    ]
                    for k in expired_keys:
                        del cache[k]
                
                return result
            
            return wrapper
        return decorator
    
    @staticmethod
    def measure_performance(threshold: float = 1.0):
        """æ€§èƒ½æµ‹é‡è£…é¥°å™¨"""
        def decorator(func: Callable) -> Callable:
            @functools.wraps(func)
            async def wrapper(*args, **kwargs):
                start_time = time.time()
                
                try:
                    result = await func(*args, **kwargs)
                    execution_time = time.time() - start_time
                    
                    # è®°å½•æ…¢æ“ä½œ
                    if execution_time > threshold:
                        import logging
                        logger = logging.getLogger(__name__)
                        logger.warning(
                            f"Slow operation: {func.__name__} took {execution_time:.2f}s"
                        )
                    
                    return result
                    
                except Exception as e:
                    execution_time = time.time() - start_time
                    import logging
                    logger = logging.getLogger(__name__)
                    logger.error(
                        f"Error in {func.__name__} after {execution_time:.2f}s: {str(e)}"
                    )
                    raise
            
            return wrapper
        return decorator

# åœ¨ EnhancedMemory ä¸­åº”ç”¨ä¼˜åŒ–
class OptimizedEnhancedMemory(EnhancedMemory):
    """æ€§èƒ½ä¼˜åŒ–çš„å¢å¼ºè®°å¿†ç³»ç»Ÿ"""
    
    def __init__(self):
        super().__init__()
        self.optimizer = PerformanceOptimizer()
    
    @PerformanceOptimizer.cached(ttl=600)
    async def recall_with_thinking(self, query: str, context: Dict[str, Any]) -> List[Dict]:
        """ç¼“å­˜çš„è®°å¿†å¬å›"""
        return await super().recall_with_thinking(query, context)
    
    @PerformanceOptimizer.measure_performance(threshold=0.5)
    async def _execute_thinking_mode(self, mode: str, problem: str, 
                                   context: Dict, depth: str) -> Dict:
        """æ€§èƒ½ç›‘æ§çš„æ€ç»´æ¨¡å¼æ‰§è¡Œ"""
        return await super()._execute_thinking_mode(mode, problem, context, depth)
```

#### 5.2 é…ç½®æ–‡ä»¶

```yaml
# conf/enhanced_features.yaml
"""å¢å¼ºåŠŸèƒ½é…ç½®"""

memory_system:
  # è®°å¿†å­˜å‚¨é…ç½®
  storage:
    base_path: ".zen_memory"
    max_memory_size_mb: 100
    auto_cleanup_days: 90
  
  # è®°å¿†å±‚çº§é…ç½®
  layers:
    global:
      persistence: true
      max_items: 10000
      priority: high
    project:
      persistence: true
      max_items: 5000
      priority: medium
    session:
      persistence: false
      max_items: 1000
      priority: low
  
  # å¬å›é…ç½®
  recall:
    default_limit: 10
    max_limit: 50
    relevance_threshold: 0.3
    strategies:
      - keyword_matching
      - semantic_similarity
      - pattern_matching
      - temporal_relevance

thinking_system:
  # æ€ç»´æ¨¡å¼é…ç½®
  modes:
    enabled: all  # æˆ–æŒ‡å®šåˆ—è¡¨
    default_depth: medium
    max_rounds: 15
  
  # è‡ªåŠ¨é€‰æ‹©é…ç½®
  auto_selection:
    enabled: true
    max_modes_per_analysis: 5
    confidence_threshold: 0.7
  
  # å­¦ä¹ é…ç½®
  learning:
    enabled: true
    min_quality_for_learning: 0.7
    pattern_retention_days: 180
    success_threshold: 0.8

path_intelligence:
  # è·¯å¾„æ£€æµ‹é…ç½®
  detection:
    auto_detect_venv: true
    venv_names: ["venv", ".venv", "env", ".env", "virtualenv"]
    project_markers: ["setup.py", "package.json", "Cargo.toml", "go.mod"]
  
  # æ™ºèƒ½æ¨èé…ç½®
  recommendations:
    use_frequency_data: true
    consider_project_structure: true
    max_alternatives: 5

todo_management:
  # TODOè§£æé…ç½®
  parsing:
    file_names: ["TODO.md", "todo.md", "TODO.txt", "tasks.md"]
    task_patterns:
      - "- [ ] (.*) #(\\w+)"
      - "\\* TODO: (.*)"
      - "\\[ \\] (.*)"
  
  # åˆ†æ”¯ç®¡ç†é…ç½®
  branching:
    auto_branch_keywords: ["ç´§æ€¥", "urgent", "critical", "hotfix"]
    max_branch_duration_hours: 24
    auto_return: true
    warn_on_long_branch: true

performance:
  # ç¼“å­˜é…ç½®
  caching:
    enabled: true
    memory_ttl_seconds: 600
    thinking_ttl_seconds: 300
    max_cache_size_mb: 50
  
  # æ€§èƒ½ç›‘æ§
  monitoring:
    enabled: true
    slow_operation_threshold_seconds: 1.0
    log_performance_metrics: true
```

#### 5.3 ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env æ–‡ä»¶æ·»åŠ å¢å¼ºåŠŸèƒ½é…ç½®

# å¢å¼ºè®°å¿†ç³»ç»Ÿ
ENABLE_ENHANCED_MEMORY=true
MEMORY_AUTO_SAVE=true
MEMORY_AUTO_DETECT_ENV=true
MEMORY_STORAGE_PATH=.zen_memory
MEMORY_MAX_RECALL=20

# æ€ç»´å·¥å…·ç®±
ENABLE_THINKING_MODES=true
THINKING_DEFAULT_DEPTH=medium
THINKING_AUTO_MODE=true
THINKING_LEARN_PATTERNS=true
THINKING_MAX_MODES=5

# TODOç®¡ç†
TODO_AUTO_PARSE=true
TODO_FILE_PATH=TODO.md
TODO_BRANCH_AUTO_RETURN=true
TODO_BRANCH_MAX_DURATION=24

# è·¯å¾„æ™ºèƒ½
PATH_AUTO_DETECT=true
PATH_LEARN_PATTERNS=true
PATH_SUGGESTION_COUNT=5

# æ€§èƒ½ä¼˜åŒ–
ENABLE_CACHING=true
CACHE_TTL_SECONDS=600
ENABLE_PERFORMANCE_MONITORING=true
SLOW_OP_THRESHOLD=1.0
```

### ç¬¬å…­é˜¶æ®µï¼šæ–‡æ¡£ä¸ä½¿ç”¨æŒ‡å—ï¼ˆDay 10ï¼‰

#### 6.1 ç”¨æˆ·ä½¿ç”¨æŒ‡å—

```markdown
# å¢å¼ºåŠŸèƒ½ä½¿ç”¨æŒ‡å—

## å¿«é€Ÿå¼€å§‹

### 1. é¦–æ¬¡ä½¿ç”¨ - ç¯å¢ƒåˆå§‹åŒ–

å½“æ‚¨é¦–æ¬¡åœ¨é¡¹ç›®ä¸­ä½¿ç”¨ Zen MCP Server æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ï¼š
- æ£€æµ‹é¡¹ç›®ç±»å‹å’Œç»“æ„
- è¯†åˆ«è™šæ‹Ÿç¯å¢ƒ
- è§£æ TODO æ–‡ä»¶
- å»ºç«‹é¡¹ç›®è®°å¿†åŸºç¡€

æ— éœ€ä»»ä½•é…ç½®ï¼Œåªéœ€æ­£å¸¸ä½¿ç”¨å³å¯ã€‚

### 2. æ™ºèƒ½è®°å¿†åŠŸèƒ½

#### ä¿å­˜é‡è¦ä¿¡æ¯
```
"è®°ä½æˆ‘æ€»æ˜¯å–œæ¬¢ä½¿ç”¨ TypeScript è€Œä¸æ˜¯ JavaScript"
"è¿™ä¸ªé¡¹ç›®çš„æ•°æ®åº“æ˜¯ PostgreSQLï¼Œè¿æ¥æ± å¤§å°è®¾ä¸º 20"
"éƒ¨ç½²æ—¶alwaysè®°å¾—å…ˆè¿è¡Œæµ‹è¯•"
```

ç³»ç»Ÿä¼šè‡ªåŠ¨ï¼š
- è¯†åˆ«ä¿¡æ¯ç±»å‹ï¼ˆåå¥½/é¡¹ç›®é…ç½®/ç»éªŒï¼‰
- é€‰æ‹©åˆé€‚çš„å­˜å‚¨å±‚çº§
- å»ºç«‹ç´¢å¼•ä¾¿äºåç»­æ£€ç´¢

#### æ™ºèƒ½å¬å›
å½“æ‚¨æé—®æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨è°ƒç”¨ç›¸å…³è®°å¿†ï¼š
```
"å¦‚ä½•å¤„ç†æ•°æ®åº“è¿æ¥ï¼Ÿ"
â†’ ç³»ç»Ÿè‡ªåŠ¨å¬å›ï¼šé¡¹ç›®ä½¿ç”¨ PostgreSQLï¼Œè¿æ¥æ± é…ç½®ä¸º 20
```

### 3. 25ç§æ€ç»´æ¨¡å¼

#### è‡ªåŠ¨æ¨¡å¼
```
"å¸®æˆ‘åˆ†æä¸ºä»€ä¹ˆç³»ç»Ÿå“åº”å˜æ…¢äº†"
â†’ è‡ªåŠ¨æ¿€æ´»ï¼š5 Whysï¼ˆæ ¹å› åˆ†æï¼‰+ çº¦æŸç†è®ºï¼ˆç“¶é¢ˆè¯†åˆ«ï¼‰+ ç³»ç»Ÿæ€ç»´ï¼ˆå…¨å±€å½±å“ï¼‰
```

#### æŒ‡å®šæ¨¡å¼
```
"ç”¨ç¬¬ä¸€æ€§åŸç†æ€è€ƒå¦‚ä½•é‡æ–°è®¾è®¡è®¤è¯ç³»ç»Ÿ"
"ä½¿ç”¨ SWOT åˆ†æè¯„ä¼°å¾®æœåŠ¡æ¶æ„çš„åˆ©å¼Š"
```

#### æ·±åº¦æ§åˆ¶
- `shallow`ï¼ˆæµ…å±‚ï¼‰ï¼šå¿«é€Ÿåˆ†æï¼Œ2è½®æ€è€ƒ
- `medium`ï¼ˆä¸­å±‚ï¼‰ï¼šå¹³è¡¡åˆ†æï¼Œ5è½®æ€è€ƒï¼ˆé»˜è®¤ï¼‰
- `deep`ï¼ˆæ·±å±‚ï¼‰ï¼šæ·±å…¥åˆ†æï¼Œ8è½®æ€è€ƒ
- `expert`ï¼ˆä¸“å®¶ï¼‰ï¼šç©·å°½åˆ†æï¼Œ12è½®æ€è€ƒ

### 4. TODOé©±åŠ¨å¼€å‘

#### è‡ªåŠ¨è§£æ
ç³»ç»Ÿè‡ªåŠ¨è§£æé¡¹ç›®ä¸­çš„ TODO.md æ–‡ä»¶ï¼Œå»ºç«‹ä»»åŠ¡ä¸»çº¿ã€‚

#### æ™ºèƒ½åˆ†æ”¯
```
"ç´§æ€¥ä¿®å¤ç”Ÿäº§ç¯å¢ƒçš„ç™»å½•bug"
â†’ ç³»ç»Ÿè‡ªåŠ¨ï¼š
1. åˆ›å»ºä¸´æ—¶åˆ†æ”¯
2. ä¿å­˜å½“å‰ä¸Šä¸‹æ–‡
3. åˆ‡æ¢åˆ°ç´§æ€¥ä»»åŠ¡
4. å®Œæˆåæé†’è¿”å›ä¸»çº¿
```

#### ä¿æŒä¸“æ³¨
ç³»ç»Ÿä¼šï¼š
- æé†’æ‚¨å½“å‰åœ¨å“ªä¸ªä»»åŠ¡ä¸Šä¸‹æ–‡
- é˜²æ­¢åç¦»ä¸»è¦ä»»åŠ¡å¤ªè¿œ
- æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ›å»ºåˆ†æ”¯

### 5. è·¯å¾„æ™ºèƒ½

#### æ–‡ä»¶åˆ›å»ºå»ºè®®
```
"åˆ›å»ºä¸€ä¸ªæ–°çš„å·¥å…·ç±» user_validator.py"
â†’ ç³»ç»Ÿå»ºè®®ï¼š
- ä¸»è¦å»ºè®®ï¼š/src/utils/user_validator.pyï¼ˆåŸºäºé¡¹ç›®ç»“æ„ï¼‰
- å¤‡é€‰1ï¼š/src/validators/user_validator.pyï¼ˆç±»ä¼¼æ–‡ä»¶ä½ç½®ï¼‰
- å¤‡é€‰2ï¼š/lib/user_validator.pyï¼ˆåŸºäºä½¿ç”¨é¢‘ç‡ï¼‰
```

#### è™šæ‹Ÿç¯å¢ƒæ„ŸçŸ¥
ç³»ç»Ÿè‡ªåŠ¨æ£€æµ‹å¹¶è®°ä½è™šæ‹Ÿç¯å¢ƒä½ç½®ï¼Œæä¾›å¿«æ·å‘½ä»¤ï¼š
```
Python: /project/venv/bin/python
Pip: /project/venv/bin/pip
```

## é«˜çº§åŠŸèƒ½

### è®°å¿†æ¨¡å¼åˆ†æ
```
"åˆ†ææˆ‘çš„é—®é¢˜è§£å†³æ¨¡å¼"
â†’ è¾“å‡ºï¼š
- æœ€å¸¸ç”¨æ€ç»´æ¨¡å¼ï¼š5 Whys (45%), ç³»ç»Ÿæ€ç»´ (30%)
- æˆåŠŸç‡æœ€é«˜ç»„åˆï¼š5 Whys + çº¦æŸç†è®º (æˆåŠŸç‡ 85%)
- é¢†åŸŸä¸“é•¿ï¼šè°ƒè¯• (ä¼˜ç§€), æ¶æ„è®¾è®¡ (è‰¯å¥½)
```

### æ‰¹é‡è®°å¿†æ“ä½œ
```
"å¯¼å‡ºæœ¬é¡¹ç›®çš„æ‰€æœ‰æ¶æ„å†³ç­–"
"åˆ†æè¿‡å»ä¸€å‘¨çš„ç¼–ç æ¨¡å¼"
"æ¸…ç†30å¤©å‰çš„ä¼šè¯è®°å¿†"
```

### æ€ç»´æ¨¡å¼å­¦ä¹ 
ç³»ç»Ÿä¼šå­¦ä¹ ï¼š
- å“ªäº›æ¨¡å¼å¯¹ç‰¹å®šé—®é¢˜æœ€æœ‰æ•ˆ
- æ‚¨åå¥½çš„æ€ç»´æ¨¡å¼ç»„åˆ
- ä¸åŒåœºæ™¯ä¸‹çš„æœ€ä½³å®è·µ

## æœ€ä½³å®è·µ

### 1. ä¸»åŠ¨è®°å¿†
- é‡è¦å†³ç­–åè¯´"è®°ä½è¿™ä¸ªå†³å®š"
- è§£å†³é—®é¢˜ååˆ†äº«ç»éªŒ
- æ˜ç¡®æŒ‡å‡ºåå¥½å’Œçº¦å®š

### 2. åˆ©ç”¨ä¸Šä¸‹æ–‡
- æé—®æ—¶æä¾›èƒŒæ™¯ä¿¡æ¯
- å¼•ç”¨ä¹‹å‰çš„è®¨è®º
- ä½¿ç”¨é¡¹ç›®ç‰¹å®šæœ¯è¯­

### 3. æ¸è¿›å¼åˆ†æ
- ç®€å•é—®é¢˜ç”¨æµ…å±‚åˆ†æ
- å¤æ‚é—®é¢˜é€æ­¥åŠ æ·±
- å…³é”®å†³ç­–ç”¨ä¸“å®¶æ¨¡å¼

### 4. ä¿æŒä¸»çº¿æ¸…æ™°
- å®šæœŸæ›´æ–° TODO.md
- åŠæ—¶å®Œæˆåˆ†æ”¯ä»»åŠ¡
- é¿å…è¿‡å¤šå¹¶è¡Œåˆ†æ”¯

## æ•…éšœæ’é™¤

### è®°å¿†æœªç”Ÿæ•ˆ
- æ£€æŸ¥ .env ä¸­ ENABLE_ENHANCED_MEMORY=true
- ç¡®è®¤æœ‰å†™å…¥æƒé™åˆ° .zen_memory ç›®å½•
- æŸ¥çœ‹æ—¥å¿—ä¸­çš„è®°å¿†ä¿å­˜ä¿¡æ¯

### æ€ç»´æ¨¡å¼æœªè§¦å‘
- ç¡®è®¤ ENABLE_THINKING_MODES=true
- æ£€æŸ¥é—®é¢˜æè¿°ä¸­çš„å…³é”®è¯
- å°è¯•æ˜ç¡®æŒ‡å®šæ¨¡å¼

### TODOè§£æå¤±è´¥
- ç¡®è®¤ TODO.md æ ¼å¼æ­£ç¡®
- ä½¿ç”¨æ”¯æŒçš„ä»»åŠ¡æ ‡è®°æ ¼å¼
- æ£€æŸ¥æ–‡ä»¶ç¼–ç ï¼ˆUTF-8ï¼‰

## éšç§è¯´æ˜

- æ‰€æœ‰è®°å¿†å­˜å‚¨åœ¨æœ¬åœ° .zen_memory ç›®å½•
- ä¸ä¼šä¸Šä¼ åˆ°ä»»ä½•äº‘æœåŠ¡
- å¯éšæ—¶åˆ é™¤è®°å¿†æ–‡ä»¶
- æ”¯æŒé€‰æ‹©æ€§å¯¼å‡ºå’Œæ¸…ç†
```

#### 6.2 å¼€å‘è€…é›†æˆæŒ‡å—

```markdown
# å¼€å‘è€…é›†æˆæŒ‡å—

## åœ¨è‡ªå®šä¹‰å·¥å…·ä¸­ä½¿ç”¨è®°å¿†ç³»ç»Ÿ

### 1. å·¥å…·åŸºç±»é›†æˆ

```python
from tools.base_tool import BaseTool
from utils.enhanced_memory import EnhancedMemory

class MyCustomTool(BaseTool):
    def __init__(self):
        super().__init__()
        self.memory_system = EnhancedMemory()
    
    async def _run_implementation(self, request: ToolRequest) -> ToolOutput:
        # è·å–ç›¸å…³è®°å¿†
        memories = self.get_relevant_memories(
            request.params.get("query", "")
        )
        
        # è·å–å»ºè®®çš„æ€ç»´æ¨¡å¼
        thinking_modes = self.get_suggested_thinking_modes()
        
        # ä½¿ç”¨è®°å¿†ä¸Šä¸‹æ–‡æ‰§è¡Œä»»åŠ¡
        # ...
        
        # ä¿å­˜æœ‰ä»·å€¼çš„ç»“æœ
        if result_quality > 0.8:
            self.memory_system.save_enhanced_memory(
                content=result,
                metadata={
                    "type": "tool_result",
                    "tool": self.get_name(),
                    "quality_score": result_quality
                }
            )
```

### 2. æ·»åŠ æ–°çš„æ€ç»´æ¨¡å¼

```python
# åœ¨ thinking_modes å­—å…¸ä¸­æ·»åŠ 
"new_thinking_mode": {
    "name": "æ–°æ€ç»´æ¨¡å¼",
    "category": "custom",
    "description": "æ¨¡å¼æè¿°",
    "process": [
        {"step": "æ­¥éª¤1", "prompt": "å¼•å¯¼é—®é¢˜1"},
        {"step": "æ­¥éª¤2", "prompt": "å¼•å¯¼é—®é¢˜2"}
    ],
    "output_format": {
        "key1": "è¾“å‡ºæ ¼å¼è¯´æ˜"
    },
    "suitable_for": ["é€‚ç”¨åœºæ™¯1", "é€‚ç”¨åœºæ™¯2"]
}

# æ·»åŠ è§¦å‘å…³é”®è¯
self.keyword_triggers["new_thinking_mode"] = ["å…³é”®è¯1", "å…³é”®è¯2"]
```

### 3. æ‰©å±•è®°å¿†ç±»å‹

```python
# è‡ªå®šä¹‰è®°å¿†ç±»å‹
class CustomMemoryType:
    def __init__(self):
        self.memory_type = "custom_type"
        self.storage_layer = "project"  # æˆ– global/session
    
    def should_save(self, content: Any) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¿å­˜"""
        # è‡ªå®šä¹‰é€»è¾‘
        return True
    
    def process_before_save(self, content: Any) -> Any:
        """ä¿å­˜å‰å¤„ç†"""
        # å¤„ç†é€»è¾‘
        return processed_content
```

### 4. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

```python
# ä½¿ç”¨ç¼“å­˜è£…é¥°å™¨
@PerformanceOptimizer.cached(ttl=300)
async def expensive_operation(self, param):
    # è€—æ—¶æ“ä½œ
    pass

# æ‰¹é‡æ“ä½œ
async def batch_save_memories(self, memories: List[Dict]):
    tasks = []
    for memory in memories:
        task = self.memory_system.save_enhanced_memory(
            memory["content"],
            memory["metadata"]
        )
        tasks.append(task)
    
    await asyncio.gather(*tasks)
```

## API å‚è€ƒ

### EnhancedMemory

```python
# ä¿å­˜è®°å¿†
memory_id = memory_system.save_enhanced_memory(
    content=Any,  # è®°å¿†å†…å®¹
    metadata={
        "type": str,  # è®°å¿†ç±»å‹
        "tags": List[str],  # æ ‡ç­¾
        "thinking_modes": List[str],  # ç›¸å…³æ€ç»´æ¨¡å¼
        "quality_score": float,  # è´¨é‡åˆ†æ•° 0-1
    }
)

# å¬å›è®°å¿†
memories = memory_system.recall_with_thinking(
    query=str,  # æŸ¥è¯¢å­—ç¬¦ä¸²
    context={
        "thinking_modes": List[str],  # å½“å‰æ€ç»´æ¨¡å¼
        "tool": str,  # è°ƒç”¨å·¥å…·
        "scopes": List[str],  # æœç´¢èŒƒå›´
    }
)

# ç¯å¢ƒæ£€æµ‹
env_info = memory_system.detect_and_learn_environment(
    project_path=str  # é¡¹ç›®è·¯å¾„
)

# TODOç®¡ç†
result = memory_system.parse_and_manage_todo(
    todo_path=str  # TODOæ–‡ä»¶è·¯å¾„
)

# ä»»åŠ¡åˆ†æ”¯
branch_id = memory_system.create_intelligent_branch(
    reason=str,  # åˆ†æ”¯åŸå› 
    context=Dict  # ä¸Šä¸‹æ–‡ä¿¡æ¯
)
```

### EnhancedThinkingTool

```python
# æ‰§è¡Œæ€ç»´åˆ†æ
result = await thinking_tool.run_workflow({
    "problem": str,  # é—®é¢˜æè¿°
    "mode": str,  # "auto" æˆ–å…·ä½“æ¨¡å¼
    "depth": str,  # shallow/medium/deep/expert
    "context": Dict,  # é¢å¤–ä¸Šä¸‹æ–‡
})

# è·å–æ¨¡å¼æ¨è
modes = thinking_tool._intelligent_mode_selection(
    problem=str,
    context=Dict,
    memory_recommendations=List[str]
)
```

## æ‰©å±•ç‚¹

### 1. è‡ªå®šä¹‰å­˜å‚¨åç«¯
å®ç° `StorageBackend` æ¥å£ä»¥ä½¿ç”¨ä¸åŒçš„å­˜å‚¨æ–¹æ¡ˆï¼ˆå¦‚ Redisã€PostgreSQLï¼‰ã€‚

### 2. æ·»åŠ æ–°çš„å¬å›ç­–ç•¥
æ‰©å±• `recall_with_thinking` æ–¹æ³•ï¼Œæ·»åŠ æ–°çš„æœç´¢ç­–ç•¥ã€‚

### 3. é›†æˆå¤–éƒ¨çŸ¥è¯†åº“
é€šè¿‡è®°å¿†ç³»ç»Ÿæ¥å£é›†æˆå¤–éƒ¨çŸ¥è¯†æºã€‚

### 4. è‡ªå®šä¹‰æ€ç»´æ¨¡å¼ç»„åˆ
åˆ›å»ºç‰¹å®šé¢†åŸŸçš„æ€ç»´æ¨¡å¼ç»„åˆæ¨¡æ¿ã€‚
```

### æ€»ç»“

è¿™ä¸ªå®Œæ•´çš„å®æ–½è®¡åˆ’æä¾›äº†ï¼š

1. **è¯¦ç»†çš„ä»£ç å®ç°** - åŒ…æ‹¬è®°å¿†ç³»ç»Ÿã€æ€ç»´å·¥å…·ã€é›†æˆä¿®æ”¹
2. **å®Œæ•´çš„æµ‹è¯•æ–¹æ¡ˆ** - å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
3. **æ€§èƒ½ä¼˜åŒ–ç­–ç•¥** - ç¼“å­˜ã€ç›‘æ§ã€é…ç½®
4. **å…¨é¢çš„æ–‡æ¡£** - ç”¨æˆ·æŒ‡å—å’Œå¼€å‘è€…æ–‡æ¡£
5. **éƒ¨ç½²é…ç½®** - ç¯å¢ƒå˜é‡å’Œé…ç½®æ–‡ä»¶

æ•´ä¸ªç³»ç»Ÿå®ç°äº†ï¼š
- ä¸‰å±‚æ™ºèƒ½è®°å¿†ä½“ç³»
- 25ç§ä¸“å®¶æ€ç»´æ¨¡å¼
- TODOé©±åŠ¨çš„ä»»åŠ¡ç®¡ç†
- æ™ºèƒ½è·¯å¾„æ¨è
- è‡ªåŠ¨ç¯å¢ƒæ£€æµ‹
- æŒç»­å­¦ä¹ ä¼˜åŒ–

è¿™å°†æå¤§æå‡ AI åŠ©æ‰‹çš„æ™ºèƒ½æ°´å¹³å’Œç”¨æˆ·ä½“éªŒï¼