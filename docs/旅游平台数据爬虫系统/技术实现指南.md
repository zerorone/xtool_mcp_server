# 旅游平台数据爬虫系统 - 技术实现指南

## 技术架构总览

### 系统架构图
```
┌─────────────────────────────────────────────────────────────┐
│                    API Gateway Layer                        │
├─────────────────────────────────────────────────────────────┤
│                  FastAPI Application                        │
├─────────────────────────────────────────────────────────────┤
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐     │
│  │   Crawl4AI   │  │  MediaCrawl  │  │   Scheduler  │     │
│  │   Engine     │  │   Engine     │  │   Manager    │     │
│  └──────────────┘  └──────────────┘  └──────────────┘     │
├─────────────────────────────────────────────────────────────┤
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐     │
│  │  Proxy Pool  │  │ Fingerprint  │  │   Behavior   │     │
│  │   Manager    │  │   Manager    │  │  Simulator   │     │
│  └──────────────┘  └──────────────┘  └──────────────┘     │
├─────────────────────────────────────────────────────────────┤
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐     │
│  │    Data      │  │    Data      │  │    Data      │     │
│  │   Cleaner    │  │ Deduplicator │  │  Enhancer    │     │
│  └──────────────┘  └──────────────┘  └──────────────┘     │
├─────────────────────────────────────────────────────────────┤
│  ┌─────────────────────────┐  ┌─────────────────────────┐  │
│  │    PostgreSQL + PostGIS │  │        Redis Cache      │  │
│  └─────────────────────────┘  └─────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

---

## 第1阶段：基础架构技术要点

### 1.1 项目结构设计
```python
# 推荐的项目结构
travel-crawler-system/
├── src/
│   ├── core/                    # 核心模块
│   │   ├── engines/            # 爬虫引擎
│   │   ├── scheduler/          # 任务调度
│   │   ├── anti_detection/     # 反爬模块
│   │   └── config/             # 配置管理
│   ├── adapters/               # 平台适配器
│   ├── processors/             # 数据处理
│   ├── api/                    # API接口
│   └── utils/                  # 工具函数
├── tests/                      # 测试代码
├── docker/                     # Docker配置
├── config/                     # 配置文件
├── scripts/                    # 脚本工具
└── docs/                       # 文档
```

### 1.2 配置管理最佳实践
```python
# src/core/config/settings.py
from pydantic_settings import BaseSettings
from functools import lru_cache
import os

class Settings(BaseSettings):
    """全局配置类"""
    
    # 使用类型注解确保配置正确性
    DATABASE_URL: str
    REDIS_URL: str
    DEBUG: bool = False
    
    # 分组配置，便于管理
    class CrawlConfig:
        MAX_WORKERS: int = 10
        TIMEOUT: int = 30
        RETRY_TIMES: int = 3
    
    # 环境变量映射
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        
@lru_cache()
def get_settings() -> Settings:
    return Settings()
```

### 1.3 异步数据库连接优化
```python
# src/core/database.py
from sqlalchemy.ext.asyncio import create_async_engine, async_sessionmaker
from sqlalchemy.pool import QueuePool

# 优化的数据库引擎配置
engine = create_async_engine(
    DATABASE_URL,
    # 连接池配置
    poolclass=QueuePool,
    pool_size=20,              # 常驻连接数
    max_overflow=10,           # 最大溢出连接
    pool_timeout=30,           # 连接超时
    pool_recycle=3600,         # 连接回收时间
    pool_pre_ping=True,        # 连接预检
    # 查询优化
    echo=False,                # 生产环境关闭SQL日志
    future=True               # 使用2.0风格API
)
```

### 1.4 Redis配置优化
```python
# src/core/redis.py
import redis.asyncio as redis
from redis.asyncio import ConnectionPool

# 优化的Redis连接池
async def create_redis_pool():
    return ConnectionPool.from_url(
        REDIS_URL,
        # 连接池配置
        max_connections=100,
        retry_on_timeout=True,
        socket_timeout=5,
        socket_connect_timeout=5,
        # 健康检查
        health_check_interval=30,
        # 编码配置
        decode_responses=True,
        encoding='utf-8'
    )
```

---

## 第2阶段：双引擎核心技术

### 2.1 引擎抽象设计模式
```python
# src/core/engines/base/engine_interface.py
from abc import ABC, abstractmethod
from typing import AsyncGenerator, List
from dataclasses import dataclass

@dataclass
class CrawlTask:
    url: str
    method: str = "GET"
    headers: dict = None
    params: dict = None
    timeout: int = 30
    
class EngineInterface(ABC):
    """爬虫引擎抽象接口"""
    
    @abstractmethod
    async def initialize(self) -> None:
        """初始化引擎"""
        pass
    
    @abstractmethod
    async def crawl(self, task: CrawlTask) -> CrawlResult:
        """执行单个爬取任务"""
        pass
    
    @abstractmethod  
    async def batch_crawl(self, tasks: List[CrawlTask]) -> AsyncGenerator[CrawlResult, None]:
        """批量爬取任务"""
        pass
    
    @abstractmethod
    async def close(self) -> None:
        """关闭引擎释放资源"""
        pass
```

### 2.2 Crawl4AI引擎实现要点
```python
# src/engines/crawl4ai/crawl4ai_engine.py
from crawl4ai import AsyncWebCrawler
from crawl4ai.extraction_strategy import LLMExtractionStrategy
import asyncio
from typing import Optional

class Crawl4AIEngine(EngineInterface):
    def __init__(self, config: dict):
        self.config = config
        self.crawler: Optional[AsyncWebCrawler] = None
        self.semaphore = asyncio.Semaphore(config.get('max_concurrent', 5))
        
    async def initialize(self):
        """初始化Crawl4AI引擎"""
        self.crawler = AsyncWebCrawler(
            # 浏览器配置
            headless=True,
            browser_type="chromium",
            # 性能优化
            page_timeout=30000,
            navigation_timeout=30000,
            # 资源过滤
            ignore_images=True,
            ignore_stylesheets=True,
            # 并发控制
            max_concurrent_crawls=5
        )
        await self.crawler.__aenter__()
    
    async def crawl(self, task: CrawlTask) -> CrawlResult:
        """执行爬取任务"""
        async with self.semaphore:
            try:
                # 配置提取策略
                extraction_strategy = None
                if task.extraction_rules:
                    extraction_strategy = LLMExtractionStrategy(
                        provider="ollama/llama2",
                        api_token="ollama",
                        instruction=task.extraction_rules.get('instruction', '')
                    )
                
                # 执行爬取
                result = await self.crawler.arun(
                    url=task.url,
                    extraction_strategy=extraction_strategy,
                    bypass_cache=True,
                    timeout=task.timeout
                )
                
                return CrawlResult(
                    task_id=task.task_id,
                    success=True,
                    html=result.html,
                    extracted_data=result.extracted_content,
                    elapsed_time=result.success_time
                )
                
            except Exception as e:
                return CrawlResult(
                    task_id=task.task_id,
                    success=False,
                    error_message=str(e)
                )
```

### 2.3 MediaCrawl引擎实现要点
```python
# src/engines/mediacrawl/mediacrawl_engine.py
from playwright.async_api import async_playwright, Browser, Page
import asyncio
from contextlib import asynccontextmanager

class MediaCrawlEngine(EngineInterface):
    def __init__(self, config: dict):
        self.config = config
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.browser_pool = asyncio.Queue(maxsize=config.get('browser_pool_size', 3))
        
    async def initialize(self):
        """初始化MediaCrawl引擎"""
        self.playwright = await async_playwright().start()
        
        # 创建浏览器池
        for _ in range(self.config.get('browser_pool_size', 3)):
            browser = await self.playwright.chromium.launch(
                headless=self.config.get('headless', True),
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor'
                ]
            )
            await self.browser_pool.put(browser)
    
    @asynccontextmanager
    async def get_browser(self):
        """获取浏览器实例"""
        browser = await self.browser_pool.get()
        try:
            yield browser
        finally:
            await self.browser_pool.put(browser)
    
    async def crawl(self, task: CrawlTask) -> CrawlResult:
        """执行动态爬取"""
        async with self.get_browser() as browser:
            page = await browser.new_page()
            
            try:
                # 设置用户代理
                if task.headers.get('User-Agent'):
                    await page.set_extra_http_headers({'User-Agent': task.headers['User-Agent']})
                
                # 访问页面
                response = await page.goto(
                    task.url,
                    wait_until='networkidle',
                    timeout=task.timeout * 1000
                )
                
                # 等待动态内容加载
                if task.wait_selector:
                    await page.wait_for_selector(task.wait_selector, timeout=10000)
                
                # 模拟用户行为
                if task.use_behavior_simulation:
                    await self._simulate_user_behavior(page)
                
                # 提取数据
                extracted_data = {}
                if task.extraction_rules:
                    extracted_data = await self._extract_data(page, task.extraction_rules)
                
                # 获取页面内容
                html = await page.content()
                
                # 截图（如果需要）
                screenshot = None
                if task.take_screenshot:
                    screenshot = await page.screenshot(type='png')
                
                return CrawlResult(
                    task_id=task.task_id,
                    success=True,
                    status_code=response.status,
                    html=html,
                    extracted_data=extracted_data,
                    screenshot=screenshot
                )
                
            except Exception as e:
                return CrawlResult(
                    task_id=task.task_id,
                    success=False,
                    error_message=str(e)
                )
            finally:
                await page.close()
    
    async def _simulate_user_behavior(self, page: Page):
        """模拟用户行为"""
        # 随机滚动
        await page.evaluate("""
            window.scrollTo({
                top: Math.random() * document.body.scrollHeight,
                behavior: 'smooth'
            });
        """)
        
        # 随机延迟
        await asyncio.sleep(random.uniform(1, 3))
        
        # 模拟鼠标移动
        await page.mouse.move(
            random.randint(100, 800),
            random.randint(100, 600)
        )
```

---

## 第3阶段：反爬系统技术要点

### 3.1 代理池管理系统
```python
# src/anti_detection/proxy/proxy_manager.py
import asyncio
import aiohttp
from typing import List, Optional
from dataclasses import dataclass, field
from datetime import datetime, timedelta

@dataclass
class ProxyInfo:
    url: str
    protocol: str  # http, https, socks5
    location: str
    provider: str
    score: int = 100
    response_time: float = 0.0
    success_count: int = 0
    fail_count: int = 0
    last_used: Optional[datetime] = None
    last_checked: Optional[datetime] = None
    is_active: bool = True

class ProxyManager:
    def __init__(self, redis_client, config: dict):
        self.redis = redis_client
        self.config = config
        self.proxy_pool: List[ProxyInfo] = []
        self.check_interval = config.get('check_interval', 300)
        self.min_score = config.get('min_score', 70)
        
    async def initialize(self):
        """初始化代理池"""
        await self._load_proxies_from_db()
        await self._start_health_checker()
        
    async def get_proxy(self, location: str = None) -> Optional[ProxyInfo]:
        """获取可用代理"""
        available_proxies = [
            p for p in self.proxy_pool 
            if p.is_active and p.score >= self.min_score
        ]
        
        if location:
            available_proxies = [p for p in available_proxies if p.location == location]
        
        if not available_proxies:
            return None
            
        # 选择评分最高且最近未使用的代理
        proxy = max(available_proxies, key=lambda p: (p.score, -(p.last_used or datetime.min).timestamp()))
        proxy.last_used = datetime.utcnow()
        
        return proxy
    
    async def report_proxy_result(self, proxy: ProxyInfo, success: bool, response_time: float = 0):
        """报告代理使用结果"""
        if success:
            proxy.success_count += 1
            proxy.response_time = response_time
            proxy.score = min(100, proxy.score + 1)
        else:
            proxy.fail_count += 1
            proxy.score = max(0, proxy.score - 5)
            
        # 如果分数太低，标记为不可用
        if proxy.score < self.min_score:
            proxy.is_active = False
            
        # 更新Redis缓存
        await self._update_proxy_in_cache(proxy)
    
    async def _check_proxy_health(self, proxy: ProxyInfo) -> bool:
        """检查代理健康状态"""
        test_urls = [
            'http://httpbin.org/ip',
            'https://httpbin.org/ip'
        ]
        
        for url in test_urls:
            try:
                async with aiohttp.ClientSession(
                    connector=aiohttp.TCPConnector(ssl=False),
                    timeout=aiohttp.ClientTimeout(total=10)
                ) as session:
                    start_time = asyncio.get_event_loop().time()
                    
                    async with session.get(
                        url,
                        proxy=proxy.url,
                        headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                    ) as response:
                        if response.status == 200:
                            response_time = asyncio.get_event_loop().time() - start_time
                            await self.report_proxy_result(proxy, True, response_time)
                            return True
                            
            except Exception:
                continue
                
        await self.report_proxy_result(proxy, False)
        return False
```

### 3.2 浏览器指纹生成系统
```python
# src/anti_detection/fingerprint/fingerprint_generator.py
import random
import json
from typing import Dict, List, Any
from faker import Faker

class FingerprintGenerator:
    def __init__(self):
        self.fake = Faker()
        self.user_agents = self._load_user_agents()
        self.screen_resolutions = [
            '1920x1080', '1366x768', '1536x864', '1440x900',
            '1280x720', '1600x900', '2560x1440', '3840x2160'
        ]
        self.timezones = [
            'America/New_York', 'America/Los_Angeles', 'Europe/London',
            'Europe/Berlin', 'Asia/Tokyo', 'Asia/Shanghai', 'Asia/Kolkata'
        ]
        
    def generate_fingerprint(self) -> Dict[str, Any]:
        """生成浏览器指纹"""
        return {
            'user_agent': self._generate_user_agent(),
            'screen_resolution': random.choice(self.screen_resolutions),
            'timezone': random.choice(self.timezones),
            'languages': self._generate_languages(),
            'webgl_vendor': self._generate_webgl_vendor(),
            'webgl_renderer': self._generate_webgl_renderer(),
            'canvas_fingerprint': self._generate_canvas_fingerprint(),
            'fonts': self._generate_fonts(),
            'plugins': self._generate_plugins(),
            'hardware_concurrency': random.choice([2, 4, 8, 16]),
            'device_memory': random.choice([2, 4, 8, 16, 32]),
            'platform': self._generate_platform()
        }
    
    def _generate_user_agent(self) -> str:
        """生成User-Agent"""
        chrome_versions = ['119.0.0.0', '118.0.0.0', '117.0.0.0', '116.0.0.0']
        webkit_versions = ['537.36']
        
        version = random.choice(chrome_versions)
        webkit = random.choice(webkit_versions)
        
        return f'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/{webkit} (KHTML, like Gecko) Chrome/{version} Safari/{webkit}'
    
    def _generate_canvas_fingerprint(self) -> str:
        """生成Canvas指纹"""
        # 模拟Canvas渲染结果的哈希值
        import hashlib
        random_data = f"{random.random()}{random.randint(1000, 9999)}"
        return hashlib.md5(random_data.encode()).hexdigest()
    
    def _generate_webgl_vendor(self) -> str:
        """生成WebGL供应商信息"""
        vendors = [
            'Google Inc.',
            'Mozilla',
            'WebKit',
            'Microsoft Corporation'
        ]
        return random.choice(vendors)
    
    def _generate_webgl_renderer(self) -> str:
        """生成WebGL渲染器信息"""
        renderers = [
            'ANGLE (Intel(R) HD Graphics 4000 Direct3D11 vs_5_0 ps_5_0)',
            'ANGLE (NVIDIA GeForce GTX 1060 Direct3D11 vs_5_0 ps_5_0)',
            'ANGLE (AMD Radeon R9 200 Series Direct3D11 vs_5_0 ps_5_0)',
            'WebKit WebGL'
        ]
        return random.choice(renderers)
    
    async def apply_fingerprint_to_page(self, page, fingerprint: Dict[str, Any]):
        """将指纹应用到页面"""
        # 设置User-Agent
        await page.set_user_agent(fingerprint['user_agent'])
        
        # 注入JavaScript代码修改指纹
        await page.add_init_script(f"""
            // 修改屏幕分辨率
            Object.defineProperty(screen, 'width', {{
                get: () => {fingerprint['screen_resolution'].split('x')[0]}
            }});
            Object.defineProperty(screen, 'height', {{
                get: () => {fingerprint['screen_resolution'].split('x')[1]}
            }});
            
            // 修改时区
            Date.prototype.getTimezoneOffset = function() {{
                return {random.randint(-720, 720)};
            }};
            
            // 修改语言
            Object.defineProperty(navigator, 'languages', {{
                get: () => {json.dumps(fingerprint['languages'])}
            }});
            
            // 修改硬件并发数
            Object.defineProperty(navigator, 'hardwareConcurrency', {{
                get: () => {fingerprint['hardware_concurrency']}
            }});
            
            // 修改设备内存
            Object.defineProperty(navigator, 'deviceMemory', {{
                get: () => {fingerprint['device_memory']}
            }});
            
            // 修改WebGL指纹
            const getParameter = WebGLRenderingContext.prototype.getParameter;
            WebGLRenderingContext.prototype.getParameter = function(parameter) {{
                if (parameter === 37445) return '{fingerprint['webgl_vendor']}';
                if (parameter === 37446) return '{fingerprint['webgl_renderer']}';
                return getParameter.call(this, parameter);
            }};
        """)
```

### 3.3 行为模拟系统
```python
# src/anti_detection/behavior/behavior_simulator.py
import asyncio
import random
import math
from typing import List, Tuple
from playwright.async_api import Page

class BehaviorSimulator:
    def __init__(self):
        self.mouse_patterns = self._load_mouse_patterns()
        self.typing_patterns = self._load_typing_patterns()
        
    async def simulate_human_behavior(self, page: Page, duration: int = 30):
        """模拟人类浏览行为"""
        actions = [
            self._random_mouse_movement,
            self._random_scroll,
            self._pause_and_read,
            self._hover_elements
        ]
        
        end_time = asyncio.get_event_loop().time() + duration
        
        while asyncio.get_event_loop().time() < end_time:
            action = random.choice(actions)
            await action(page)
            await asyncio.sleep(random.uniform(0.5, 3.0))
    
    async def _random_mouse_movement(self, page: Page):
        """随机鼠标移动"""
        # 获取页面尺寸
        viewport_size = await page.viewport_size()
        width, height = viewport_size['width'], viewport_size['height']
        
        # 生成贝塞尔曲线路径
        start_x, start_y = random.randint(0, width), random.randint(0, height)
        end_x, end_y = random.randint(0, width), random.randint(0, height)
        
        # 创建中间控制点
        control_points = self._generate_bezier_points(
            (start_x, start_y), 
            (end_x, end_y)
        )
        
        # 沿路径移动鼠标
        for x, y in control_points:
            await page.mouse.move(x, y)
            await asyncio.sleep(random.uniform(0.01, 0.05))
    
    async def _random_scroll(self, page: Page):
        """随机滚动"""
        scroll_distance = random.randint(100, 800)
        scroll_direction = random.choice(['up', 'down'])
        
        if scroll_direction == 'down':
            await page.mouse.wheel(0, scroll_distance)
        else:
            await page.mouse.wheel(0, -scroll_distance)
        
        # 滚动后暂停
        await asyncio.sleep(random.uniform(0.5, 2.0))
    
    async def _pause_and_read(self, page: Page):
        """暂停阅读"""
        read_time = random.uniform(2.0, 8.0)
        await asyncio.sleep(read_time)
    
    async def _hover_elements(self, page: Page):
        """悬停在元素上"""
        try:
            # 查找可交互元素
            elements = await page.query_selector_all('a, button, input, [onclick]')
            if elements:
                element = random.choice(elements)
                await element.hover()
                await asyncio.sleep(random.uniform(0.5, 2.0))
        except Exception:
            pass  # 忽略错误，继续其他行为
    
    def _generate_bezier_points(self, start: Tuple[int, int], end: Tuple[int, int], num_points: int = 20) -> List[Tuple[int, int]]:
        """生成贝塞尔曲线路径点"""
        x1, y1 = start
        x4, y4 = end
        
        # 随机控制点
        x2 = x1 + random.randint(-100, 100)
        y2 = y1 + random.randint(-100, 100)
        x3 = x4 + random.randint(-100, 100)
        y3 = y4 + random.randint(-100, 100)
        
        points = []
        for i in range(num_points + 1):
            t = i / num_points
            
            # 贝塞尔曲线公式
            x = (1-t)**3 * x1 + 3*(1-t)**2*t * x2 + 3*(1-t)*t**2 * x3 + t**3 * x4
            y = (1-t)**3 * y1 + 3*(1-t)**2*t * y2 + 3*(1-t)*t**2 * y3 + t**3 * y4
            
            points.append((int(x), int(y)))
        
        return points
    
    async def simulate_typing(self, page: Page, element_selector: str, text: str):
        """模拟人类打字"""
        element = await page.query_selector(element_selector)
        if not element:
            return
            
        await element.click()
        
        for char in text:
            await page.keyboard.type(char)
            # 模拟打字间隔
            delay = random.uniform(0.05, 0.3)
            
            # 某些字符打字更慢
            if char in ' .,!?':
                delay *= random.uniform(1.5, 3.0)
            
            await asyncio.sleep(delay)
            
            # 偶尔打字错误并修正
            if random.random() < 0.05:  # 5%概率打错
                await page.keyboard.press('Backspace')
                await asyncio.sleep(random.uniform(0.1, 0.3))
                await page.keyboard.type(char)
```

---

## 第4-6阶段：平台适配器技术要点

### 4.1 平台适配器基类设计
```python
# src/adapters/base/platform_adapter.py
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import asyncio
from src.core.engines.base.engine_interface import EngineInterface

@dataclass
class SearchQuery:
    keyword: str
    location: str = ""
    category: str = ""
    page: int = 1
    limit: int = 20
    filters: Dict[str, Any] = None

@dataclass 
class POIData:
    platform: str
    platform_id: str
    name: str
    address: str = ""
    location: Tuple[float, float] = None
    rating: float = 0.0
    review_count: int = 0
    price_level: int = 0
    categories: List[str] = None
    images: List[str] = None
    raw_data: Dict[str, Any] = None

class PlatformAdapter(ABC):
    """平台适配器基类"""
    
    def __init__(self, engine: EngineInterface, config: Dict[str, Any]):
        self.engine = engine
        self.config = config
        self.platform_name = self.__class__.__name__.lower().replace('adapter', '')
        
    @abstractmethod
    async def search_pois(self, query: SearchQuery) -> List[POIData]:
        """搜索POI数据"""
        pass
    
    @abstractmethod
    async def get_poi_detail(self, poi_id: str) -> Optional[POIData]:
        """获取POI详情"""
        pass
    
    @abstractmethod
    def parse_search_results(self, html: str) -> List[POIData]:
        """解析搜索结果"""
        pass
    
    @abstractmethod  
    def parse_poi_detail(self, html: str) -> POIData:
        """解析POI详情"""
        pass
    
    async def batch_crawl_pois(self, queries: List[SearchQuery]) -> List[POIData]:
        """批量爬取POI"""
        all_pois = []
        
        # 控制并发数
        semaphore = asyncio.Semaphore(self.config.get('max_concurrent', 5))
        
        async def crawl_single_query(query: SearchQuery):
            async with semaphore:
                try:
                    pois = await self.search_pois(query)
                    return pois
                except Exception as e:
                    self.logger.error(f"Failed to crawl query {query}: {e}")
                    return []
        
        # 并发执行所有查询
        tasks = [crawl_single_query(query) for query in queries]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, list):
                all_pois.extend(result)
        
        return all_pois
```

### 4.2 高德地图适配器实现示例
```python
# src/adapters/amap/amap_adapter.py
import re
import json
from typing import List, Optional
from urllib.parse import urlencode
from bs4 import BeautifulSoup

class AmapAdapter(PlatformAdapter):
    """高德地图适配器"""
    
    def __init__(self, engine: EngineInterface, config: Dict[str, Any]):
        super().__init__(engine, config)
        self.base_url = "https://ditu.amap.com"
        self.search_url = "https://ditu.amap.com/search"
        
    async def search_pois(self, query: SearchQuery) -> List[POIData]:
        """搜索POI"""
        # 构建搜索URL
        params = {
            'query': query.keyword,
            'city': query.location,
            'page': query.page
        }
        
        search_url = f"{self.search_url}?{urlencode(params)}"
        
        # 创建爬取任务
        task = CrawlTask(
            task_id=f"amap_search_{query.keyword}_{query.page}",
            url=search_url,
            timeout=30,
            extraction_rules={
                'wait_selector': '.poi-list .poi-item',
                'extraction_type': 'custom'
            }
        )
        
        # 使用引擎爬取
        result = await self.engine.crawl(task)
        
        if not result.success:
            raise Exception(f"Failed to crawl Amap: {result.error_message}")
        
        # 解析结果
        return self.parse_search_results(result.html)
    
    def parse_search_results(self, html: str) -> List[POIData]:
        """解析搜索结果"""
        soup = BeautifulSoup(html, 'html.parser')
        pois = []
        
        # 查找POI列表项
        poi_items = soup.select('.poi-list .poi-item')
        
        for item in poi_items:
            try:
                # 提取基本信息
                name_elem = item.select_one('.poi-title a')
                name = name_elem.get_text(strip=True) if name_elem else ""
                
                poi_id = self._extract_poi_id(name_elem.get('href', '')) if name_elem else ""
                
                # 提取地址
                address_elem = item.select_one('.poi-address')
                address = address_elem.get_text(strip=True) if address_elem else ""
                
                # 提取评分
                rating_elem = item.select_one('.poi-rating .rating-score')
                rating = float(rating_elem.get_text(strip=True)) if rating_elem else 0.0
                
                # 提取评论数
                review_elem = item.select_one('.poi-rating .rating-count')
                review_count = self._extract_number(review_elem.get_text(strip=True)) if review_elem else 0
                
                # 提取坐标（如果有）
                location = self._extract_location_from_item(item)
                
                # 提取分类
                category_elem = item.select_one('.poi-category')
                categories = [category_elem.get_text(strip=True)] if category_elem else []
                
                poi = POIData(
                    platform='amap',
                    platform_id=poi_id,
                    name=name,
                    address=address,
                    location=location,
                    rating=rating,
                    review_count=review_count,
                    categories=categories,
                    raw_data={
                        'html': str(item),
                        'source_url': self.search_url
                    }
                )
                
                pois.append(poi)
                
            except Exception as e:
                self.logger.warning(f"Failed to parse POI item: {e}")
                continue
        
        return pois
    
    async def get_poi_detail(self, poi_id: str) -> Optional[POIData]:
        """获取POI详情"""
        detail_url = f"{self.base_url}/detail/{poi_id}"
        
        task = CrawlTask(
            task_id=f"amap_detail_{poi_id}",
            url=detail_url,
            timeout=30,
            extraction_rules={
                'wait_selector': '.poi-detail-content',
                'extraction_type': 'detail'
            }
        )
        
        result = await self.engine.crawl(task)
        
        if not result.success:
            return None
        
        return self.parse_poi_detail(result.html)
    
    def parse_poi_detail(self, html: str) -> POIData:
        """解析POI详情页"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # 提取详细信息
        name = self._safe_extract_text(soup, '.poi-detail-title h1')
        address = self._safe_extract_text(soup, '.poi-detail-address')
        phone = self._safe_extract_text(soup, '.poi-detail-phone')
        
        # 提取营业时间
        hours_elem = soup.select_one('.poi-detail-hours')
        business_hours = self._parse_business_hours(hours_elem) if hours_elem else {}
        
        # 提取图片
        image_elems = soup.select('.poi-detail-images img')
        images = [img.get('src') for img in image_elems if img.get('src')]
        
        # 提取坐标
        location = self._extract_location_from_detail(soup)
        
        return POIData(
            platform='amap',
            name=name,
            address=address,
            location=location,
            images=images,
            raw_data={
                'phone': phone,
                'business_hours': business_hours,
                'html': str(soup),
                'source_url': html
            }
        )
    
    def _extract_poi_id(self, href: str) -> str:
        """从链接提取POI ID"""
        match = re.search(r'/detail/([^/?]+)', href)
        return match.group(1) if match else ""
    
    def _extract_location_from_item(self, item) -> Optional[Tuple[float, float]]:
        """从搜索结果项提取坐标"""
        # 查找data-lat和data-lng属性
        lat_elem = item.get('data-lat')
        lng_elem = item.get('data-lng')
        
        if lat_elem and lng_elem:
            try:
                return (float(lat_elem), float(lng_elem))
            except ValueError:
                pass
                
        return None
    
    def _parse_business_hours(self, hours_elem) -> Dict[str, str]:
        """解析营业时间"""
        hours_text = hours_elem.get_text(strip=True)
        # 这里可以添加更复杂的营业时间解析逻辑
        return {'raw': hours_text}
    
    def _safe_extract_text(self, soup, selector: str) -> str:
        """安全提取文本"""
        elem = soup.select_one(selector)
        return elem.get_text(strip=True) if elem else ""
    
    def _extract_number(self, text: str) -> int:
        """从文本中提取数字"""
        match = re.search(r'(\d+)', text)
        return int(match.group(1)) if match else 0
```

---

## 第5阶段：数据处理系统技术要点

### 5.1 数据清洗系统
```python
# src/processors/cleaner/data_cleaner.py
import re
import html
from typing import Any, Dict, List
from bs4 import BeautifulSoup

class DataCleaner:
    def __init__(self):
        self.html_tag_pattern = re.compile(r'<[^>]+>')
        self.whitespace_pattern = re.compile(r'\s+')
        self.phone_pattern = re.compile(r'[\d\-\(\)\s\+]+')
        
    async def clean_poi_data(self, poi_data: POIData) -> POIData:
        """清洗POI数据"""
        # 清洗名称
        poi_data.name = self.clean_text(poi_data.name)
        
        # 清洗地址
        poi_data.address = self.clean_address(poi_data.address)
        
        # 验证评分
        poi_data.rating = self.validate_rating(poi_data.rating)
        
        # 清洗分类
        poi_data.categories = self.clean_categories(poi_data.categories)
        
        # 清洗图片URL
        poi_data.images = self.clean_image_urls(poi_data.images)
        
        return poi_data
    
    def clean_text(self, text: str) -> str:
        """清洗文本"""
        if not text:
            return ""
        
        # 移除HTML标签
        text = self.html_tag_pattern.sub('', text)
        
        # 解码HTML实体
        text = html.unescape(text)
        
        # 标准化空白字符
        text = self.whitespace_pattern.sub(' ', text)
        
        # 移除前后空白
        text = text.strip()
        
        return text
    
    def clean_address(self, address: str) -> str:
        """清洗地址"""
        address = self.clean_text(address)
        
        # 移除多余的地址前缀
        prefixes = ['地址:', '详细地址:', 'Address:', '位置:']
        for prefix in prefixes:
            if address.startswith(prefix):
                address = address[len(prefix):].strip()
        
        return address
    
    def validate_rating(self, rating: float) -> float:
        """验证评分"""
        if not isinstance(rating, (int, float)):
            return 0.0
        
        # 评分范围检查
        if rating < 0:
            return 0.0
        elif rating > 5:
            return 5.0
        
        return round(rating, 1)
    
    def clean_categories(self, categories: List[str]) -> List[str]:
        """清洗分类"""
        if not categories:
            return []
        
        cleaned = []
        for category in categories:
            category = self.clean_text(category)
            if category and category not in cleaned:
                cleaned.append(category)
        
        return cleaned
    
    def clean_image_urls(self, images: List[str]) -> List[str]:
        """清洗图片URL"""
        if not images:
            return []
        
        cleaned = []
        for url in images:
            url = url.strip()
            if url and self.is_valid_url(url):
                cleaned.append(url)
        
        return cleaned
    
    def is_valid_url(self, url: str) -> bool:
        """验证URL格式"""
        url_pattern = re.compile(
            r'^https?://'  # http:// or https://
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
            r'localhost|'  # localhost...
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
            r'(?::\d+)?'  # optional port
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)
        return url_pattern.match(url) is not None
```

### 5.2 数据去重系统
```python
# src/processors/deduplicator/deduplicator.py
import hashlib
from typing import List, Dict, Set, Tuple
from difflib import SequenceMatcher
from geopy.distance import geodesic

class POIDeduplicator:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.similarity_threshold = config.get('similarity_threshold', 0.85)
        self.distance_threshold = config.get('distance_threshold', 100)  # 米
        
    async def deduplicate_pois(self, pois: List[POIData]) -> List[POIData]:
        """去重POI数据"""
        if not pois:
            return []
        
        # 按平台分组
        platform_groups = self._group_by_platform(pois)
        
        # 平台内去重
        deduplicated_groups = {}
        for platform, platform_pois in platform_groups.items():
            deduplicated_groups[platform] = await self._deduplicate_platform_pois(platform_pois)
        
        # 跨平台去重
        all_deduplicated = []
        for platform_pois in deduplicated_groups.values():
            all_deduplicated.extend(platform_pois)
        
        return await self._cross_platform_deduplication(all_deduplicated)
    
    def _group_by_platform(self, pois: List[POIData]) -> Dict[str, List[POIData]]:
        """按平台分组"""
        groups = {}
        for poi in pois:
            platform = poi.platform
            if platform not in groups:
                groups[platform] = []
            groups[platform].append(poi)
        return groups
    
    async def _deduplicate_platform_pois(self, pois: List[POIData]) -> List[POIData]:
        """平台内去重"""
        if len(pois) <= 1:
            return pois
        
        unique_pois = []
        processed_ids = set()
        
        for poi in pois:
            # 检查是否已处理（基于platform_id）
            if poi.platform_id in processed_ids:
                continue
                
            # 查找相似POI
            similar_pois = [poi]
            for other_poi in pois:
                if (other_poi.platform_id != poi.platform_id and 
                    other_poi.platform_id not in processed_ids and
                    await self._are_similar_pois(poi, other_poi)):
                    similar_pois.append(other_poi)
                    processed_ids.add(other_poi.platform_id)
            
            # 合并相似POI
            merged_poi = await self._merge_similar_pois(similar_pois)
            unique_pois.append(merged_poi)
            processed_ids.add(poi.platform_id)
        
        return unique_pois
    
    async def _cross_platform_deduplication(self, pois: List[POIData]) -> List[POIData]:
        """跨平台去重"""
        if len(pois) <= 1:
            return pois
        
        unique_pois = []
        processed_indices = set()
        
        for i, poi in enumerate(pois):
            if i in processed_indices:
                continue
                
            # 查找跨平台相似POI
            similar_group = [poi]
            similar_indices = {i}
            
            for j, other_poi in enumerate(pois[i+1:], i+1):
                if (j not in processed_indices and 
                    poi.platform != other_poi.platform and
                    await self._are_cross_platform_similar(poi, other_poi)):
                    similar_group.append(other_poi)
                    similar_indices.add(j)
            
            # 合并跨平台POI
            merged_poi = await self._merge_cross_platform_pois(similar_group)
            unique_pois.append(merged_poi)
            processed_indices.update(similar_indices)
        
        return unique_pois
    
    async def _are_similar_pois(self, poi1: POIData, poi2: POIData) -> bool:
        """判断两个POI是否相似（同平台）"""
        # 名称相似度
        name_similarity = SequenceMatcher(None, poi1.name, poi2.name).ratio()
        
        # 地址相似度
        address_similarity = SequenceMatcher(None, poi1.address, poi2.address).ratio()
        
        # 地理距离（如果有坐标）
        distance_check = True
        if poi1.location and poi2.location:
            distance = geodesic(poi1.location, poi2.location).meters
            distance_check = distance <= self.distance_threshold
        
        # 综合判断
        overall_similarity = (name_similarity + address_similarity) / 2
        
        return (overall_similarity >= self.similarity_threshold and distance_check)
    
    async def _are_cross_platform_similar(self, poi1: POIData, poi2: POIData) -> bool:
        """判断跨平台POI是否相似"""
        # 跨平台相似度阈值稍高
        threshold = self.similarity_threshold + 0.05
        
        # 名称相似度
        name_similarity = SequenceMatcher(None, poi1.name, poi2.name).ratio()
        
        # 必须有地理坐标且距离很近
        if not (poi1.location and poi2.location):
            return False
            
        distance = geodesic(poi1.location, poi2.location).meters
        distance_check = distance <= self.distance_threshold / 2  # 跨平台要求更严格
        
        return name_similarity >= threshold and distance_check
    
    async def _merge_similar_pois(self, pois: List[POIData]) -> POIData:
        """合并相似POI"""
        if len(pois) == 1:
            return pois[0]
        
        # 选择主POI（信息最完整的）
        main_poi = max(pois, key=lambda p: self._calculate_completeness_score(p))
        
        # 合并其他信息
        merged_poi = POIData(
            platform=main_poi.platform,
            platform_id=main_poi.platform_id,
            name=main_poi.name,
            address=main_poi.address or self._get_best_address(pois),
            location=main_poi.location or self._get_best_location(pois),
            rating=self._calculate_average_rating(pois),
            review_count=sum(p.review_count for p in pois),
            price_level=main_poi.price_level,
            categories=self._merge_categories(pois),
            images=self._merge_images(pois),
            raw_data={
                'merged_from': [p.platform_id for p in pois],
                'source_platforms': list(set(p.platform for p in pois)),
                'merge_timestamp': datetime.utcnow().isoformat()
            }
        )
        
        return merged_poi
    
    def _calculate_completeness_score(self, poi: POIData) -> int:
        """计算POI信息完整度分数"""
        score = 0
        if poi.name: score += 1
        if poi.address: score += 1
        if poi.location: score += 1
        if poi.rating > 0: score += 1
        if poi.review_count > 0: score += 1
        if poi.categories: score += 1
        if poi.images: score += 1
        return score
```

---

## 监控和性能优化

### 监控指标设计
```python
# src/monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, CollectorRegistry
import time
from functools import wraps

# 创建指标注册表
registry = CollectorRegistry()

# 爬取指标
crawl_requests_total = Counter(
    'crawler_requests_total',
    'Total number of crawl requests',
    ['platform', 'engine', 'status'],
    registry=registry
)

crawl_duration_seconds = Histogram(
    'crawler_request_duration_seconds',
    'Time spent on crawl requests',
    ['platform', 'engine'],
    registry=registry
)

# 数据库指标
db_connections_active = Gauge(
    'database_connections_active',
    'Number of active database connections',
    registry=registry
)

# 代理指标
proxy_pool_size = Gauge(
    'proxy_pool_size_total',
    'Total number of proxies in pool',
    ['status'],
    registry=registry
)

def monitor_crawl_performance(platform: str, engine: str):
    """爬取性能监控装饰器"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            start_time = time.time()
            status = 'success'
            
            try:
                result = await func(*args, **kwargs)
                return result
            except Exception as e:
                status = 'error'
                raise
            finally:
                duration = time.time() - start_time
                crawl_requests_total.labels(platform=platform, engine=engine, status=status).inc()
                crawl_duration_seconds.labels(platform=platform, engine=engine).observe(duration)
        
        return wrapper
    return decorator
```

这个技术实现指南涵盖了整个项目的核心技术要点，包括：

1. **基础架构**：配置管理、数据库连接、Redis缓存
2. **双引擎实现**：抽象设计、Crawl4AI和MediaCrawl的具体实现
3. **反爬系统**：代理池、浏览器指纹、行为模拟的完整实现
4. **平台适配器**：基类设计和具体平台实现示例
5. **数据处理**：清洗、去重系统的核心算法
6. **监控系统**：性能指标和监控装饰器

每个模块都提供了具体的代码实现和最佳实践，可以作为开发过程中的技术参考。