"""
Document Chunk Mixin - æ–‡æ¡£åˆ†ç‰‡åŠŸèƒ½æ··å…¥ç±»

ä¸ºå·¥å…·æä¾›æ–‡æ¡£åˆ†ç‰‡åŠŸèƒ½ï¼Œå½“è¾“å‡ºè¶…è¿‡æŒ‡å®š token é™åˆ¶æ—¶ï¼Œ
è‡ªåŠ¨å°†å“åº”åˆ†æˆå¤šä¸ªéƒ¨åˆ†ã€‚

è¿™ä¸ª mixin å¯ä»¥è¢«ä»»ä½•å·¥å…·ç»§æ‰¿ï¼Œæä¾›ï¼š
1. è‡ªåŠ¨æ£€æµ‹è¾“å‡ºé•¿åº¦
2. æ™ºèƒ½åˆ†ç‰‡å¤„ç†
3. å¤šéƒ¨åˆ†å“åº”è¿”å›
4. ç”¨æˆ·äº¤äº’æç¤º
"""

import logging
from typing import Any, Optional, Union

from mcp.types import TextContent

from tools.models import ToolOutput
from utils.document_chunker import chunk_document
from utils.token_utils import estimate_tokens

logger = logging.getLogger(__name__)


class DocumentChunkMixin:
    """
    æ–‡æ¡£åˆ†ç‰‡åŠŸèƒ½æ··å…¥ç±»

    ä¸ºå·¥å…·æä¾›æ™ºèƒ½æ–‡æ¡£åˆ†ç‰‡åŠŸèƒ½ï¼Œå¤„ç†è¶…é•¿è¾“å‡ºã€‚
    """

    # é»˜è®¤åˆ†ç‰‡é˜ˆå€¼ï¼ˆå¯è¢«å·¥å…·è¦†ç›–ï¼‰
    CHUNK_THRESHOLD_TOKENS = 30000

    def get_chunk_threshold(self) -> int:
        """
        è·å–åˆ†ç‰‡é˜ˆå€¼ï¼ˆtoken æ•°ï¼‰

        å·¥å…·å¯ä»¥è¦†ç›–æ­¤æ–¹æ³•æ¥è‡ªå®šä¹‰é˜ˆå€¼ã€‚

        Returns:
            int: è§¦å‘åˆ†ç‰‡çš„ token æ•°é˜ˆå€¼
        """
        return self.CHUNK_THRESHOLD_TOKENS

    def should_chunk_response(self, content: str) -> bool:
        """
        åˆ¤æ–­å“åº”æ˜¯å¦éœ€è¦åˆ†ç‰‡

        Args:
            content: å“åº”å†…å®¹

        Returns:
            bool: True å¦‚æœéœ€è¦åˆ†ç‰‡
        """
        if not content:
            return False

        token_count = estimate_tokens(content)
        threshold = self.get_chunk_threshold()

        if token_count > threshold:
            logger.info(
                f"{self.name} tool: Response has {token_count:,} tokens, "
                f"exceeding threshold of {threshold:,}. Will chunk."
            )
            return True
        return False

    def chunk_response(self, content: str) -> list[str]:
        """
        å°†å“åº”å†…å®¹åˆ†ç‰‡

        Args:
            content: è¦åˆ†ç‰‡çš„å†…å®¹

        Returns:
            List[str]: åˆ†ç‰‡åçš„å†…å®¹åˆ—è¡¨
        """
        threshold = self.get_chunk_threshold()
        return chunk_document(content, max_tokens=threshold)

    def create_chunked_response(
        self, content: str, original_request: Any, model_info: Optional[dict] = None, chunk_index: int = 0
    ) -> Union[ToolOutput, list[TextContent]]:
        """
        åˆ›å»ºåˆ†ç‰‡å“åº”

        å½“è¾“å‡ºè¶…è¿‡é˜ˆå€¼æ—¶ï¼Œè¿”å›ç¬¬ä¸€ç‰‡å¹¶æä¾›ç»§ç»­é€‰é¡¹ã€‚

        Args:
            content: å®Œæ•´çš„å“åº”å†…å®¹
            original_request: åŸå§‹è¯·æ±‚å¯¹è±¡
            model_info: æ¨¡å‹ä¿¡æ¯
            chunk_index: å½“å‰åˆ†ç‰‡ç´¢å¼•ï¼ˆ0 è¡¨ç¤ºç¬¬ä¸€ç‰‡ï¼‰

        Returns:
            ToolOutput æˆ– List[TextContent]: åŒ…å«åˆ†ç‰‡å†…å®¹å’Œç»§ç»­é€‰é¡¹çš„å“åº”
        """
        # è·å–æ‰€æœ‰åˆ†ç‰‡
        chunks = self.chunk_response(content)

        if chunk_index >= len(chunks):
            # è¯·æ±‚çš„åˆ†ç‰‡ç´¢å¼•è¶…å‡ºèŒƒå›´
            return self._create_error_response(f"Invalid chunk index {chunk_index}. Document has {len(chunks)} parts.")

        # è·å–å½“å‰åˆ†ç‰‡
        current_chunk = chunks[chunk_index]

        # å¦‚æœè¿™ä¸æ˜¯æœ€åä¸€ç‰‡ï¼Œåˆ›å»ºç»§ç»­é€‰é¡¹
        if chunk_index < len(chunks) - 1:
            # ä¿å­˜å‰©ä½™å†…å®¹çš„çŠ¶æ€
            continuation_state = {
                "action": "continue_chunks",
                "chunks": chunks,
                "current_index": chunk_index,
                "original_request": original_request,
                "model_info": model_info,
            }

            # åˆ›å»ºç»§ç»­æç¤º
            continuation_prompt = (
                f"\n\n{'=' * 60}\n"
                f"ğŸ“‹ This is Part {chunk_index + 1} of {len(chunks)}. "
                f"The document continues in the next part.\n"
                f"{'=' * 60}\n\n"
                f"To view the next part, please use the continuation option."
            )

            # æ·»åŠ ç»§ç»­æç¤ºåˆ°å½“å‰åˆ†ç‰‡
            display_content = current_chunk + continuation_prompt

            # åˆ›å»ºå¸¦ç»§ç»­é€‰é¡¹çš„è¾“å‡º
            # æ³¨æ„ï¼šContinuationOffer éœ€è¦ continuation_id, note, remaining_turns
            # å¯¹äºæ–‡æ¡£åˆ†ç‰‡ï¼Œæˆ‘ä»¬ä½¿ç”¨ç‰¹æ®Šçš„ continuation_id
            return ToolOutput(
                content=display_content,
                content_type="text",
                status="continuation_available",
                metadata={
                    "chunk_info": {
                        "current_part": chunk_index + 1,
                        "total_parts": len(chunks),
                        "total_tokens": sum(estimate_tokens(c) for c in chunks),
                        "current_tokens": estimate_tokens(current_chunk),
                    },
                    "continuation_state": continuation_state,  # å­˜å‚¨åœ¨ metadata ä¸­
                },
            )
        else:
            # è¿™æ˜¯æœ€åä¸€ç‰‡ï¼Œä¸éœ€è¦ç»§ç»­é€‰é¡¹
            return ToolOutput(
                content=current_chunk,
                content_type="text",
                status="success",
                metadata={
                    "chunk_info": {
                        "current_part": chunk_index + 1,
                        "total_parts": len(chunks),
                        "total_tokens": sum(estimate_tokens(c) for c in chunks),
                        "current_tokens": estimate_tokens(current_chunk),
                    }
                },
            )

    def handle_chunk_continuation(self, state: dict[str, Any]) -> Union[ToolOutput, list[TextContent]]:
        """
        å¤„ç†åˆ†ç‰‡ç»§ç»­è¯·æ±‚

        Args:
            state: ç»§ç»­çŠ¶æ€ï¼ŒåŒ…å«åˆ†ç‰‡ä¿¡æ¯

        Returns:
            ä¸‹ä¸€ä¸ªåˆ†ç‰‡çš„å“åº”
        """
        if state.get("action") != "continue_chunks":
            return self._create_error_response("Invalid continuation state")

        chunks = state.get("chunks", [])
        current_index = state.get("current_index", 0)
        next_index = current_index + 1

        if next_index >= len(chunks):
            return self._create_error_response(f"No more chunks available. Document has {len(chunks)} parts.")

        # ç›´æ¥è¿”å›ä¸‹ä¸€ä¸ªåˆ†ç‰‡
        current_chunk = chunks[next_index]

        # å¦‚æœè¿˜æœ‰æ›´å¤šåˆ†ç‰‡ï¼Œåˆ›å»ºç»§ç»­é€‰é¡¹
        if next_index < len(chunks) - 1:
            continuation_state = {
                "action": "continue_chunks",
                "chunks": chunks,
                "current_index": next_index,
            }

            continuation_prompt = (
                f"\n\n{'=' * 60}\n"
                f"ğŸ“‹ This is Part {next_index + 1} of {len(chunks)}. "
                f"The document continues in the next part.\n"
                f"{'=' * 60}\n\n"
                f"To view the next part, please use the continuation option."
            )

            display_content = current_chunk + continuation_prompt

            return ToolOutput(
                content=display_content,
                content_type="text",
                status="continuation_available",
                metadata={
                    "chunk_info": {
                        "current_part": next_index + 1,
                        "total_parts": len(chunks),
                        "current_tokens": estimate_tokens(current_chunk),
                    },
                    "continuation_state": continuation_state,  # å­˜å‚¨åœ¨ metadata ä¸­
                },
            )
        else:
            # æœ€åä¸€ç‰‡
            return ToolOutput(
                content=current_chunk,
                content_type="text",
                status="success",
                metadata={
                    "chunk_info": {
                        "current_part": next_index + 1,
                        "total_parts": len(chunks),
                        "current_tokens": estimate_tokens(current_chunk),
                    }
                },
            )

    def wrap_response_with_chunking(
        self, content: str, original_request: Any, model_info: Optional[dict] = None
    ) -> Union[str, ToolOutput]:
        """
        åŒ…è£…å“åº”ï¼Œå¦‚æœéœ€è¦åˆ™è¿›è¡Œåˆ†ç‰‡

        è¿™æ˜¯ä¸€ä¸ªä¾¿æ·æ–¹æ³•ï¼Œå·¥å…·å¯ä»¥åœ¨ format_response ä¸­è°ƒç”¨ã€‚

        Args:
            content: åŸå§‹å“åº”å†…å®¹
            original_request: åŸå§‹è¯·æ±‚
            model_info: æ¨¡å‹ä¿¡æ¯

        Returns:
            å¦‚æœä¸éœ€è¦åˆ†ç‰‡è¿”å›åŸå§‹å†…å®¹å­—ç¬¦ä¸²ï¼Œ
            å¦‚æœéœ€è¦åˆ†ç‰‡è¿”å› ToolOutput å¯¹è±¡
        """
        if self.should_chunk_response(content):
            logger.info(f"{self.name} tool: Chunking response due to size")
            return self.create_chunked_response(content, original_request, model_info)
        return content

    def _create_error_response(self, error_message: str) -> ToolOutput:
        """
        åˆ›å»ºé”™è¯¯å“åº”

        Args:
            error_message: é”™è¯¯æ¶ˆæ¯

        Returns:
            ToolOutput: é”™è¯¯å“åº”å¯¹è±¡
        """
        return ToolOutput(
            content=f"Error: {error_message}", content_type="text", status="error", metadata={"error": error_message}
        )

    def estimate_response_chunks(self, content: str) -> int:
        """
        ä¼°ç®—å“åº”éœ€è¦åˆ†æˆå¤šå°‘ç‰‡

        Args:
            content: å“åº”å†…å®¹

        Returns:
            int: ä¼°è®¡çš„åˆ†ç‰‡æ•°
        """
        from utils.document_chunker import estimate_chunk_count

        return estimate_chunk_count(content, self.get_chunk_threshold())
