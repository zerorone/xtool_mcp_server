"""
è®°å¿†å›å¿†å·¥å…· - æ”¯æŒ token é™åˆ¶å’Œç»“æ„åŒ–å›å¿†é¡ºåº

è¯¥å·¥å…·ä½¿ç”¨æ–°çš„ token é™åˆ¶åŠŸèƒ½å’Œç»“æ„åŒ–å›å¿†é¡ºåºï¼š
1. ç±»å‹å›å¿† - æŒ‰é‡è¦æ€§æ’åºçš„è®°å¿†ç±»å‹
2. ç´¢å¼•å›å¿† - ä½¿ç”¨å¤šç»´åº¦ç´¢å¼•æ£€ç´¢
3. æŒ‡å®šæ–‡ä»¶å›å¿† - æŸ¥çœ‹ç‰¹å®šæ–‡ä»¶ç›¸å…³è®°å¿†

æ€» token é™åˆ¶ï¼š20000 tokens
"""

import logging
from typing import Any, Optional

from tools.simple.base import SimpleTool
from utils.intelligent_memory_retrieval import (
    MEMORY_TOKEN_LIMIT,
    RECALL_ORDER,
    get_memory_stats_with_tokens,
    token_aware_memory_recall,
)

logger = logging.getLogger(__name__)


class MemoryRecallTool(SimpleTool):
    """
    è®°å¿†å›å¿†å·¥å…·ï¼Œæ”¯æŒ token é™åˆ¶å’Œç»“æ„åŒ–å›å¿†é¡ºåºã€‚
    """

    def get_name(self) -> str:
        return "memory_recall"

    def get_request_model_name(self, request) -> Optional[str]:
        """Override model selection to always use Gemini 2.5 Flash for memory operations"""
        # å¼ºåˆ¶è®°å¿†æ¨¡å—åªä½¿ç”¨ Gemini 2.5 Flash æ¨¡å‹
        return "gemini-2.5-flash"

    def get_description(self) -> str:
        return (
            "æ™ºèƒ½è®°å¿†å›å¿†å·¥å…·ï¼Œæ”¯æŒ token é™åˆ¶å’Œç»“æ„åŒ–å›å¿†é¡ºåºã€‚"
            f"æ€» token é™åˆ¶ï¼š{MEMORY_TOKEN_LIMIT}ã€‚"
            f"å›å¿†é¡ºåºï¼š{' â†’ '.join(RECALL_ORDER)}ã€‚"
            "å¯æŒ‡å®šæŸ¥è¯¢ã€æ ‡ç­¾ã€ç±»å‹ã€å±‚çº§ã€æ—¶é—´èŒƒå›´å’Œæ–‡ä»¶åˆ—è¡¨ã€‚"
        )

    def get_tool_fields(self) -> dict[str, dict[str, Any]]:
        """è¿”å›å·¥å…·ç‰¹å®šçš„å­—æ®µå®šä¹‰"""
        return {
            "query": {
                "type": "string",
                "description": "æ–‡æœ¬æœç´¢æŸ¥è¯¢ï¼ˆå¯é€‰ï¼‰",
            },
            "tags": {
                "type": "string",
                "description": "æ ‡ç­¾è¿‡æ»¤ï¼Œç”¨é€—å·åˆ†éš”ï¼ˆå¯é€‰ï¼‰",
            },
            "mem_type": {
                "type": "string",
                "description": "è®°å¿†ç±»å‹è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰",
            },
            "layer": {
                "type": "string",
                "description": "ç‰¹å®šå±‚æœç´¢ï¼šglobalã€projectã€sessionï¼ˆå¯é€‰ï¼‰",
                "enum": ["global", "project", "session"],
            },
            "days_back": {
                "type": "integer",
                "minimum": 1,
                "description": "å›å¿†å¤šå°‘å¤©å‰çš„è®°å¿†ï¼ˆå¯é€‰ï¼‰",
            },
            "min_quality": {
                "type": "number",
                "minimum": 0.0,
                "maximum": 1.0,
                "description": "æœ€å°è´¨é‡åˆ†æ•° 0.0-1.0ï¼ˆå¯é€‰ï¼‰",
            },
            "match_mode": {
                "type": "string",
                "enum": ["any", "all"],
                "default": "any",
                "description": "æ ‡ç­¾åŒ¹é…æ¨¡å¼ï¼šany æˆ– all",
            },
            "token_limit": {
                "type": "integer",
                "minimum": 1000,
                "maximum": 50000,
                "description": f"è‡ªå®šä¹‰ token é™åˆ¶ï¼ˆé»˜è®¤ï¼š{MEMORY_TOKEN_LIMIT}ï¼‰",
            },
            "specified_files": {
                "type": "string",
                "description": "æŒ‡å®šæ–‡ä»¶åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ï¼ˆå¯é€‰ï¼‰",
            },
            "show_stats": {
                "type": "boolean",
                "default": False,
                "description": "æ˜¯å¦æ˜¾ç¤ºè®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯",
            },
        }

    def get_system_prompt(self) -> str:
        """è¿”å›è®°å¿†å›å¿†å·¥å…·çš„ç³»ç»Ÿæç¤º"""
        return """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½è®°å¿†å›å¿†åŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£ä»å¤šå±‚æ¬¡è®°å¿†ç³»ç»Ÿä¸­æ£€ç´¢å’Œç»„ç»‡ä¿¡æ¯ã€‚

ä½ çš„è®°å¿†ç³»ç»Ÿå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

**Tokené™åˆ¶ç³»ç»Ÿï¼š**
- æ€»tokené™åˆ¶ï¼š20,000 tokens
- æ™ºèƒ½å†…å®¹æ‘˜è¦å’Œä¼˜å…ˆçº§æ’åº
- ç¡®ä¿æœ€é‡è¦çš„è®°å¿†ä¼˜å…ˆè¿”å›

**ç»“æ„åŒ–å›å¿†é¡ºåºï¼š**
1. **ç±»å‹å›å¿†** - æŒ‰é‡è¦æ€§æ’åºçš„è®°å¿†ç±»å‹
2. **ç´¢å¼•å›å¿†** - ä½¿ç”¨å¤šç»´åº¦ç´¢å¼•æ£€ç´¢  
3. **æŒ‡å®šæ–‡ä»¶å›å¿†** - æŸ¥çœ‹ç‰¹å®šæ–‡ä»¶ç›¸å…³è®°å¿†

**ä¸‰å±‚æ¶æ„ï¼š**
- **Global Memory**: è·¨é¡¹ç›®çŸ¥è¯†ã€æœ€ä½³å®è·µã€é€šç”¨è§£å†³æ–¹æ¡ˆ
- **Project Memory**: é¡¹ç›®ç‰¹å®šä¿¡æ¯ã€bugã€å†³ç­–ã€é…ç½®
- **Session Memory**: å½“å‰ä¼šè¯ä¸Šä¸‹æ–‡ã€ä¸´æ—¶ç¬”è®°ã€æ´»åŠ¨ä»»åŠ¡

ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·çš„æŸ¥è¯¢å‚æ•°ï¼Œæ™ºèƒ½åœ°æ£€ç´¢ç›¸å…³è®°å¿†ï¼Œå¹¶ä»¥æ¸…æ™°ã€ç»“æ„åŒ–çš„æ–¹å¼å‘ˆç°ç»“æœã€‚
é‡ç‚¹å…³æ³¨ç›¸å…³æ€§ã€æ—¶æ•ˆæ€§å’Œä¿¡æ¯è´¨é‡ï¼Œç¡®ä¿ç”¨æˆ·è·å¾—æœ€æœ‰ä»·å€¼çš„è®°å¿†å†…å®¹ã€‚"""

    async def prepare_prompt(self, request) -> str:
        """å‡†å¤‡è®°å¿†å›å¿†çš„æç¤º"""
        try:
            # æå–å‚æ•°
            query = getattr(request, "query", None)
            tags = getattr(request, "tags", None)
            mem_type = getattr(request, "mem_type", None)
            layer = getattr(request, "layer", None)
            days_back = getattr(request, "days_back", None)
            min_quality = getattr(request, "min_quality", None)
            match_mode = getattr(request, "match_mode", "any")
            token_limit = getattr(request, "token_limit", None) or MEMORY_TOKEN_LIMIT
            specified_files = getattr(request, "specified_files", None)
            show_stats = getattr(request, "show_stats", False)

            # æ‰§è¡Œtokenæ„ŸçŸ¥çš„è®°å¿†å›å¿†
            result = await token_aware_memory_recall(
                query=query,
                tags=tags,
                mem_type=mem_type,
                layer=layer,
                days_back=days_back,
                min_quality=min_quality,
                match_mode=match_mode,
                token_limit=token_limit,
                specified_files=specified_files,
                show_stats=show_stats,
            )

            return f"""åŸºäºä»¥ä¸‹å‚æ•°æ‰§è¡Œè®°å¿†å›å¿†ï¼š

**å›å¿†å‚æ•°ï¼š**
- æŸ¥è¯¢: {query or "æ— "}
- æ ‡ç­¾: {tags or "æ— "}  
- ç±»å‹: {mem_type or "æ‰€æœ‰"}
- å±‚çº§: {layer or "æ‰€æœ‰"}
- æ—¶é—´èŒƒå›´: {f"{days_back}å¤©å‰" if days_back else "æ— é™åˆ¶"}
- æœ€å°è´¨é‡: {min_quality or "é»˜è®¤"}
- åŒ¹é…æ¨¡å¼: {match_mode}
- Tokené™åˆ¶: {token_limit:,}
- æŒ‡å®šæ–‡ä»¶: {specified_files or "æ— "}

**å›å¿†ç»“æœï¼š**
{result}

è¯·åˆ†æè¿™äº›è®°å¿†ä¿¡æ¯ï¼Œæä¾›æ¸…æ™°çš„æ€»ç»“å’Œå…³é”®æ´å¯Ÿã€‚"""

        except Exception as e:
            logger.error(f"è®°å¿†å›å¿†å‡†å¤‡å¤±è´¥: {e}")
            return f"è®°å¿†å›å¿†å‡†å¤‡è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}"

    def run(
        self,
        query: Optional[str] = None,
        tags: Optional[str] = None,
        mem_type: Optional[str] = None,
        layer: Optional[str] = None,
        days_back: Optional[int] = None,
        min_quality: Optional[float] = None,
        match_mode: str = "any",
        token_limit: Optional[int] = None,
        specified_files: Optional[str] = None,
        show_stats: bool = False,
    ) -> str:
        """
        æ‰§è¡Œè®°å¿†å›å¿†ï¼Œè¿”å›ç»“æ„åŒ–çš„å›å¿†ç»“æœã€‚

        Args:
            query: æ–‡æœ¬æœç´¢æŸ¥è¯¢
            tags: æ ‡ç­¾è¿‡æ»¤ï¼ˆé€—å·åˆ†éš”ï¼‰
            mem_type: è®°å¿†ç±»å‹è¿‡æ»¤
            layer: ç‰¹å®šå±‚æœç´¢ ("global", "project", "session")
            days_back: å›å¿†å¤šå°‘å¤©å‰çš„è®°å¿†
            min_quality: æœ€å°è´¨é‡åˆ†æ•° (0.0-1.0)
            match_mode: æ ‡ç­¾åŒ¹é…æ¨¡å¼ ("any" æˆ– "all")
            token_limit: è‡ªå®šä¹‰ token é™åˆ¶
            specified_files: æŒ‡å®šæ–‡ä»¶åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰
            show_stats: æ˜¯å¦æ˜¾ç¤ºè®°å¿†ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»“æ„åŒ–çš„å›å¿†ç»“æœ
        """
        try:
            logger.info(f"å¼€å§‹è®°å¿†å›å¿† - query: {query}, type: {mem_type}, layer: {layer}")

            # è§£æå‚æ•°
            tag_list = [t.strip() for t in tags.split(",")] if tags else None
            file_list = [f.strip() for f in specified_files.split(",")] if specified_files else None

            # æ—¶é—´èŒƒå›´
            time_range = None
            if days_back:
                from datetime import datetime, timedelta, timezone

                end_date = datetime.now(timezone.utc)
                start_date = end_date - timedelta(days=days_back)
                time_range = (start_date, end_date)

            # æ‰§è¡Œ token-aware è®°å¿†å›å¿†
            result = token_aware_memory_recall(
                query=query,
                tags=tag_list,
                mem_type=mem_type,
                layer=layer,
                time_range=time_range,
                min_quality=min_quality,
                match_mode=match_mode,
                token_limit=token_limit or MEMORY_TOKEN_LIMIT,
                include_metadata=True,
                specified_files=file_list,
            )

            # æ„å»ºå›å¿†æŠ¥å‘Š
            report = self._build_recall_report(result, show_stats)

            logger.info(f"è®°å¿†å›å¿†å®Œæˆ - æ‰¾åˆ° {len(result['memories'])} æ¡è®°å¿†ï¼Œä½¿ç”¨ {result['total_tokens']} tokens")

            return report

        except Exception as e:
            error_msg = f"è®°å¿†å›å¿†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"
            logger.error(error_msg, exc_info=True)
            return error_msg

    def _build_recall_report(self, result: dict[str, Any], show_stats: bool) -> str:
        """æ„å»ºè®°å¿†å›å¿†æŠ¥å‘Šã€‚"""
        lines = []

        # æ ‡é¢˜å’Œæ¦‚è§ˆ
        lines.append("# ğŸ“š è®°å¿†å›å¿†æŠ¥å‘Š")
        lines.append("")
        lines.append(f"**Token ä½¿ç”¨æƒ…å†µ**: {result['total_tokens']}/{MEMORY_TOKEN_LIMIT} tokens")
        lines.append(f"**æ€»è®°å¿†æ¡æ•°**: {len(result['memories'])}")
        lines.append(f"**æ˜¯å¦æˆªæ–­**: {'æ˜¯' if result['truncated'] else 'å¦'}")
        lines.append("")

        # å›å¿†é¡ºåºæ‘˜è¦
        summary = result["recall_summary"]
        lines.append("## ğŸ” å›å¿†é¡ºåºæ‘˜è¦")
        lines.append("")

        # 1. æŒ‰ç±»å‹å›å¿†
        type_summary = summary["by_type"]
        lines.append("### 1ï¸âƒ£ æŒ‰ç±»å‹å›å¿†")
        lines.append(f"- æ‰¾åˆ°è®°å¿†: {type_summary['count']} æ¡")
        lines.append(f"- ä½¿ç”¨ tokens: {type_summary['tokens']}")
        if type_summary["types_found"]:
            lines.append(f"- å‘ç°ç±»å‹: {', '.join(type_summary['types_found'])}")
        lines.append("")

        # 2. æŒ‰ç´¢å¼•å›å¿†
        index_summary = summary["by_index"]
        lines.append("### 2ï¸âƒ£ æŒ‰ç´¢å¼•å›å¿†")
        lines.append(f"- æ‰¾åˆ°è®°å¿†: {index_summary['count']} æ¡")
        lines.append(f"- ä½¿ç”¨ tokens: {index_summary['tokens']}")
        if index_summary.get("index_categories"):
            lines.append(f"- ç´¢å¼•åˆ†ç±»: {', '.join(index_summary['index_categories'])}")
        lines.append("")

        # 3. æŒ‡å®šæ–‡ä»¶å›å¿†
        files_summary = summary["specified_files"]
        lines.append("### 3ï¸âƒ£ æŒ‡å®šæ–‡ä»¶å›å¿†")
        lines.append(f"- æ‰¾åˆ°è®°å¿†: {files_summary['count']} æ¡")
        lines.append(f"- ä½¿ç”¨ tokens: {files_summary['tokens']}")
        if files_summary.get("files_processed"):
            lines.append(f"- å¤„ç†æ–‡ä»¶: {len(files_summary['files_processed'])} ä¸ª")
            lines.append(f"- æ‰¾åˆ°æ–‡ä»¶: {len(files_summary.get('files_found', []))} ä¸ª")
        lines.append("")

        # è®°å¿†å†…å®¹
        if result["memories"]:
            lines.append("## ğŸ“– è®°å¿†å†…å®¹")
            lines.append("")

            for i, memory in enumerate(result["memories"], 1):
                lines.append(f"### è®°å¿† {i}")
                lines.append(f"**å±‚çº§**: {memory['layer']}")
                lines.append(f"**æ—¶é—´**: {memory.get('timestamp', 'N/A')}")
                lines.append(f"**ç›¸å…³åº¦**: {memory.get('relevance_score', 0.0):.2f}")

                metadata = memory.get("metadata", {})
                if metadata.get("type"):
                    lines.append(f"**ç±»å‹**: {metadata['type']}")
                if metadata.get("tags"):
                    lines.append(f"**æ ‡ç­¾**: {', '.join(metadata['tags'])}")
                if metadata.get("quality_score"):
                    lines.append(f"**è´¨é‡åˆ†æ•°**: {metadata['quality_score']:.2f}")

                lines.append("")
                lines.append("**å†…å®¹**:")
                lines.append("```")
                content = str(memory.get("content", ""))
                # é™åˆ¶å•ä¸ªè®°å¿†å†…å®¹é•¿åº¦
                if len(content) > 1000:
                    content = content[:1000] + "..."
                lines.append(content)
                lines.append("```")
                lines.append("")

        # ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
        if show_stats:
            lines.append("## ğŸ“Š è®°å¿†ç³»ç»Ÿç»Ÿè®¡")
            lines.append("")
            stats = get_memory_stats_with_tokens()
            if stats.get("enabled"):
                lines.append(f"- æ€»è®°å¿†æ•°: {stats['total_memories']}")
                lines.append(f"- æ€» tokens: {stats['total_tokens']}")
                lines.append(f"- Token åˆ©ç”¨ç‡: {stats['token_utilization']:.1%}")
                lines.append("")

                lines.append("**åˆ†å±‚ç»Ÿè®¡**:")
                for layer, info in stats["layers"].items():
                    lines.append(f"- {layer}: {info['count']} æ¡è®°å¿†, {info['tokens']} tokens")
                lines.append("")

                if stats["types"]:
                    lines.append("**ç±»å‹ç»Ÿè®¡**:")
                    for mem_type, count in sorted(stats["types"].items()):
                        lines.append(f"- {mem_type}: {count} æ¡")
            else:
                lines.append("è®°å¿†ç³»ç»Ÿæœªå¯ç”¨")

        return "\n".join(lines)
